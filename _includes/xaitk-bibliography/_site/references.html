<style>ol.bibliography { list-style: none; }</style>

<h2 class="bibliography">2021</h2>
<ol class="bibliography"><li><span id="fan2021interpretability">Fan, Feng-Lei, et al. “On Interpretability of Artificial Neural Networks: A Survey.” <i>IEEE Transactions on Radiation and Plasma Medical Sciences</i>, IEEE, 2021.</span></li>
<li><span id="janizek2021explaining">Janizek, Joseph D., et al. “Explaining Explanations: Axiomatic Feature Interactions for Deep Networks.” <i>Journal of Machine Learning Research</i>, vol. 22, no. 104, 2021, pp. 1–54.</span></li>
<li><span id="zunino2021excitation">Zunino, Andrea, et al. “Excitation Dropout: Encouraging Plasticity in Deep Neural Networks.” <i>International Journal of Computer Vision</i>, vol. 129, no. 4, Springer, 2021, pp. 1139–52.</span></li></ol>
<h2 class="bibliography">2020</h2>
<ol class="bibliography"><li><span id="alipour2020study">Alipour, Kamran, et al. “A Study on Multimodal and Interactive Explanations for Visual Question Answering.” <i>ArXiv Preprint ArXiv:2003.00431</i>, 2020.</span></li>
<li><span id="arrieta2020explainable">Arrieta, Alejandro Barredo, et al. “Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI.” <i>Information Fusion</i>, vol. 58, Elsevier, 2020, pp. 82–115.</span></li>
<li><span id="chakraborti2020emerging">Chakraborti, Tathagata, et al. “The Emerging Landscape of Explainable Ai Planning and Decision Making.” <i>ArXiv Preprint ArXiv:2002.11697</i>, 2020.</span></li>
<li><span id="chen2020concept">Chen, Zhi, et al. “Concept Whitening for Interpretable Image Recognition.” <i>Nature Machine Intelligence</i>, vol. 2, no. 12, Nature Publishing Group, 2020, pp. 772–82.</span></li>
<li><span id="ehsan2020human">Ehsan, Upol, and Mark O. Riedl. “Human-Centered Explainable Ai: Towards a Reflective Sociotechnical Approach.” <i>International Conference on Human-Computer Interaction</i>, Springer, 2020, pp. 449–66.</span></li>
<li><span id="elton2020self">Elton, Daniel C. “Self-Explaining AI as an Alternative to Interpretable AI.” <i>International Conference on Artificial General Intelligence</i>, Springer, 2020, pp. 95–106.</span></li>
<li><span id="guidotti2020black">Guidotti, Riccardo, et al. “Black Box Explanation by Learning Image Exemplars in the Latent Feature Space.” <i>ArXiv Preprint ArXiv:2002.03746</i>, 2020.</span></li>
<li><span id="guidotti2020blacl">---. “Black Box Explanation by Learning Image Exemplars in the Latent Feature Space.” <i>ArXiv Preprint ArXiv:2002.03746</i>, 2020.</span></li>
<li><span id="guo2020explainable">Guo, Weisi. “Explainable Artificial Intelligence for 6G: Improving Trust between Human and Machine.” <i>IEEE Communications Magazine</i>, vol. 58, no. 6, IEEE, 2020, pp. 39–45.</span></li>
<li><span id="islam2020towards">Islam, Sheikh Rabiul, et al. “Towards Quantification of Explainability in Explainable Artificial Intelligence Methods.” <i>The Thirty-Third International Flairs Conference</i>, 2020.</span></li>
<li><span id="jung2020information">Jung, Alexander, and Pedro H. J. Nardelli. “An Information-Theoretic Approach to Personalized Explainable Machine Learning.” <i>IEEE Signal Processing Letters</i>, vol. 27, IEEE, 2020, pp. 825–29.</span></li>
<li><span id="kuzba2020would">Kuźba, Michał, and Przemysław Biecek. “What Would You Ask the Machine Learning Model? Identification of User Needs for Model Explanations Based on Human-Model Conversations.” <i>Joint European Conference on Machine Learning and Knowledge Discovery in Databases</i>, Springer, 2020, pp. 447–59.</span></li>
<li><span id="lakkaraju2020fool">Lakkaraju, Himabindu, and Osbert Bastani. “" How Do I Fool You?" Manipulating User Trust via Misleading Black Box Explanations.” <i>Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society</i>, 2020, pp. 79–85.</span></li>
<li><span id="liu2020towards">Liu, Wenqian, et al. “Towards Visually Explaining Variational Autoencoders.” <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 2020, pp. 8642–51.</span></li>
<li><span id="liu2020interpretable">Liu, Yi-Chieh, et al. “Interpretable Self-Attention Temporal Reasoning for Driving Behavior Understanding.” <i>ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>, IEEE, 2020, pp. 2338–42.</span></li>
<li><span id="madumal2020distal">Madumal, Prashan, et al. “Distal Explanations for Explainable Reinforcement Learning Agents.” <i>ArXiv Preprint ArXiv:2001.10284</i>, 2020.</span></li>
<li><span id="mai2020keeping">Mai, Theresa, et al. “Keeping It’ Organized and Logical’ after-Action Review for AI (AAR/AI).” <i>Proceedings of the 25th International Conference on Intelligent User Interfaces</i>, 2020, pp. 465–76.</span></li>
<li><span id="patir2020interpretability">Patir, Rupam, et al. “Interpretability of Black Box Models Through Data-View Extraction and Shadow Model Creation.” <i>International Conference on Neural Information Processing</i>, Springer, 2020, pp. 378–85.</span></li>
<li><span id="phillips2020four">Phillips, P. Jonathon, and Mark Przybocki. “Four Principles of Explainable AI as Applied to Biometrics and Facial Forensic Algorithms.” <i>ArXiv Preprint ArXiv:2002.01014</i>, 2020.</span></li>
<li><span id="putnam2020toward">Putnam, Vanessa. <i>Toward XAI for Intelligent Tutoring Systems: a Case Study</i>. University of British Columbia, 2020.</span></li>
<li><span id="saralajew2020fast">Saralajew, Sascha, et al. “Fast Adversarial Robustness Certification of Nearest Prototype Classifiers for Arbitrary Seminorms.” <i>NeurIPS</i>, 2020.</span></li>
<li><span id="schrills2020answer">Schrills, Tim, and Thomas Franke. “How to Answer Why–Evaluating the Explanations of AI Through Mental Model Analysis.” <i>ArXiv Preprint ArXiv:2002.02526</i>, 2020.</span></li>
<li><span id="schwalbe2020survey">Schwalbe, Gesina, and Martin Schels. “A Survey on Methods for the Safety Assurance of Machine Learning Based Systems.” <i>10th European Congress on Embedded Real Time Software and Systems (ERTS 2020)</i>, 2020.</span></li>
<li><span id="slack2020fooling">Slack, Dylan, et al. “Fooling Lime and Shap: Adversarial Attacks on Post Hoc Explanation Methods.” <i>Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society</i>, 2020, pp. 180–86.</span></li>
<li><span id="sokol2020explainability">Sokol, Kacper, and Peter Flach. “Explainability Fact Sheets: a Framework for Systematic Assessment of Explainable Approaches.” <i>Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</i>, 2020, pp. 56–67.</span></li>
<li><span id="tuckey2020general">Tuckey, David, et al. “A General Framework for Scientifically Inspired Explanations in AI.” <i>ArXiv Preprint ArXiv:2003.00749</i>, 2020.</span></li>
<li><span id="visani2020statistical">Visani, Giorgio, et al. “Statistical Stability Indices for LIME: Obtaining Reliable Explanations for Machine Learning Models.” <i>Journal of the Operational Research Society</i>, Taylor &amp; Francis, 2020, pp. 1–11.</span></li></ol>
<h2 class="bibliography">2019</h2>
<ol class="bibliography"><li><span id="annasamy2019towards">Annasamy, Raghuram Mandyam, and Katia Sycara. “Towards Better Interpretability in Deep q-Networks.” <i>Proceedings of the AAAI Conference on Artificial Intelligence</i>, vol. 33, no. 01, 2019, pp. 4561–69.</span></li>
<li><span id="artelt2019computation">Artelt, André, and Barbara Hammer. “On the Computation of Counterfactual Explanations–A Survey.” <i>ArXiv Preprint ArXiv:1911.07749</i>, 2019.</span></li>
<li><span id="atrey2019exploratory">Atrey, Akanksha, et al. “Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning.” <i>ArXiv Preprint ArXiv:1912.05743</i>, 2019.</span></li>
<li><span id="chapman2019emap">Chapman-Rounds, Matt, et al. “EMAP: Explanation by Minimal Adversarial Perturbation.” <i>ArXiv Preprint ArXiv:1912.00872</i>, 2019.</span></li>
<li><span id="cheng2019interpretability">Cheng, Taoli. “Interpretability Study on Deep Learning for Jet Physics at the Large Hadron Collider.” <i>ArXiv Preprint ArXiv:1911.01872</i>, 2019.</span></li>
<li><span id="cui2019face">Cui, Wanxin, et al. “Face Recognition via Convolutional Neural Networks and Siamese Neural Networks.” <i>2019 International Conference on Intelligent Computing, Automation and Systems (ICICAS)</i>, IEEE, 2019, pp. 746–50.</span></li>
<li><span id="du2019techniques">Du, Mengnan, et al. “Techniques for Interpretable Machine Learning.” <i>Communications of the ACM</i>, vol. 63, no. 1, ACM New York, NY, USA, 2019, pp. 68–77.</span></li>
<li><span id="edmonds2019tale">Edmonds, Mark, et al. “A Tale of Two Explanations: Enhancing Human Trust by Explaining Robot Behavior.” <i>Science Robotics</i>, vol. 4, no. 37, Science Robotics, 2019.</span></li>
<li><span id="fadnis2019heuristics">Fadnis, Kshitij, et al. “Heuristics for Interpretable Knowledge Graph Contextualization.” <i>ArXiv Preprint ArXiv:1911.02085</i>, 2019.</span></li>
<li><span id="goyal2019counterfactual">Goyal, Yash, et al. “Counterfactual Visual Explanations.” <i>International Conference on Machine Learning</i>, PMLR, 2019, pp. 2376–84.</span></li>
<li><span id="hossain2019comprehensive">Hossain, M. D. Zakir, et al. “A Comprehensive Survey of Deep Learning for Image Captioning.” <i>ACM Computing Surveys (CsUR)</i>, vol. 51, no. 6, ACM New York, NY, USA, 2019, pp. 1–36.</span></li>
<li><span id="juozapaitis2019explainable">Juozapaitis, Zoe, et al. “Explainable Reinforcement Learning via Reward Decomposition.” <i>IJCAI/ECAI Workshop on Explainable Artificial Intelligence</i>, 2019.</span></li>
<li><span id="keane2019case">Keane, Mark T., and Eoin M. Kenny. “How Case-Based Reasoning Explains Neural Networks: A Theoretical Analysis of XAI Using Post-Hoc Explanation-by-Example from a Survey of ANN-CBR Twin-Systems.” <i>International Conference on Case-Based Reasoning</i>, Springer, 2019, pp. 155–71.</span></li>
<li><span id="le2019x">Le, Thai, et al. “Why X Rather than Y? Explaining Neural Model’Predictions by Generating Intervention Counterfactual Samples.” <i>CoRR</i>, 2019.</span></li>
<li><span id="li2019formal">Li, Xiao, et al. “A Formal Methods Approach to Interpretable Reinforcement Learning for Robotic Planning.” <i>Science Robotics</i>, vol. 4, no. 37, Science Robotics, 2019.</span></li>
<li><span id="licato2019scenarios">Licato, John, et al. “Scenarios and Recommendations for Ethical Interpretive Ai.” <i>ArXiv Preprint ArXiv:1911.01917</i>, 2019.</span></li>
<li><span id="lucic2019focus">Lucic, Ana, et al. “FOCUS: Flexible Optimizable Counterfactual Explanations for Tree Ensembles.” <i>ArXiv Preprint ArXiv:1911.12199</i>, 2019.</span></li>
<li><span id="mahajan2019preserving">Mahajan, Divyat, et al. “Preserving Causal Constraints in Counterfactual Explanations for Machine Learning Classifiers.” <i>ArXiv Preprint ArXiv:1912.03277</i>, 2019.</span></li>
<li><span id="miller2019explanation">Miller, Tim. “Explanation in Artificial Intelligence: Insights from the Social Sciences.” <i>Artificial Intelligence</i>, vol. 267, Elsevier, 2019, pp. 1–38.</span></li>
<li><span id="miller2019explanatioo">---. “Explanation in Artificial Intelligence: Insights from the Social Sciences.” <i>Artificial Intelligence</i>, vol. 267, Elsevier, 2019, pp. 1–38.</span></li>
<li><span id="mittelstadt2019explaining">Mittelstadt, Brent, et al. “Explaining Explanations in AI.” <i>Proceedings of the Conference on Fairness, Accountability, and Transparency</i>, 2019, pp. 279–88.</span></li>
<li><span id="morichetta2019explain">Morichetta, Andrea, et al. “EXPLAIN-IT: towards Explainable AI for Unsupervised Network Traffic Analysis.” <i>Proceedings of the 3rd ACM CoNEXT Workshop on Big DAta, Machine Learning and Artificial Intelligence for Data Communication Networks</i>, 2019, pp. 22–28.</span></li>
<li><span id="mundhenk2019efficient">Mundhenk, T. Nathan, et al. “Efficient Saliency Maps for Explainable AI.” <i>ArXiv Preprint ArXiv:1911.11293</i>, 2019.</span></li>
<li><span id="odena2019tensorfuzz">Odena, Augustus, et al. “Tensorfuzz: Debugging Neural Networks with Coverage-Guided Fuzzing.” <i>International Conference on Machine Learning</i>, PMLR, 2019, pp. 4901–11.</span></li>
<li><span id="park2019infossm">Park, Young-Jin, and Han-Lim Choi. “InfoSSM: Interpretable Unsupervised Learning of Nonparametric State-Space Model for Multi-Modal Dynamics.” <i>AIAA Scitech 2019 Forum</i>, 2019, p. 0681.</span></li>
<li><span id="ramon2019counterfactual">Ramon, Yanou, et al. “Counterfactual Explanation Algorithms for Behavioral and Textual Data.” <i>ArXiv Preprint ArXiv:1912.01819</i>, 2019.</span></li>
<li><span id="scholkopf2019causality">Schölkopf, Bernhard. “Causality for Machine Learning.” <i>ArXiv Preprint ArXiv:1911.10500</i>, 2019.</span></li>
<li><span id="seo2019regional">Seo, Dasom, et al. “Regional Multi-Scale Approach for Visually Pleasing Explanations of Deep Neural Networks.” <i>IEEE Access</i>, vol. 8, IEEE, 2019, pp. 8572–82.</span></li>
<li><span id="zafar2019dlime">Zafar, Muhammad Rehman, and Naimul Mefraz Khan. “DLIME: A Deterministic Local Interpretable Model-Agnostic Explanations Approach for Computer-Aided Diagnosis Systems.” <i>ArXiv Preprint ArXiv:1906.10263</i>, 2019.</span></li>
<li><span id="zhang2019towards">Zhang, Hao, et al. “Towards a Unified Evaluation of Explanation Methods without Ground Truth.” <i>ArXiv Preprint ArXiv:1911.09017</i>, 2019.</span></li>
<li><span id="zhang2019anomaly">Zhang, Xiao, et al. <i>An Anomaly Contribution Explainer for Cyber-Security Applications</i>. 2019.</span></li>
<li><span id="zhang2019interpretable">Zhang, Yundong, et al. “Interpretable Visual Question Answering by Visual Grounding from Attention Supervision Mining.” <i>2019 Ieee Winter Conference on Applications of Computer Vision (Wacv)</i>, IEEE, 2019, pp. 349–57.</span></li></ol>
<h2 class="bibliography">2018</h2>
<ol class="bibliography"><li><span id="abdul2018trends">Abdul, Ashraf, et al. “Trends and Trajectories for Explainable, Accountable and Intelligible Systems: An Hci Research Agenda.” <i>Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</i>, 2018, pp. 1–18.</span></li>
<li><span id="alonso2018bibliometric">Alonso, Jose M., et al. “A Bibliometric Analysis of the Explainable Artificial Intelligence Research Field.” <i>International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems</i>, Springer, 2018, pp. 3–15.</span></li>
<li><span id="alvarez2018robustness">Alvarez-Melis, David, and Tommi S. Jaakkola. “On the Robustness of Interpretability Methods.” <i>ArXiv Preprint ArXiv:1806.08049</i>, 2018.</span></li>
<li><span id="alvarez2018towards">---. “Towards Robust Interpretability with Self-Explaining Neural Networks.” <i>ArXiv Preprint ArXiv:1806.07538</i>, 2018.</span></li>
<li><span id="besold2018and">Besold, Tarek R., and Sara L. Uckelman. “The What, the Why, and the How of Artificial Explanations in Automated Decision-Making.” <i>ArXiv Preprint ArXiv:1808.07074</i>, 2018.</span></li>
<li><span id="blandfort2018overview">Blandfort, Philipp, et al. “An Overview of Computational Approaches for Interpretation Analysis.” <i>ArXiv Preprint ArXiv:1811.04028</i>, 2018.</span></li>
<li><span id="chang2018explaining">Chang, Chun-Hao, et al. “Explaining Image Classifiers by Adaptive Dropout and Generative in-Filling.” <i>ArXiv Preprint ArXiv:1807.08024</i>, vol. 2, 2018.</span></li>
<li><span id="charles2018interpreting">Charles, Adam S. “Interpreting Deep Learning: The Machine Learning Rorschach Test?” <i>ArXiv Preprint ArXiv:1806.00148</i>, 2018.</span></li>
<li><span id="dhurandhar2018explanations">Dhurandhar, Amit, et al. “Explanations Based on the Missing: Towards Contrastive Explanations with Pertinent Negatives.” <i>ArXiv Preprint ArXiv:1802.07623</i>, 2018.</span></li>
<li><span id="dovsilovic2018explainable">Došilović, Filip Karlo, et al. “Explainable Artificial Intelligence: A Survey.” <i>2018 41st International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)</i>, IEEE, 2018, pp. 0210–15.</span></li>
<li><span id="elsayed2018adversarial">Elsayed, Gamaleldin F., et al. “Adversarial Reprogramming of Neural Networks.” <i>ArXiv Preprint ArXiv:1806.11146</i>, 2018.</span></li>
<li><span id="ge2018towards">Ge, Xiaoyu, et al. “Towards Explainable Inference about Object Motion Using Qualitative Reasoning.” <i>Sixteenth International Conference on Principles of Knowledge Representation and Reasoning</i>, 2018.</span></li>
<li><span id="gilpin2018explaining">Gilpin, Leilani H., et al. “Explaining Explanations: An Overview of Interpretability of Machine Learning.” <i>2018 IEEE 5th International Conference on Data Science and Advanced Analytics (DSAA)</i>, IEEE, 2018, pp. 80–89.</span></li>
<li><span id="gilpin2018explaininh">---. <i>Explaining Explanations: An Approach to Evaluating Interpretability of Machine Learning.(2018)</i>. 2018.</span></li>
<li><span id="grosz2018century">Grosz, Barbara J., and Peter Stone. “A Century-Long Commitment to Assessing Artificial Intelligence and Its Impact on Society.” <i>Communications of the ACM</i>, vol. 61, no. 12, ACM New York, NY, USA, 2018, pp. 68–73.</span></li>
<li><span id="guidotti2018survey">Guidotti, Riccardo, et al. “A Survey of Methods for Explaining Black Box Models.” <i>ACM Computing Surveys (CSUR)</i>, vol. 51, no. 5, ACM New York, NY, USA, 2018, pp. 1–42.</span></li>
<li><span id="harbecke2018learning">Harbecke, David, et al. “Learning Explanations from Language Data.” <i>ArXiv Preprint ArXiv:1808.04127</i>, 2018.</span></li>
<li><span id="hind2018increasing">Hind, Michael, et al. “Increasing Trust in AI Services through Supplier’s Declarations of Conformity.” <i>ArXiv Preprint ArXiv:1808.07261</i>, vol. 18, 2018, pp. 2813–69.</span></li>
<li><span id="hoffman2018explaining">Hoffman, Robert R., et al. “Explaining Explanation for ‘Explainable Ai.’” <i>Proceedings of the Human Factors and Ergonomics Society Annual Meeting</i>, vol. 62, no. 1, SAGE Publications Sage CA: Los Angeles, CA, 2018, pp. 197–201.</span></li>
<li><span id="hoffman2018metrics">---. “Metrics for Explainable AI: Challenges and Prospects.” <i>ArXiv Preprint ArXiv:1812.04608</i>, 2018.</span></li>
<li><span id="hohman2018visual">Hohman, Fred, et al. “Visual Analytics in Deep Learning: An Interrogative Survey for the next Frontiers.” <i>IEEE Transactions on Visualization and Computer Graphics</i>, vol. 25, no. 8, IEEE, 2018, pp. 2674–93.</span></li>
<li><span id="honegger2018shedding">Honegger, Milo. “Shedding Light on Black Box Machine Learning Algorithms: Development of an Axiomatic Framework to Assess the Quality of Methods That Explain Individual Predictions.” <i>ArXiv Preprint ArXiv:1808.05054</i>, 2018.</span></li>
<li><span id="hu2018locally">Hu, Linwei, et al. “Locally Interpretable Models and Effects Based on Supervised Partitioning (LIME-SUP).” <i>ArXiv Preprint ArXiv:1806.00663</i>, 2018.</span></li>
<li><span id="hu2018explainable">Hu, Ronghang, et al. “Explainable Neural Computation via Stack Neural Module Networks.” <i>Proceedings of the European Conference on Computer Vision (ECCV)</i>, 2018, pp. 53–69.</span></li>
<li><span id="iyer2018transparency">Iyer, Rahul, et al. “Transparency and Explanation in Deep Reinforcement Learning Neural Networks.” <i>Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society</i>, 2018, pp. 144–50.</span></li>
<li><span id="karim2018machine">Karim, Abdul, et al. “Machine Learning Interpretability: A Science Rather than a Tool.” <i>ArXiv Preprint ArXiv:1807.06722</i>, 2018.</span></li>
<li><span id="kleinerman2018providing">Kleinerman, Akiva, et al. “Providing Explanations for Recommendations in Reciprocal Environments.” <i>Proceedings of the 12th ACM Conference on Recommender Systems</i>, 2018, pp. 22–30.</span></li>
<li><span id="lage2018human">Lage, Isaac, et al. “Human-in-the-Loop Interpretability Prior.” <i>Advances in Neural Information Processing Systems</i>, vol. 31, NIH Public Access, 2018.</span></li>
<li><span id="lipton2018mythos">Lipton, Zachary C. “The Mythos of Model Interpretability: In Machine Learning, the Concept of Interpretability Is Both Important and Slippery.” <i>Queue</i>, vol. 16, no. 3, ACM New York, NY, USA, 2018, pp. 31–57.</span></li>
<li><span id="noothigattu2018interpretable">Noothigattu, Ritesh, et al. “Interpretable Multi-Objective Reinforcement Learning through Policy Orchestration.” <i>ArXiv Preprint ArXiv:1809.08343</i>, 2018.</span></li>
<li><span id="ouarti2018out">Ouarti, Nizar, and David Carmona. “Out of the Black Box: Properties of Deep Neural Networks and Their Applications.” <i>ArXiv Preprint ArXiv:1808.04433</i>, 2018.</span></li>
<li><span id="petsiuk2018rise">Petsiuk, Vitali, et al. “Rise: Randomized Input Sampling for Explanation of Black-Box Models.” <i>ArXiv Preprint ArXiv:1806.07421</i>, 2018.</span></li>
<li><span id="ras2018explanation">Ras, Gabriëlle, et al. “Explanation Methods in Deep Learning: Users, Values, Concerns and Challenges.” <i>Explainable and Interpretable Models in Computer Vision and Machine Learning</i>, Springer, 2018, pp. 19–36.</span></li>
<li><span id="takahashi2018interpretable">Takahashi, Ryo, et al. “Interpretable and Compositional Relation Learning by Joint Training with an Autoencoder.” <i>ArXiv Preprint ArXiv:1805.09547</i>, 2018.</span></li>
<li><span id="tomsett2018interpretable">Tomsett, Richard, et al. “Interpretable to Whom? A Role-Based Model for Analyzing Interpretable Machine Learning Systems.” <i>ArXiv Preprint ArXiv:1806.07552</i>, 2018.</span></li>
<li><span id="vaughan2018explainable">Vaughan, Joel, et al. “Explainable Neural Networks Based on Additive Index Models.” <i>ArXiv Preprint ArXiv:1806.01933</i>, 2018.</span></li>
<li><span id="velivckovic2018deep">Veličković, Petar, et al. “Deep Graph Infomax.” <i>ArXiv Preprint ArXiv:1809.10341</i>, 2018.</span></li>
<li><span id="wagstaff2018interpretable">Wagstaff, Kiri L., and Jake Lee. “Interpretable Discovery in Large Image Data Sets.” <i>ArXiv Preprint ArXiv:1806.08340</i>, 2018.</span></li>
<li><span id="xiang2018verification">Xiang, Weiming, et al. “Verification for Machine Learning, Autonomy, and Neural Networks Survey.” <i>ArXiv Preprint ArXiv:1810.01989</i>, 2018.</span></li>
<li><span id="zhang2018interpreting">Zhang, Quanshi, et al. “Interpreting Cnn Knowledge via an Explanatory Graph.” <i>Thirty-Second AAAI Conference on Artificial Intelligence</i>, 2018.</span></li>
<li><span id="zhang2018examining">---. “Examining Cnn Representations with Respect to Dataset Bias.” <i>Proceedings of the AAAI Conference on Artificial Intelligence</i>, vol. 32, no. 1, 2018.</span></li>
<li><span id="zhang2018interpretable">---. “Interpretable Convolutional Neural Networks.” <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i>, 2018, pp. 8827–36.</span></li>
<li><span id="zhang2018interpretablf">---. “Interpretable Convolutional Neural Networks.” <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i>, 2018, pp. 8827–36.</span></li>
<li><span id="zhang2018visual">Zhang, Quanshi, and Song-Chun Zhu. “Visual Interpretability for Deep Learning: a Survey.” <i>ArXiv Preprint ArXiv:1802.00614</i>, 2018.</span></li>
<li><span id="zhou2018interpreting">Zhou, Bolei, et al. “Interpreting Deep Visual Representations via Network Dissection.” <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, vol. 41, no. 9, IEEE, 2018, pp. 2131–45.</span></li></ol>
<h2 class="bibliography">2017</h2>
<ol class="bibliography"><li><span id="biran2017explanation">Biran, O., and C. Cotton. <i>Explanation and Justification in Ml: A Survey</i>. IJCAI, 2017.</span></li>
<li><span id="chakraborti2017ai">Chakraborti, Tathagata, et al. “AI Challenges in Human-Robot Cognitive Teaming.” <i>ArXiv Preprint ArXiv:1707.04775</i>, 2017.</span></li>
<li><span id="goodman2017european">Goodman, Bryce, and Seth Flaxman. “European Union Regulations on Algorithmic Decision-Making and a ‘Right to Explanation.’” <i>AI Magazine</i>, vol. 38, no. 3, 2017, pp. 50–57.</span></li>
<li><span id="lundberg2017unified">Lundberg, Scott M., and Su-In Lee. “A Unified Approach to Interpreting Model Predictions.” <i>Proceedings of the 31st International Conference on Neural Information Processing Systems</i>, 2017, pp. 4768–77.</span></li>
<li><span id="nematzadeh2017evaluating">Nematzadeh, Aida, et al. “Evaluating Vector-Space Models of Word Representation, or, The Unreasonable Effectiveness of Counting Words Near Other Words.” <i>CogSci</i>, 2017.</span></li>
<li><span id="riguzzi2017survey">Riguzzi, Fabrizio, et al. “A Survey of Lifted Inference Approaches for Probabilistic Logic Programming under the Distribution Semantics.” <i>International Journal of Approximate Reasoning</i>, vol. 80, Elsevier, 2017, pp. 313–33.</span></li></ol>
<h2 class="bibliography">2013</h2>
<ol class="bibliography"><li><span id="ulianov2013intelligent">Ulianov, Serghei. “Intelligent Self-Organized Robust Control Design Based on Quantum/Soft Computing Technologies and Kansei Engineering.” <i>Computer Science Journal of Moldova</i>, vol. 62, no. 2, 2013, pp. 242–79.</span></li></ol>
<h2 class="bibliography">2010</h2>
<ol class="bibliography"><li><span id="amershi2010examining">Amershi, Saleema, et al. “Examining Multiple Potential Models in End-User Interactive Concept Learning.” <i>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</i>, 2010, pp. 1357–60.</span></li></ol>
