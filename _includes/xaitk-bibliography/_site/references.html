<h1 id="references">References</h1>

<p>Zunino, A., Bargal, S. A., Morerio, P., Zhang, J., Sclaroff, S., &amp; Murino, V. Excitation dropout: Encouraging plasticity in deep neural networks. <i>International Journal of Computer Vision</i>, <i>129</i>(4), 1139–1152.</p>

<p>Alipour, K., Schulze, J. P., Yao, Y., Ziskind, A., &amp; Burachas, G. A study on multimodal and interactive explanations for visual question answering. <i>ArXiv Preprint ArXiv:2003.00431</i>.</p>

<p>Chakraborti, T., Sreedharan, S., &amp; Kambhampati, S. The emerging landscape of explainable ai planning and decision making. <i>ArXiv Preprint ArXiv:2002.11697</i>.</p>

<p>Chen, Z., Bei, Y., &amp; Rudin, C. Concept whitening for interpretable image recognition. <i>Nature Machine Intelligence</i>, <i>2</i>(12), 772–782.</p>

<p>Ehsan, U., &amp; Riedl, M. O. Human-centered explainable ai: Towards a reflective sociotechnical approach. <i>International Conference on Human-Computer Interaction</i>, 449–466.</p>

<p>Elton, D. C. Self-explaining AI as an alternative to interpretable AI. <i>International Conference on Artificial General Intelligence</i>, 95–106.</p>

<p>Fan, F.-L., Xiong, J., Li, M., &amp; Wang, G. On interpretability of artificial neural networks: A survey. <i>IEEE Transactions on Radiation and Plasma Medical Sciences</i>.</p>

<p>Guidotti, R., Monreale, A., Matwin, S., &amp; Pedreschi, D. Black box explanation by learning image exemplars in the latent feature space. <i>ArXiv Preprint ArXiv:2002.03746</i>.</p>

<p>Janizek, J. D., Sturmfels, P., &amp; Lee, S.-I. Explaining explanations: Axiomatic feature interactions for deep networks. <i>Journal of Machine Learning Research</i>, <i>22</i>(104), 1–54.</p>

<p>Jung, A., &amp; Nardelli, P. H. J. An information-theoretic approach to personalized explainable machine learning. <i>IEEE Signal Processing Letters</i>, <i>27</i>, 825–829.</p>

<p>Kuźba, M., &amp; Biecek, P. What Would You Ask the Machine Learning Model? Identification of User Needs for Model Explanations Based on Human-Model Conversations. <i>Joint European Conference on Machine Learning and Knowledge Discovery in Databases</i>, 447–459.</p>

<p>Madumal, P., Miller, T., Sonenberg, L., &amp; Vetere, F. Distal explanations for explainable reinforcement learning agents. <i>ArXiv Preprint ArXiv:2001.10284</i>.</p>

<p>Mai, T., Khanna, R., Dodge, J., Irvine, J., Lam, K.-H., Lin, Z., Kiddle, N., Newman, E., Raja, S., Matthews, C., &amp; others. Keeping it” organized and logical” after-action review for AI (AAR/AI). <i>Proceedings of the 25th International Conference on Intelligent User Interfaces</i>, 465–476.</p>

<p>Morichetta, A., Casas, P., &amp; Mellia, M. EXPLAIN-IT: towards explainable AI for unsupervised network traffic analysis. <i>Proceedings of the 3rd ACM CoNEXT Workshop on Big DAta, Machine Learning and Artificial Intelligence for Data Communication Networks</i>, 22–28.</p>

<p>Patir, R., Singhal, S., Anantaram, C., &amp; Goyal, V. Interpretability of Black Box Models Through Data-View Extraction and Shadow Model Creation. <i>International Conference on Neural Information Processing</i>, 378–385.</p>

<p>Phillips, P. J., &amp; Przybocki, M. Four principles of explainable AI as applied to biometrics and facial forensic algorithms. <i>ArXiv Preprint ArXiv:2002.01014</i>.</p>

<p>Ulianov, S. Intelligent self-organized robust control design based on quantum/soft computing technologies and Kansei Engineering. <i>Computer Science Journal of Moldova</i>, <i>62</i>(2), 242–279.</p>

<p>Schrills, T., &amp; Franke, T. How to Answer Why–Evaluating the Explanations of AI Through Mental Model Analysis. <i>ArXiv Preprint ArXiv:2002.02526</i>.</p>

<p>Nematzadeh, A., Meylan, S. C., &amp; Griffiths, T. L. Evaluating Vector-Space Models of Word Representation, or, The Unreasonable Effectiveness of Counting Words Near Other Words. <i>CogSci</i>.</p>

<p>Tuckey, D., Russo, A., &amp; Broda, K. A general framework for scientifically inspired explanations in AI. <i>ArXiv Preprint ArXiv:2003.00749</i>.</p>

<p>Visani, G., Bagli, E., Chesani, F., Poluzzi, A., &amp; Capuzzo, D. Statistical stability indices for LIME: obtaining reliable explanations for machine learning models. <i>Journal of the Operational Research Society</i>, 1–11.</p>

<p>Arrieta, A. B., Dı́az-Rodrı́guez Natalia, Del Ser, J., Bennetot, A., Tabik, S., Barbado, A., Garcı́a Salvador, Gil-López, S., Molina, D., Benjamins, R., &amp; others. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. <i>Information Fusion</i>, <i>58</i>, 82–115.</p>

<p>Artelt, A., &amp; Hammer, B. On the computation of counterfactual explanations–A survey. <i>ArXiv Preprint ArXiv:1911.07749</i>.</p>

<p>Atrey, A., Clary, K., &amp; Jensen, D. Exploratory not explanatory: Counterfactual analysis of saliency maps for deep reinforcement learning. <i>ArXiv Preprint ArXiv:1912.05743</i>.</p>

<p>Blandfort, P., Hees, J., &amp; Patton, D. U. An Overview of Computational Approaches for Interpretation Analysis. <i>ArXiv Preprint ArXiv:1811.04028</i>.</p>

<p>Guidotti, R., Monreale, A., Matwin, S., &amp; Pedreschi, D. Black box explanation by learning image exemplars in the latent feature space. <i>ArXiv Preprint ArXiv:2002.03746</i>.</p>

<p>Chapman-Rounds, M., Schulz, M.-A., Pazos, E., &amp; Georgatzis, K. EMAP: Explanation by Minimal Adversarial Perturbation. <i>ArXiv Preprint ArXiv:1912.00872</i>.</p>

<p>Cheng, T. Interpretability Study on Deep Learning for Jet Physics at the Large Hadron Collider. <i>ArXiv Preprint ArXiv:1911.01872</i>.</p>

<p>Du, M., Liu, N., &amp; Hu, X. Techniques for interpretable machine learning. <i>Communications of the ACM</i>, <i>63</i>(1), 68–77.</p>

<p>Edmonds, M., Gao, F., Liu, H., Xie, X., Qi, S., Rothrock, B., Zhu, Y., Wu, Y. N., Lu, H., &amp; Zhu, S.-C. A tale of two explanations: Enhancing human trust by explaining robot behavior. <i>Science Robotics</i>, <i>4</i>(37).</p>

<p>Fadnis, K., Talamadupula, K., Kapanipathi, P., Ishfaq, H., Roukos, S., &amp; Fokoue, A. Heuristics for interpretable knowledge graph contextualization. <i>ArXiv Preprint ArXiv:1911.02085</i>.</p>

<p>Gilpin, L. H., Bau, D., Yuan, B. Z., Bajwa, A., Specter, M., &amp; Kagal, L. Explaining explanations: An overview of interpretability of machine learning. <i>2018 IEEE 5th International Conference on Data Science and Advanced Analytics (DSAA)</i>, 80–89.</p>

<p>Goyal, Y., Wu, Z., Ernst, J., Batra, D., Parikh, D., &amp; Lee, S. Counterfactual visual explanations. <i>International Conference on Machine Learning</i>, 2376–2384.</p>

<p>Došilović, F. K., Brčić, M., &amp; Hlupić, N. Explainable artificial intelligence: A survey. <i>2018 41st International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)</i>, 0210–0215.</p>

<p>Hoffman, R. R., Mueller, S. T., Klein, G., &amp; Litman, J. Metrics for explainable AI: Challenges and prospects. <i>ArXiv Preprint ArXiv:1812.04608</i>.</p>

<p>Islam, S. R., Eberle, W., &amp; Ghafoor, S. K. Towards quantification of explainability in explainable artificial intelligence methods. <i>The Thirty-Third International Flairs Conference</i>.</p>

<p>Juozapaitis, Z., Koul, A., Fern, A., Erwig, M., &amp; Doshi-Velez, F. Explainable reinforcement learning via reward decomposition. <i>IJCAI/ECAI Workshop on Explainable Artificial Intelligence</i>.</p>

<p>Keane, M. T., &amp; Kenny, E. M. How case-based reasoning explains neural networks: A theoretical analysis of XAI using post-hoc explanation-by-example from a survey of ANN-CBR twin-systems. <i>International Conference on Case-Based Reasoning</i>, 155–171.</p>

<p>Zafar, M. R., &amp; Khan, N. M. DLIME: A deterministic local interpretable model-agnostic explanations approach for computer-aided diagnosis systems. <i>ArXiv Preprint ArXiv:1906.10263</i>.</p>

<p>Lakkaraju, H., &amp; Bastani, O. “ How do I fool you?” Manipulating User Trust via Misleading Black Box Explanations. <i>Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society</i>, 79–85.</p>

<p>Le, T., Wang, S., &amp; Lee, D. Why X rather than Y? Explaining Neural Model’Predictions by Generating Intervention Counterfactual Samples. <i>CoRR</i>.</p>

<p>Li, X., Serlin, Z., Yang, G., &amp; Belta, C. A formal methods approach to interpretable reinforcement learning for robotic planning. <i>Science Robotics</i>, <i>4</i>(37).</p>

<p>Licato, J., Marji, Z., &amp; Abraham, S. Scenarios and recommendations for ethical interpretive ai. <i>ArXiv Preprint ArXiv:1911.01917</i>.</p>

<p>Liu, Y.-C., Hsieh, Y.-A., Chen, M.-H., Yang, C.-H. H., Tegner, J., &amp; Tsai, Y.-C. J. Interpretable self-attention temporal reasoning for driving behavior understanding. <i>ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>, 2338–2342.</p>

<p>Liu, W., Li, R., Zheng, M., Karanam, S., Wu, Z., Bhanu, B., Radke, R. J., &amp; Camps, O. Towards visually explaining variational autoencoders. <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 8642–8651.</p>

<p>Lucic, A., Oosterhuis, H., Haned, H., &amp; de Rijke, M. FOCUS: Flexible optimizable counterfactual explanations for tree ensembles. <i>ArXiv Preprint ArXiv:1911.12199</i>.</p>

<p>Mahajan, D., Tan, C., &amp; Sharma, A. Preserving causal constraints in counterfactual explanations for machine learning classifiers. <i>ArXiv Preprint ArXiv:1912.03277</i>.</p>

<p>Miller, T. Explanation in artificial intelligence: Insights from the social sciences. <i>Artificial Intelligence</i>, <i>267</i>, 1–38.</p>

<p>Mittelstadt, B., Russell, C., &amp; Wachter, S. Explaining explanations in AI. <i>Proceedings of the Conference on Fairness, Accountability, and Transparency</i>, 279–288.</p>

<p>Mundhenk, T. N., Chen, B. Y., &amp; Friedland, G. Efficient saliency maps for Explainable AI. <i>ArXiv Preprint ArXiv:1911.11293</i>.</p>

<p>Saralajew, S., Holdijk, L., &amp; Villmann, T. Fast Adversarial Robustness Certification of Nearest Prototype Classifiers for Arbitrary Seminorms. <i>NeurIPS</i>.</p>

<p>Zhang, Q., Wu, Y. N., &amp; Zhu, S.-C. Interpretable convolutional neural networks. <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i>, 8827–8836.</p>

<p>Putnam, V. <i>Toward XAI for Intelligent Tutoring Systems: a case study</i> [PhD thesis]. University of British Columbia.</p>

<p>Ramon, Y., Martens, D., Provost, F., &amp; Evgeniou, T. Counterfactual explanation algorithms for behavioral and textual data. <i>ArXiv Preprint ArXiv:1912.01819</i>.</p>

<p>Schölkopf, B. Causality for machine learning. <i>ArXiv Preprint ArXiv:1911.10500</i>.</p>

<p>Slack, D., Hilgard, S., Jia, E., Singh, S., &amp; Lakkaraju, H. Fooling lime and shap: Adversarial attacks on post hoc explanation methods. <i>Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society</i>, 180–186.</p>

<p>Sokol, K., &amp; Flach, P. Explainability fact sheets: a framework for systematic assessment of explainable approaches. <i>Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</i>, 56–67.</p>

<p>Cui, W., Zhan, W., Yu, J., Sun, C., &amp; Zhang, Y. Face recognition via convolutional neural networks and siamese neural networks. <i>2019 International Conference on Intelligent Computing, Automation and Systems (ICICAS)</i>, 746–750.</p>

<p>Zhang, Y., Niebles, J. C., &amp; Soto, A. Interpretable visual question answering by visual grounding from attention supervision mining. <i>2019 Ieee Winter Conference on Applications of Computer Vision (Wacv)</i>, 349–357.</p>

<p>Zhang, X., Marwah, M., Lee, I.-ta, Arlitt, M., &amp; Goldwasser, D. <i>An Anomaly Contribution Explainer for Cyber-Security Applications</i>.</p>

<p>Zhang, H., Chen, J., Xue, H., &amp; Zhang, Q. Towards a unified evaluation of explanation methods without ground truth. <i>ArXiv Preprint ArXiv:1911.09017</i>.</p>

<p>Abdul, A., Vermeulen, J., Wang, D., Lim, B. Y., &amp; Kankanhalli, M. Trends and trajectories for explainable, accountable and intelligible systems: An hci research agenda. <i>Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</i>, 1–18.</p>

<p>Alonso, J. M., Castiello, C., &amp; Mencar, C. A bibliometric analysis of the explainable artificial intelligence research field. <i>International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems</i>, 3–15.</p>

<p>Alvarez-Melis, D., &amp; Jaakkola, T. S. On the robustness of interpretability methods. <i>ArXiv Preprint ArXiv:1806.08049</i>.</p>

<p>Alvarez-Melis, D., &amp; Jaakkola, T. S. Towards robust interpretability with self-explaining neural networks. <i>ArXiv Preprint ArXiv:1806.07538</i>.</p>

<p>Annasamy, R. M., &amp; Sycara, K. Towards better interpretability in deep q-networks. <i>Proceedings of the AAAI Conference on Artificial Intelligence</i>, <i>33</i>(01), 4561–4569.</p>

<p>Besold, T. R., &amp; Uckelman, S. L. The what, the why, and the how of artificial explanations in automated decision-making. <i>ArXiv Preprint ArXiv:1808.07074</i>.</p>

<p>Chang, C.-H., Creager, E., Goldenberg, A., &amp; Duvenaud, D. Explaining image classifiers by adaptive dropout and generative in-filling. <i>ArXiv Preprint ArXiv:1807.08024</i>, <i>2</i>.</p>

<p>Charles, A. S. Interpreting deep learning: The machine learning rorschach test? <i>ArXiv Preprint ArXiv:1806.00148</i>.</p>

<p>Dhurandhar, A., Chen, P.-Y., Luss, R., Tu, C.-C., Ting, P., Shanmugam, K., &amp; Das, P. Explanations based on the missing: Towards contrastive explanations with pertinent negatives. <i>ArXiv Preprint ArXiv:1802.07623</i>.</p>

<p>Elsayed, G. F., Goodfellow, I., &amp; Sohl-Dickstein, J. Adversarial reprogramming of neural networks. <i>ArXiv Preprint ArXiv:1806.11146</i>.</p>

<p>Ge, X., Renz, J., &amp; Hua, H. Towards explainable inference about object motion using qualitative reasoning. <i>Sixteenth International Conference on Principles of Knowledge Representation and Reasoning</i>.</p>

<p>Gilpin, L. H., Bau, D., Yuan, B. Z., Bajwa, A., Specter, M., &amp; Kagal, L. <i>Explaining Explanations: An Approach to Evaluating Interpretability of Machine Learning.(2018)</i>.</p>

<p>Grosz, B. J., &amp; Stone, P. A century-long commitment to assessing artificial intelligence and its impact on society. <i>Communications of the ACM</i>, <i>61</i>(12), 68–73.</p>

<p>Guidotti, R., Monreale, A., Ruggieri, S., Turini, F., Giannotti, F., &amp; Pedreschi, D. A survey of methods for explaining black box models. <i>ACM Computing Surveys (CSUR)</i>, <i>51</i>(5), 1–42.</p>

<p>Guo, W. Explainable artificial intelligence for 6G: Improving trust between human and machine. <i>IEEE Communications Magazine</i>, <i>58</i>(6), 39–45.</p>

<p>Harbecke, D., Schwarzenberg, R., &amp; Alt, C. Learning explanations from language data. <i>ArXiv Preprint ArXiv:1808.04127</i>.</p>

<p>Hind, M., Mehta, S., Mojsilovic, A., Nair, R., Ramamurthy, K. N., Olteanu, A., &amp; Varshney, K. R. Increasing trust in AI services through supplier’s declarations of conformity. <i>ArXiv Preprint ArXiv:1808.07261</i>, <i>18</i>, 2813–2869.</p>

<p>Hoffman, R. R., Klein, G., &amp; Mueller, S. T. Explaining explanation for “explainable ai.” <i>Proceedings of the Human Factors and Ergonomics Society Annual Meeting</i>, <i>62</i>(1), 197–201.</p>

<p>Hohman, F., Kahng, M., Pienta, R., &amp; Chau, D. H. Visual analytics in deep learning: An interrogative survey for the next frontiers. <i>IEEE Transactions on Visualization and Computer Graphics</i>, <i>25</i>(8), 2674–2693.</p>

<p>Honegger, M. Shedding light on black box machine learning algorithms: Development of an axiomatic framework to assess the quality of methods that explain individual predictions. <i>ArXiv Preprint ArXiv:1808.05054</i>.</p>

<p>Hossain, M. D. Z., Sohel, F., Shiratuddin, M. F., &amp; Laga, H. A comprehensive survey of deep learning for image captioning. <i>ACM Computing Surveys (CsUR)</i>, <i>51</i>(6), 1–36.</p>

<p>Hu, R., Andreas, J., Darrell, T., &amp; Saenko, K. Explainable neural computation via stack neural module networks. <i>Proceedings of the European Conference on Computer Vision (ECCV)</i>, 53–69.</p>

<p>Hu, L., Chen, J., Nair, V. N., &amp; Sudjianto, A. Locally interpretable models and effects based on supervised partitioning (LIME-SUP). <i>ArXiv Preprint ArXiv:1806.00663</i>.</p>

<p>Schwalbe, G., &amp; Schels, M. A survey on methods for the safety assurance of machine learning based systems. <i>10th European Congress on Embedded Real Time Software and Systems (ERTS 2020)</i>.</p>

<p>Iyer, R., Li, Y., Li, H., Lewis, M., Sundar, R., &amp; Sycara, K. Transparency and explanation in deep reinforcement learning neural networks. <i>Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society</i>, 144–150.</p>

<p>Karim, A., Mishra, A., Newton, M. A., &amp; Sattar, A. Machine Learning Interpretability: A Science rather than a tool. <i>ArXiv Preprint ArXiv:1807.06722</i>.</p>

<p>Kleinerman, A., Rosenfeld, A., &amp; Kraus, S. Providing explanations for recommendations in reciprocal environments. <i>Proceedings of the 12th ACM Conference on Recommender Systems</i>, 22–30.</p>

<p>Lage, I., Ross, A. S., Kim, B., Gershman, S. J., &amp; Doshi-Velez, F. Human-in-the-loop interpretability prior. <i>Advances in Neural Information Processing Systems</i>, <i>31</i>.</p>

<p>Miller, T. Explanation in artificial intelligence: Insights from the social sciences. <i>Artificial Intelligence</i>, <i>267</i>, 1–38.</p>

<p>Noothigattu, R., Bouneffouf, D., Mattei, N., Chandra, R., Madan, P., Varshney, K., Campbell, M., Singh, M., &amp; Rossi, F. Interpretable multi-objective reinforcement learning through policy orchestration. <i>ArXiv Preprint ArXiv:1809.08343</i>.</p>

<p>Odena, A., Olsson, C., Andersen, D., &amp; Goodfellow, I. Tensorfuzz: Debugging neural networks with coverage-guided fuzzing. <i>International Conference on Machine Learning</i>, 4901–4911.</p>

<p>Ouarti, N., &amp; Carmona, D. Out of the Black Box: Properties of deep neural networks and their applications. <i>ArXiv Preprint ArXiv:1808.04433</i>.</p>

<p>Park, Y.-J., &amp; Choi, H.-L. InfoSSM: Interpretable Unsupervised Learning of Nonparametric State-Space Model for Multi-modal Dynamics. <i>AIAA Scitech 2019 Forum</i>, 0681.</p>

<p>Petsiuk, V., Das, A., &amp; Saenko, K. Rise: Randomized input sampling for explanation of black-box models. <i>ArXiv Preprint ArXiv:1806.07421</i>.</p>

<p>Ras, G., van Gerven, M., &amp; Haselager, P. Explanation methods in deep learning: Users, values, concerns and challenges. In <i>Explainable and interpretable models in computer vision and machine learning</i> (pp. 19–36). Springer.</p>

<p>Seo, D., Oh, K., &amp; Oh, I.-S. Regional multi-scale approach for visually pleasing explanations of deep neural networks. <i>IEEE Access</i>, <i>8</i>, 8572–8582.</p>

<p>Takahashi, R., Tian, R., &amp; Inui, K. Interpretable and compositional relation learning by joint training with an autoencoder. <i>ArXiv Preprint ArXiv:1805.09547</i>.</p>

<p>Tomsett, R., Braines, D., Harborne, D., Preece, A., &amp; Chakraborty, S. Interpretable to whom? A role-based model for analyzing interpretable machine learning systems. <i>ArXiv Preprint ArXiv:1806.07552</i>.</p>

<p>Vaughan, J., Sudjianto, A., Brahimi, E., Chen, J., &amp; Nair, V. N. Explainable neural networks based on additive index models. <i>ArXiv Preprint ArXiv:1806.01933</i>.</p>

<p>Veličković, P., Fedus, W., Hamilton, W. L., Liò, P., Bengio, Y., &amp; Hjelm, R. D. Deep graph infomax. <i>ArXiv Preprint ArXiv:1809.10341</i>.</p>

<p>Wagstaff, K. L., &amp; Lee, J. Interpretable discovery in large image data sets. <i>ArXiv Preprint ArXiv:1806.08340</i>.</p>

<p>Xiang, W., Musau, P., Wild, A. A., Lopez, D. M., Hamilton, N., Yang, X., Rosenfeld, J., &amp; Johnson, T. T. Verification for machine learning, autonomy, and neural networks survey. <i>ArXiv Preprint ArXiv:1810.01989</i>.</p>

<p>Zhang, Q., &amp; Zhu, S.-C. Visual interpretability for deep learning: a survey. <i>ArXiv Preprint ArXiv:1802.00614</i>.</p>

<p>Zhang, Q., Wang, W., &amp; Zhu, S.-C. Examining cnn representations with respect to dataset bias. <i>Proceedings of the AAAI Conference on Artificial Intelligence</i>, <i>32</i>(1).</p>

<p>Zhang, Q., Wu, Y. N., &amp; Zhu, S.-C. Interpretable convolutional neural networks. <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i>, 8827–8836.</p>

<p>Zhou, B., Bau, D., Oliva, A., &amp; Torralba, A. Interpreting deep visual representations via network dissection. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, <i>41</i>(9), 2131–2145.</p>

<p>Biran, O., &amp; Cotton, C. <i>Explanation and justification in ml: A survey</i>.</p>

<p>Chakraborti, T., Kambhampati, S., Scheutz, M., &amp; Zhang, Y. AI challenges in human-robot cognitive teaming. <i>ArXiv Preprint ArXiv:1707.04775</i>.</p>

<p>Goodman, B., &amp; Flaxman, S. European Union regulations on algorithmic decision-making and a “right to explanation.” <i>AI Magazine</i>, <i>38</i>(3), 50–57.</p>

<p>Lipton, Z. C. The Mythos of Model Interpretability: In machine learning, the concept of interpretability is both important and slippery. <i>Queue</i>, <i>16</i>(3), 31–57.</p>

<p>Lundberg, S. M., &amp; Lee, S.-I. A unified approach to interpreting model predictions. <i>Proceedings of the 31st International Conference on Neural Information Processing Systems</i>, 4768–4777.</p>

<p>Zhang, Q., Cao, R., Shi, F., Wu, Y. N., &amp; Zhu, S.-C. Interpreting cnn knowledge via an explanatory graph. <i>Thirty-Second AAAI Conference on Artificial Intelligence</i>.</p>

<p>Riguzzi, F., Bellodi, E., Zese, R., Cota, G., &amp; Lamma, E. A survey of lifted inference approaches for probabilistic logic programming under the distribution semantics. <i>International Journal of Approximate Reasoning</i>, <i>80</i>, 313–333.</p>

<p>Amershi, S., Fogarty, J., Kapoor, A., &amp; Tan, D. Examining multiple potential models in end-user interactive concept learning. <i>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</i>, 1357–1360.</p>
