---
---
References
==========

@article{zunino2021excitation,
 abstract = {We propose a guided dropout regularizer for deep networks based on the evidence of a network prediction defined as the firing of neurons in specific paths. In this work, we utilize the evidence at each neuron to determine the probability of dropout, rather than dropping out neurons uniformly at random as in standard dropout. In essence, we dropout with higher probability those neurons which contribute more to decision making at training time. This approach penalizes high saliency neurons that are most relevant for model prediction, ie},
 author = {Zunino, Andrea and Bargal, Sarah Adel and Morerio, Pietro and Zhang, Jianming and Sclaroff, Stan and Murino, Vittorio},
 journal = {International Journal of Computer Vision},
 number = {4},
 pages = {1139--1152},
 year = {2021},
 publisher = {Springer},
 title = {Excitation dropout: Encouraging plasticity in deep neural networks},
 venue = {International Journal of …},
 volume = {129}
}

@article{alipour2020study,
 abstract = {Explainability and interpretability of AI models is an essential factor affecting the safety of AI. While various explainable AI (XAI) approaches aim at mitigating the lack of transparency in deep networks, the evidence of the effectiveness of these approaches in improving usability, trust, and understanding of AI systems are still missing. We evaluate multimodal explanations in the setting of a Visual Question Answering (VQA) task, by asking users to predict the response accuracy of a VQA agent with and without explanations. We use},
 author = {Alipour, Kamran and Schulze, Jurgen P and Yao, Yi and Ziskind, Avi and Burachas, Giedrius},
 journal = {arXiv preprint arXiv:2003.00431},
 year = {2020},
 title = {A study on multimodal and interactive explanations for visual question answering},
 venue = {arXiv preprint arXiv …}
}

@article{chakraborti2020emerging,
 abstract = {[3] Dan Bryce, J Benton, and Michael W Boldt. Maintain- ing Evolving Domain Models. In IJCAI, 2016  Security? The Emerging Landscape of Inter- pretable Agent Behavior  How Explainable Plans Can Make Planning Faster. In IJCAI Workshop on Explainable AI (XAI), 2018},
 author = {Chakraborti, Tathagata and Sreedharan, Sarath and Kambhampati, Subbarao},
 journal = {arXiv preprint arXiv:2002.11697},
 year = {2020},
 title = {The emerging landscape of explainable ai planning and decision making},
 venue = {arXiv preprint arXiv …}
}

@article{chen2020concept,
 abstract = {What does a neural network encode about a concept as we traverse through the layers? Interpretability in machine learning is undoubtedly important, but the calculations of neural networks are very challenging to understand. Attempts to see inside their hidden layers can be misleading, unusable or rely on the latent space to possess properties that it may not have. Here, rather than attempting to analyse a neural network post hoc, we introduce a mechanism, called concept whitening (CW), to alter a given layer of the network to allow us},
 author = {Chen, Zhi and Bei, Yijie and Rudin, Cynthia},
 journal = {Nature Machine Intelligence},
 number = {12},
 pages = {772--782},
 year = {2020},
 publisher = {Nature Publishing Group},
 title = {Concept whitening for interpretable image recognition},
 venue = {Nature Machine Intelligence},
 volume = {2}
}

@inproceedings{ehsan2020human,
 abstract = {Explanations—a form of post-hoc interpretability—play an instrumental role in making systems accessible as AI continues to proliferate complex and sensitive sociotechnical systems. In this paper, we introduce Human-centered Explainable AI (HCXAI) as an approach that puts the human at the center of technology design. It develops a holistic understanding of “who” the human is by considering the interplay of values, interpersonal dynamics, and the socially situated nature of AI systems. In particular, we advocate for a},
 author = {Ehsan, Upol and Riedl, Mark O},
 booktitle = {International Conference on Human-Computer Interaction},
 organization = {Springer},
 pages = {449--466},
 year = {2020},
 title = {Human-centered explainable ai: Towards a reflective sociotechnical approach},
 venue = {International Conference on Human-Computer …}
}

@inproceedings{elton2020self,
 abstract = {The ability to explain decisions made by AI systems is highly sought after, especially in domains where human lives are at stake such as medicine or autonomous vehicles. While it is often possible to approximate the input-output relations of deep neural networks with a few human-understandable rules, the discovery of the double descent phenomena suggests that such approximations do not accurately capture the mechanism by which deep neural networks work. Double descent indicates that deep neural networks typically operate by},
 author = {Elton, Daniel C},
 booktitle = {International Conference on Artificial General Intelligence},
 organization = {Springer},
 pages = {95--106},
 year = {2020},
 title = {Self-explaining AI as an alternative to interpretable AI},
 venue = {International Conference on Artificial General …}
}

@article{fan2021interpretability,
 abstract = {Deep learning as represented by the artificial deep neural networks (DNNs) has achieved great success recently in many important areas that deal with text, images, videos, graphs, and so on. However, the black-box nature of DNNs has become one of the primary obstacles for their wide adoption in mission-critical applications such as medical diagnosis and therapy. Because of the huge potentials of deep learning, increasing the interpretability of deep neural networks has recently attracted much research attention. In this paper, we},
 author = {Fan, Feng-Lei and Xiong, Jinjun and Li, Mengzhou and Wang, Ge},
 journal = {IEEE Transactions on Radiation and Plasma Medical Sciences},
 year = {2021},
 publisher = {IEEE},
 title = {On interpretability of artificial neural networks: A survey},
 venue = {IEEE Transactions on …}
}

@article{guidotti2020black,
 abstract = {We present an approach to explain the decisions of black box models for image classification. While using the black box to label images, our explanation method exploits the latent feature space learned through an adversarial autoencoder. The proposed method},
 author = {Guidotti, Riccardo and Monreale, Anna and Matwin, Stan and Pedreschi, Dino},
 journal = {arXiv preprint arXiv:2002.03746},
 year = {2020},
 title = {Black box explanation by learning image exemplars in the latent feature space},
 venue = {arXiv preprint arXiv …}
}

@article{janizek2021explaining,
 abstract = {Recent work has shown great promise in explaining neural network behavior. In particular, feature attribution methods explain the features that are important to a model's prediction on a given input. However, for many tasks, simply identifying significant features may be insufficient for understanding model behavior. The interactions between features within the model may better explain not only the model, but why certain features outrank others in importance. In this work, we present Integrated Hessians, an extension of Integrated},
 author = {Janizek, Joseph D and Sturmfels, Pascal and Lee, Su-In},
 journal = {Journal of Machine Learning Research},
 number = {104},
 pages = {1--54},
 year = {2021},
 title = {Explaining explanations: Axiomatic feature interactions for deep networks},
 venue = {Journal of Machine Learning Research},
 volume = {22}
}

@article{jung2020information,
 abstract = {JUNG AND NARDELLI: INFORMATION-THEORETIC APPROACH TO PERSONALIZED EXPLAINABLE MACHINE LEARNING  MJ Wainwright, and MI Jordan, “Learning to explain: An information-theoretic perspective on  A Guide for Making Black Box Models Explainable, 2019},
 author = {Jung, Alexander and Nardelli, Pedro HJ},
 journal = {IEEE Signal Processing Letters},
 pages = {825--829},
 year = {2020},
 publisher = {IEEE},
 title = {An information-theoretic approach to personalized explainable machine learning},
 venue = {IEEE Signal Processing Letters},
 volume = {27}
}

@inproceedings{kuzba2020would,
 abstract = {how to improve – actionable queries for maximizing the prediction, eg what should I do to survive  You can download the model from the Open image in new window [6] database with a  We would like to thank 3 anonymous reviewers for their insightful comments and suggestions},
 author = {Ku{\'z}ba, Micha{\l} and Biecek, Przemys{\l}aw},
 booktitle = {Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
 organization = {Springer},
 pages = {447--459},
 year = {2020},
 title = {What Would You Ask the Machine Learning Model? Identification of User Needs for Model Explanations Based on Human-Model Conversations},
 venue = {Joint European Conference on Machine Learning …}
}

@article{madumal2020distal,
 abstract = {2 RELATED WORK Explaining the decisions and policies of autonomous agents has been explored widely, with the focus often given to  Distal Explanations for Explainable Reinforcement Learning Agents  Minimal Sufficient Explanations for Factored Markov Decision Processes},
 author = {Madumal, Prashan and Miller, Tim and Sonenberg, Liz and Vetere, Frank},
 journal = {arXiv preprint arXiv:2001.10284},
 year = {2020},
 title = {Distal explanations for explainable reinforcement learning agents},
 venue = {arXiv preprint arXiv …}
}

@inproceedings{mai2020keeping,
 abstract = {Explainable AI (XAI) is growing in importance as AI pervades modern society, but few have studied how XAI can directly support people trying to assess an AI agent. Without a rigorous process, people may approach assessment in ad hoc ways---leading to the possibility of wide variations in assessment of the same agent due only to variations in their processes. AAR, or After-Action Review, is a method some military organizations use to assess human agents, and it has been validated in many domains. Drawing upon this strategy, we derived},
 author = {Mai, Theresa and Khanna, Roli and Dodge, Jonathan and Irvine, Jed and Lam, Kin-Ho and Lin, Zhengxian and Kiddle, Nicholas and Newman, Evan and Raja, Sai and Matthews, Caleb and others},
 booktitle = {Proceedings of the 25th International Conference on Intelligent User Interfaces},
 pages = {465--476},
 year = {2020},
 title = {Keeping it" organized and logical" after-action review for AI (AAR/AI)},
 venue = {Proceedings of the 25th …}
}

@inproceedings{morichetta2019explain,
 abstract = {Cluster C1 contains mostly LD and SD YouTube sessions; cluster C0 is more shifted towards SD and HD, while HD sessions predominantly characterize cluster C2  27 Page 7. EXPLAIN-IT: XAI for Unsupervised Network Traffic Analysis},
 author = {Morichetta, Andrea and Casas, Pedro and Mellia, Marco},
 booktitle = {Proceedings of the 3rd ACM CoNEXT Workshop on Big DAta, Machine Learning and Artificial Intelligence for Data Communication Networks},
 pages = {22--28},
 year = {2019},
 title = {EXPLAIN-IT: towards explainable AI for unsupervised network traffic analysis},
 venue = {… for Data Communication Networks}
}

@inproceedings{patir2020interpretability,
 abstract = {A., Specter, M., Kagal, L.: Explaining explanations: an overview of interpretability of machine  Agarwal, P., Verma, S., Krishnamurthy, B.: MAGIX: model agnostic globally interpretable explanations  M., Rastogi, M.: Using formal concept analysis to explain black box deep learning},
 author = {Patir, Rupam and Singhal, Shubham and Anantaram, C and Goyal, Vikram},
 booktitle = {International Conference on Neural Information Processing},
 organization = {Springer},
 pages = {378--385},
 year = {2020},
 title = {Interpretability of Black Box Models Through Data-View Extraction and Shadow Model Creation},
 venue = {International Conference on …}
}

@article{phillips2020four,
 abstract = {Traditionally, researchers in automatic face recognition and biometric technologies have focused on developing accurate algorithms. With this technology being integrated into operational systems, engineers and scientists are being asked, do these systems meet societal norms? The origin of this line of inquiry istrust'of artificial intelligence (AI) systems. In this paper, we concentrate on adapting explainable AI to face recognition and biometrics, and we present four principles of explainable AI to face recognition and biometrics. The},
 author = {Phillips, P Jonathon and Przybocki, Mark},
 journal = {arXiv preprint arXiv:2002.01014},
 year = {2020},
 title = {Four principles of explainable AI as applied to biometrics and facial forensic algorithms},
 venue = {arXiv preprint arXiv:2002.01014}
}

@article{ulianov2013intelligent,
 abstract = {Kansei is a subjective and unexplainable function  dq dt = ϕ(q, S (t), t, u, ξ (t)), u = f (q, qd,t), (1) where q is the vector of generalized coordinates describing the dynamics of the controlled plant; S is the generalized entropy of dynamic system (1); u is the control force (the},
 author = {Ulianov, Serghei},
 journal = {Computer Science Journal of Moldova},
 number = {2},
 pages = {242--279},
 year = {2013},
 title = {Intelligent self-organized robust control design based on quantum/soft computing technologies and Kansei Engineering},
 venue = {Computer Science Journal of Moldova},
 volume = {62}
}

@article{schrills2020answer,
 abstract = {To achieve optimal human-system integration in the context of user-AI interaction it is important that users develop a valid representation of how AI works. In most of the everyday interaction with technical systems users construct mental models (ie, an abstraction of the anticipated mechanisms a system uses to perform a given task). If no explicit explanations are provided by a system (eg by a self-explaining AI) or other sources (eg an instructor), the mental model is typically formed based on experiences, ie the observations of the user},
 author = {Schrills, Tim and Franke, Thomas},
 journal = {arXiv preprint arXiv:2002.02526},
 year = {2020},
 title = {How to Answer Why--Evaluating the Explanations of AI Through Mental Model Analysis},
 venue = {arXiv preprint arXiv:2002.02526}
}

@inproceedings{nematzadeh2017evaluating,
 abstract = {Page 1. Evaluating Vector-Space Models of Word Representation, or, The Unreasonable Effectiveness of Counting Words Near Other Words  So- ciety for Artificial Intelligence and Statistics, 2005. DL Nelson, CL McEvoy, and TA Schreiber},
 author = {Nematzadeh, Aida and Meylan, Stephan C and Griffiths, Thomas L},
 booktitle = {CogSci},
 year = {2017},
 title = {Evaluating Vector-Space Models of Word Representation, or, The Unreasonable Effectiveness of Counting Words Near Other Words.},
 venue = {CogSci}
}

@article{tuckey2020general,
 abstract = {Explainability in AI is gaining attention in the computer science community in response to the increasing success of deep learning and the important need of justifying how such systems make predictions in life-critical applications. The focus of explainability in AI has predominantly been on trying to gain insights into how machine learning systems function by exploring relationships between input data and predicted outcomes or by extracting simpler interpretable models. Through literature surveys of philosophy and social science, authors},
 author = {Tuckey, David and Russo, Alessandra and Broda, Krysia},
 journal = {arXiv preprint arXiv:2003.00749},
 year = {2020},
 title = {A general framework for scientifically inspired explanations in AI},
 venue = {arXiv preprint arXiv:2003.00749}
}

@article{visani2020statistical,
 abstract = {Nowadays we are witnessing a transformation of the business processes towards a more computation driven approach. The ever increasing usage of Machine Learning techniques is the clearest example of such trend. This sort of revolution is often providing advantages, such as an increase in prediction accuracy and a reduced time to obtain the results. However, these methods present a major drawback: it is very difficult to understand on what grounds the algorithm took the decision. To address this issue we consider the LIME},
 author = {Visani, Giorgio and Bagli, Enrico and Chesani, Federico and Poluzzi, Alessandro and Capuzzo, Davide},
 journal = {Journal of the Operational Research Society},
 pages = {1--11},
 year = {2020},
 publisher = {Taylor \& Francis},
 title = {Statistical stability indices for LIME: obtaining reliable explanations for machine learning models},
 venue = {Journal of the …}
}

@article{arrieta2020explainable,
 abstract = {In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in},
 author = {Arrieta, Alejandro Barredo and D{\'\i}az-Rodr{\'\i}guez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garc{\'\i}a, Salvador and Gil-L{\'o}pez, Sergio and Molina, Daniel and Benjamins, Richard and others},
 journal = {Information Fusion},
 pages = {82--115},
 year = {2020},
 publisher = {Elsevier},
 title = {Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI},
 venue = {Information …},
 volume = {58}
}

@article{artelt2019computation,
 abstract = {Due to the increasing use of machine learning in practice it becomes more and more important to be able to explain the prediction and behavior of machine learning models. An instance of explanations are counterfactual explanations which provide an intuitive and useful explanations of machine learning models. In this survey we review model-specific methods for efficiently computing counterfactual explanations of many different machine learning models and propose methods for models that have not been considered in},
 author = {Artelt, Andr{\'e} and Hammer, Barbara},
 journal = {arXiv preprint arXiv:1911.07749},
 year = {2019},
 title = {On the computation of counterfactual explanations--A survey},
 venue = {arXiv preprint arXiv:1911.07749}
}

@article{atrey2019exploratory,
 abstract = {Saliency maps are frequently used to support explanations of the behavior of deep reinforcement learning (RL) agents. However, a review of how saliency maps are used in practice indicates that the derived explanations are often unfalsifiable and can be highly subjective. We introduce an empirical approach grounded in counterfactual reasoning to test the hypotheses generated from saliency maps and assess the degree to which they correspond to the semantics of RL environments. We use Atari games, a common},
 author = {Atrey, Akanksha and Clary, Kaleigh and Jensen, David},
 journal = {arXiv preprint arXiv:1912.05743},
 year = {2019},
 title = {Exploratory not explanatory: Counterfactual analysis of saliency maps for deep reinforcement learning},
 venue = {arXiv preprint arXiv:1912.05743}
}

@article{blandfort2018overview,
 abstract = {It is said that beauty is in the eye of the beholder. But how exactly can we characterize such discrepancies in interpretation? For example, are there any specific features of an image that makes person A regard an image as beautiful while person B finds the same image displeasing? Such questions ultimately aim at explaining our individual ways of interpretation, an intention that has been of fundamental importance to the social sciences from the beginning. More recently, advances in computer science brought up two related},
 author = {Blandfort, Philipp and Hees, J{\"o}rn and Patton, Desmond U},
 journal = {arXiv preprint arXiv:1811.04028},
 year = {2018},
 title = {An Overview of Computational Approaches for Interpretation Analysis},
 venue = {arXiv preprint arXiv:1811.04028}
}

@article{guidotti2020black,
 abstract = {8 we show, for some instances pre- viously analyzed, how they can be changed to move from the  21 .729 ± .21 .776 ± .22 1.485 ± .14 .726 ± .21 .393 ± .03 Table 4. Stability analysis for DNN  For gaining the trust of the user, it is crucial to analyze the stability of inter- pretable},
 author = {Guidotti, Riccardo and Monreale, Anna and Matwin, Stan and Pedreschi, Dino},
 journal = {arXiv preprint arXiv:2002.03746},
 year = {2020},
 title = {Black box explanation by learning image exemplars in the latent feature space},
 venue = {arXiv preprint arXiv …}
}

@article{chapman2019emap,
 abstract = {Modern instance-based model-agnostic explanation methods (LIME, SHAP, L2X) are of great use in data-heavy industries for model diagnostics, and for end-user explanations. These methods generally return either a weighting or subset of input features as an explanation of the classification of an instance. An alternative literature argues instead that counterfactual instances provide a more useable characterisation of a black box classifier's decisions. We present EMAP, a neural network based approach which returns as},
 author = {Chapman-Rounds, Matt and Schulz, Marc-Andre and Pazos, Erik and Georgatzis, Konstantinos},
 journal = {arXiv preprint arXiv:1912.00872},
 year = {2019},
 title = {EMAP: Explanation by Minimal Adversarial Perturbation},
 venue = {arXiv preprint arXiv …}
}

@article{cheng2019interpretability,
 abstract = {Using deep neural networks for identifying physics objects at the Large Hadron Collider (LHC) has become a powerful alternative approach in recent years. After successful training of deep neural networks, examining the trained networks not only helps us understand the behaviour of neural networks, but also helps improve the performance of deep learning models through proper interpretation. We take jet tagging problem at the LHC as an example, using recursive neural networks as a starting point, aim at a thorough},
 author = {Cheng, Taoli},
 journal = {arXiv preprint arXiv:1911.01872},
 year = {2019},
 title = {Interpretability Study on Deep Learning for Jet Physics at the Large Hadron Collider},
 venue = {arXiv preprint arXiv:1911.01872}
}

@article{du2019techniques,
 abstract = {MACHINE LEARNING IS progressing at an astounding rate, powered by complex models such  as ensemble models and deep neural networks (DNNs). These models have a wide range of  real-world applications, such as movie recommendations of Netflix, neural machine translation  of Google, and speech recognition of Amazon Alexa. Despite the successes, machine learning  has its own limitations and drawbacks. The most significant one is the lack of transparency behind  their behaviors, which leaves users with little understanding of how particular decisions are made},
 author = {Du, Mengnan and Liu, Ninghao and Hu, Xia},
 journal = {Communications of the ACM},
 number = {1},
 pages = {68--77},
 year = {2019},
 publisher = {ACM New York, NY, USA},
 title = {Techniques for interpretable machine learning},
 venue = {Communications of the ACM},
 volume = {63}
}

@article{edmonds2019tale,
 abstract = {The ability to provide comprehensive explanations of chosen actions is a hallmark of intelligence. Lack of this ability impedes the general acceptance of AI and robot systems in critical tasks. This paper examines what forms of explanations best foster human trust in},
 author = {Edmonds, Mark and Gao, Feng and Liu, Hangxin and Xie, Xu and Qi, Siyuan and Rothrock, Brandon and Zhu, Yixin and Wu, Ying Nian and Lu, Hongjing and Zhu, Song-Chun},
 journal = {Science Robotics},
 number = {37},
 year = {2019},
 publisher = {Science Robotics},
 title = {A tale of two explanations: Enhancing human trust by explaining robot behavior},
 venue = {… Robotics},
 volume = {4}
}

@article{fadnis2019heuristics,
 author = {Fadnis, Kshitij and Talamadupula, Kartik and Kapanipathi, Pavan and Ishfaq, Haque and Roukos, Salim and Fokoue, Achille},
 journal = {arXiv preprint arXiv:1911.02085},
 year = {2019},
 title = {Heuristics for interpretable knowledge graph contextualization},
 venue = {NA}
}

@inproceedings{gilpin2018explaining,
 abstract = {There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms},
 author = {Gilpin, Leilani H and Bau, David and Yuan, Ben Z and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
 booktitle = {2018 IEEE 5th International Conference on data science and advanced analytics (DSAA)},
 organization = {IEEE},
 pages = {80--89},
 year = {2018},
 title = {Explaining explanations: An overview of interpretability of machine learning},
 venue = {2018 IEEE 5th …}
}

@inproceedings{goyal2019counterfactual,
 abstract = {In this work, we develop a technique to produce counterfactual visual explanations. Given a 'query'image $ I $ for which a vision system predicts class $ c $, a counterfactual visual explanation identifies how $ I $ could change such that the system would output a different},
 author = {Goyal, Yash and Wu, Ziyan and Ernst, Jan and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
 booktitle = {International Conference on Machine Learning},
 organization = {PMLR},
 pages = {2376--2384},
 year = {2019},
 title = {Counterfactual visual explanations},
 venue = {… on Machine Learning}
}

@inproceedings{dovsilovic2018explainable,
 abstract = {In the last decade, with availability of large datasets and more computing power, machine learning systems have achieved (super) human performance in a wide variety of tasks. Examples of this rapid development can be seen in image recognition, speech analysis},
 author = {Do{\v{s}}ilovi{\'c}, Filip Karlo and Br{\v{c}}i{\'c}, Mario and Hlupi{\'c}, Nikica},
 booktitle = {2018 41st International convention on information and communication technology, electronics and microelectronics (MIPRO)},
 organization = {IEEE},
 pages = {0210--0215},
 year = {2018},
 title = {Explainable artificial intelligence: A survey},
 venue = {2018 41st International …}
}

@article{hoffman2018metrics,
 abstract = {The question addressed in this paper is: If we present to a user an AI system that explains how it works, how do we know whether the explanation works and the user has achieved a pragmatic understanding of the AI? In other words, how do we know that an explanainable},
 author = {Hoffman, Robert R and Mueller, Shane T and Klein, Gary and Litman, Jordan},
 journal = {arXiv preprint arXiv:1812.04608},
 year = {2018},
 title = {Metrics for explainable AI: Challenges and prospects},
 venue = {arXiv preprint arXiv …}
}

@inproceedings{islam2020towards,
 abstract = {of the best case (ie, one cognitive chunk) and observed case is added towards total explainability  2019) that we will then use for our proposed explainability quantification method  proper- ties of explanations and explanation methods, and a way to quantify explainability which is},
 author = {Islam, Sheikh Rabiul and Eberle, William and Ghafoor, Sheikh K},
 booktitle = {The Thirty-Third International Flairs Conference},
 year = {2020},
 title = {Towards quantification of explainability in explainable artificial intelligence methods},
 venue = {The Thirty-Third International Flairs …}
}

@inproceedings{juozapaitis2019explainable,
 abstract = {Explainable Reinforcement Learning via Reward Decomposition  We investigated this anomaly and found that during gradi- ent optimization, Adam, via its adaptive learning rate, can  a more fundamental issue related to the interac- tion of the gradient optimizer and the RL loop},
 author = {Juozapaitis, Zoe and Koul, Anurag and Fern, Alan and Erwig, Martin and Doshi-Velez, Finale},
 booktitle = {IJCAI/ECAI Workshop on Explainable Artificial Intelligence},
 year = {2019},
 title = {Explainable reinforcement learning via reward decomposition},
 venue = {… on Explainable Artificial …}
}

@inproceedings{keane2019case,
 abstract = {There are several reasons why a systematic review of ANN-CBR twins for XAI is both timely and necessary  Fourth, a systematic survey should allow us to know where we currently stand, and then to strategically road map future directions for this XAI solution},
 author = {Keane, Mark T and Kenny, Eoin M},
 booktitle = {International Conference on Case-Based Reasoning},
 organization = {Springer},
 pages = {155--171},
 year = {2019},
 title = {How case-based reasoning explains neural networks: A theoretical analysis of XAI using post-hoc explanation-by-example from a survey of ANN-CBR twin-systems},
 venue = {International Conference on Case-Based …}
}

@article{zafar2019dlime,
 abstract = {The different components of the proposed method are explained further below. 3.1.1 Hierarchical Clustering (HC)  2010. How to explain individual classification decisions  [27] M. Robnik-Åăikonja and I. Kononenko. 2008. Explaining Classifications For Individual Instances},
 author = {Zafar, Muhammad Rehman and Khan, Naimul Mefraz},
 journal = {arXiv preprint arXiv:1906.10263},
 year = {2019},
 title = {DLIME: A deterministic local interpretable model-agnostic explanations approach for computer-aided diagnosis systems},
 venue = {arXiv preprint arXiv:1906.10263}
}

@inproceedings{lakkaraju2020fool,
 abstract = {As machine learning black boxes are increasingly being deployed in critical domains such as healthcare and criminal justice, there has been a growing emphasis on developing techniques for explaining these black boxes in a human interpretable manner. There has},
 author = {Lakkaraju, Himabindu and Bastani, Osbert},
 booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
 pages = {79--85},
 year = {2020},
 title = {" How do I fool you?" Manipulating User Trust via Misleading Black Box Explanations},
 venue = {Proceedings of the AAAI/ACM Conference on AI …}
}

@article{le2019x,
 abstract = {Even though the topic of explainable AI/ML is very popular in text and computer vision domain, most of the previous literatures are not suitable for explaining black-box models' predictions on general data mining datasets. This is because these datasets are usually in},
 author = {Le, Thai and Wang, Suhang and Lee, Dongwon},
 journal = {CoRR},
 year = {2019},
 title = {Why X rather than Y? Explaining Neural Model’Predictions by Generating Intervention Counterfactual Samples},
 venue = {CoRR}
}

@article{li2019formal,
 abstract = {We did not formally verify the learned policy but instead prevented violation of the task  notes, and experimental results can be found in Materials and Methods and the  the high-level specification, making the reward synthesis problem fundamentally more formal, explainable},
 author = {Li, Xiao and Serlin, Zachary and Yang, Guang and Belta, Calin},
 journal = {Science Robotics},
 number = {37},
 year = {2019},
 publisher = {Science Robotics},
 title = {A formal methods approach to interpretable reinforcement learning for robotic planning},
 venue = {Science Robotics},
 volume = {4}
}

@article{licato2019scenarios,
 abstract = {Artificially intelligent systems, given a set of non-trivial ethical rules to follow, will inevitably be faced with scenarios which call into question the scope of those rules. In such cases, human reasoners typically will engage in interpretive reasoning, where interpretive arguments are used to support or attack claims that some rule should be understood a certain way. Artificially intelligent reasoners, however, currently lack the ability to carry out human-like interpretive reasoning, and we argue that bridging this gulf is of tremendous},
 author = {Licato, John and Marji, Zaid and Abraham, Sophia},
 journal = {arXiv preprint arXiv:1911.01917},
 year = {2019},
 title = {Scenarios and recommendations for ethical interpretive ai},
 venue = {arXiv preprint arXiv:1911.01917}
}

@inproceedings{liu2020interpretable,
 abstract = {Performing driving behaviors based on causal reasoning is essential to ensure driving safety. In this work, we investigated how state-of-the-art 3D Convolutional Neural Networks (CNNs) perform on classifying driving behaviors based on causal reasoning. We proposed a perturbation-based visual explanation method to inspect the models' performance visually. By examining the video attention saliency, we found that existing models could not precisely capture the causes (eg, traffic light) of the specific action (eg, stopping). Therefore, the},
 author = {Liu, Yi-Chieh and Hsieh, Yung-An and Chen, Min-Hung and Yang, C-H Huck and Tegner, Jesper and Tsai, Y-C James},
 booktitle = {ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
 organization = {IEEE},
 pages = {2338--2342},
 year = {2020},
 title = {Interpretable self-attention temporal reasoning for driving behavior understanding},
 venue = {ICASSP 2020-2020 …}
}

@inproceedings{liu2020towards,
 abstract = {Recent advances in Convolutional Neural Network (CNN) model interpretability have led to impressive progress in visualizing and understanding model predictions. In particular, gradient-based visual attention methods have driven much recent effort in using},
 author = {Liu, Wenqian and Li, Runze and Zheng, Meng and Karanam, Srikrishna and Wu, Ziyan and Bhanu, Bir and Radke, Richard J and Camps, Octavia},
 booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
 pages = {8642--8651},
 year = {2020},
 title = {Towards visually explaining variational autoencoders},
 venue = {Proceedings of the …}
}

@article{lucic2019focus,
 abstract = {1 INTRODUCTION Model interpretability has become an important problem in ma- chine learning  An optimal explanation provides the user with interpretable and potentially actionable feedback related to understanding the predictions of model M},
 author = {Lucic, Ana and Oosterhuis, Harrie and Haned, Hinda and de Rijke, Maarten},
 journal = {arXiv preprint arXiv:1911.12199},
 year = {2019},
 title = {FOCUS: Flexible optimizable counterfactual explanations for tree ensembles},
 venue = {arXiv preprint arXiv …}
}

@article{mahajan2019preserving,
 abstract = {To construct interpretable explanations that are consistent with the original ML model, counterfactual examples---showing how the model's output changes with small perturbations to the input---have been proposed. This paper extends the work in counterfactual explanations by addressing the challenge of feasibility of such examples. For explanations of ML models in critical domains such as healthcare and finance, counterfactual examples are useful for an end-user only to the extent that perturbation of},
 author = {Mahajan, Divyat and Tan, Chenhao and Sharma, Amit},
 journal = {arXiv preprint arXiv:1912.03277},
 year = {2019},
 title = {Preserving causal constraints in counterfactual explanations for machine learning classifiers},
 venue = {arXiv preprint arXiv:1912.03277}
}

@article{miller2019explanation,
 abstract = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer},
 author = {Miller, Tim},
 journal = {Artificial intelligence},
 pages = {1--38},
 year = {2019},
 publisher = {Elsevier},
 title = {Explanation in artificial intelligence: Insights from the social sciences},
 venue = {Artificial intelligence},
 volume = {267}
}

@inproceedings{mittelstadt2019explaining,
 abstract = {Recent work on interpretability in machine learning and AI has focused on the building of simplified models that approximate the true criteria used to make decisions. These models are a useful pedagogical device for teaching trained professionals how to predict what},
 author = {Mittelstadt, Brent and Russell, Chris and Wachter, Sandra},
 booktitle = {Proceedings of the conference on fairness, accountability, and transparency},
 pages = {279--288},
 year = {2019},
 title = {Explaining explanations in AI},
 venue = {Proceedings of the conference on …}
}

@article{mundhenk2019efficient,
 abstract = {We describe an explainable AI saliency map method for use with deep convolutional neural networks (CNN) that is much more efficient than popular fine-resolution gradient methods. It is also quantitatively similar or better in accuracy. Our technique works by measuring information at the end of each network scale which is then combined into a single saliency map. We describe how saliency measures can be made more efficient by exploiting Saliency Map Order Equivalence. We visualize individual scale/layer contributions by using},
 author = {Mundhenk, T Nathan and Chen, Barry Y and Friedland, Gerald},
 journal = {arXiv preprint arXiv:1911.11293},
 year = {2019},
 title = {Efficient saliency maps for Explainable AI},
 venue = {arXiv preprint arXiv:1911.11293}
}

@inproceedings{saralajew2020fast,
 abstract = {First, the method does not improve over state of the art and we expect issues in scaling to  Hence, we do not consider not improving over these methods as a critical limitation  Preliminary results regarding this can be found in the supplementary material Section E.2. We close this},
 author = {Saralajew, Sascha and Holdijk, Lars and Villmann, Thomas},
 booktitle = {NeurIPS},
 year = {2020},
 title = {Fast Adversarial Robustness Certification of Nearest Prototype Classifiers for Arbitrary Seminorms.},
 venue = {NeurIPS}
}

@inproceedings{zhang2018interpretable,
 abstract = {Network visualization: Visualization of filters in a C- NN is the most direct way of exploring the  All feature maps, including those of other categories, select n2 templates of {Tµi } as masks  For each fil- ter f, we computed its feature maps X after ReLu/mask operations on different},
 author = {Zhang, Quanshi and Wu, Ying Nian and Zhu, Song-Chun},
 booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
 pages = {8827--8836},
 year = {2018},
 title = {Interpretable convolutional neural networks},
 venue = {Proceedings of the IEEE …}
}

@phdthesis{putnam2020toward,
 abstract = {Description Our research is a step toward understanding when explanations of AI-driven hints and feedback are useful in Intelligent Tutoring Systems (ITS). We added an explanation functionality for the adaptive hints provided by the Adaptive CSP (ACSP) applet, an intelligent interactive simulation that helps students learn an algorithm for constraint satisfaction problems. We present the design of the explanation functionality and the results of an exploratory study to evaluate how students use it, including an analysis of how},
 author = {Putnam, Vanessa},
 year = {2020},
 school = {University of British Columbia},
 title = {Toward XAI for Intelligent Tutoring Systems: a case study},
 venue = {NA}
}

@article{ramon2019counterfactual,
 abstract = {We study the interpretability of predictive systems that use high-dimensonal behavioral and textual data. Examples include predicting product interest based on online browsing data and detecting spam emails or objectionable web content. Recently, counterfactual explanations have been proposed for generating insight into model predictions, which focus on what is relevant to a particular instance. Conducting a complete search to compute counterfactuals is very time-consuming because of the huge dimensionality. To our},
 author = {Ramon, Yanou and Martens, David and Provost, Foster and Evgeniou, Theodoros},
 journal = {arXiv preprint arXiv:1912.01819},
 year = {2019},
 title = {Counterfactual explanation algorithms for behavioral and textual data},
 venue = {arXiv preprint arXiv …}
}

@article{scholkopf2019causality,
 abstract = {Graphical causal inference as pioneered by Judea Pearl arose from research on artificial intelligence (AI), and for a long time had little connection to the field of machine learning. This article discusses where links have been and should be established, introducing key},
 author = {Sch{\"o}lkopf, Bernhard},
 journal = {arXiv preprint arXiv:1911.10500},
 year = {2019},
 title = {Causality for machine learning},
 venue = {arXiv preprint arXiv:1911.10500}
}

@inproceedings{slack2020fooling,
 abstract = {As machine learning black boxes are increasingly being deployed in domains such as healthcare and criminal justice, there is growing emphasis on building tools and techniques for explaining these black boxes in an interpretable manner. Such explanations are being},
 author = {Slack, Dylan and Hilgard, Sophie and Jia, Emily and Singh, Sameer and Lakkaraju, Himabindu},
 booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
 pages = {180--186},
 year = {2020},
 title = {Fooling lime and shap: Adversarial attacks on post hoc explanation methods},
 venue = {Proceedings of the AAAI …}
}

@inproceedings{sokol2020explainability,
 abstract = {Explanations in Machine Learning come in many forms, but a consensus regarding their desired properties is yet to emerge. In this paper we introduce a taxonomy and a set of descriptors that can be used to characterise and systematically assess explainable systems along five key dimensions: functional, operational, usability, safety and validation. In order to design a comprehensive and representative taxonomy and associated descriptors we surveyed the eXplainable Artificial Intelligence literature, extracting the},
 author = {Sokol, Kacper and Flach, Peter},
 booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
 pages = {56--67},
 year = {2020},
 title = {Explainability fact sheets: a framework for systematic assessment of explainable approaches},
 venue = {Proceedings of the 2020 Conference on Fairness …}
}

@inproceedings{cui2019face,
 abstract = {the convolutional neural network, pre-trained as explained in Section 4.1, is l2-normalised and  The purposed method result can help to build face recognition system for organizations. In the future projects, we plan to explore fine tuning approach for improving face recognition},
 author = {Cui, Wanxin and Zhan, Wei and Yu, Jingjing and Sun, Chenfan and Zhang, Yangyang},
 booktitle = {2019 International Conference on Intelligent Computing, Automation and Systems (ICICAS)},
 organization = {IEEE},
 pages = {746--750},
 year = {2019},
 title = {Face recognition via convolutional neural networks and siamese neural networks},
 venue = {… Conference on Intelligent …}
}

@inproceedings{zhang2019interpretable,
 abstract = {7.7% when is evaluated in VQA-X. This indicates that our pro- posed methods enable VQA models to provide more mean- ingful and interpretable results by generating more accurate visual grounding. Table},
 author = {Zhang, Yundong and Niebles, Juan Carlos and Soto, Alvaro},
 booktitle = {2019 ieee winter conference on applications of computer vision (wacv)},
 organization = {IEEE},
 pages = {349--357},
 year = {2019},
 title = {Interpretable visual question answering by visual grounding from attention supervision mining},
 venue = {… applications of computer vision  …}
}

@article{zhang2019anomaly,
 author = {Zhang, Xiao and Marwah, Manish and Lee, I-ta and Arlitt, Martin and Goldwasser, Dan},
 year = {2019},
 title = {An Anomaly Contribution Explainer for Cyber-Security Applications},
 venue = {NA}
}

@article{zhang2019towards,
 abstract = {This paper proposes a set of criteria to evaluate the objectiveness of explanation methods of neural networks, which is crucial for the development of explainable AI, but it also presents significant challenges. The core challenge is that people usually cannot obtain ground-truth explanations of the neural network. To this end, we design four metrics to evaluate explanation results without ground-truth explanations. Our metrics can be broadly applied to nine benchmark methods of interpreting neural networks, which provides new insights of},
 author = {Zhang, Hao and Chen, Jiayi and Xue, Haotian and Zhang, Quanshi},
 journal = {arXiv preprint arXiv:1911.09017},
 year = {2019},
 title = {Towards a unified evaluation of explanation methods without ground truth},
 venue = {arXiv preprint arXiv:1911.09017}
}

@inproceedings{abdul2018trends,
 abstract = {Advances in artificial intelligence, sensors and big data management have far-reaching societal impacts. As these systems augment our everyday lives, it becomes increasing-ly important for people to understand them and remain in control. We investigate how HCI researchers can help to develop accountable systems by performing a literature analysis of 289 core papers on explanations and explaina-ble systems, as well as 12,412 citing papers. Using topic modeling, co-occurrence and network analysis, we mapped the research space},
 author = {Abdul, Ashraf and Vermeulen, Jo and Wang, Danding and Lim, Brian Y and Kankanhalli, Mohan},
 booktitle = {Proceedings of the 2018 CHI conference on human factors in computing systems},
 pages = {1--18},
 year = {2018},
 title = {Trends and trajectories for explainable, accountable and intelligible systems: An hci research agenda},
 venue = {Proceedings of the …}
}

@inproceedings{alonso2018bibliometric,
 abstract = {Bibliometrix [18] - An R package for performing comprehensive quantitative research in Scientometrics and Bibliometrics  VOS viewer [9] - A software tool for constructing and visualizing bibliometric networks which can  We replicated the previous analysis with a modified query},
 author = {Alonso, Jose M and Castiello, Ciro and Mencar, Corrado},
 booktitle = {International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems},
 organization = {Springer},
 pages = {3--15},
 year = {2018},
 title = {A bibliometric analysis of the explainable artificial intelligence research field},
 venue = {International Conference on …}
}

@article{alvarez2018robustness,
 abstract = {We argue that robustness of explanations---ie, that similar inputs should give rise to similar explanations---is a key desideratum for interpretability. We introduce metrics to quantify robustness and demonstrate that current methods do not perform well according to these metrics. Finally, we propose ways that robustness can be enforced on existing interpretability approaches.},
 author = {Alvarez-Melis, David and Jaakkola, Tommi S},
 journal = {arXiv preprint arXiv:1806.08049},
 year = {2018},
 title = {On the robustness of interpretability methods},
 venue = {arXiv preprint arXiv:1806.08049}
}

@article{alvarez2018towards,
 abstract = {Most recent work on interpretability of complex machine learning models has focused on estimating $\textit {a posteriori} $ explanations for previously trained models around specific predictions. $\textit {Self-explaining} $ models where interpretability plays a key role already during learning have received much less attention. We propose three desiderata for explanations in general--explicitness, faithfulness, and stability--and show that existing methods do not satisfy them. In response, we design self-explaining models in stages},
 author = {Alvarez-Melis, David and Jaakkola, Tommi S},
 journal = {arXiv preprint arXiv:1806.07538},
 year = {2018},
 title = {Towards robust interpretability with self-explaining neural networks},
 venue = {arXiv preprint arXiv:1806.07538}
}

@inproceedings{annasamy2019towards,
 abstract = {Deep reinforcement learning techniques have demonstrated superior performance in a wide variety of environments. As improvements in training algorithms continue at a brisk pace, theoretical or empirical studies on understanding what these networks seem to learn, are far behind. In this paper we propose an interpretable neural network architecture for Q-learning which provides a global explanation of the model's behavior using key-value memories, attention and reconstructible embeddings. With a directed exploration strategy, our model},
 author = {Annasamy, Raghuram Mandyam and Sycara, Katia},
 booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
 number = {01},
 pages = {4561--4569},
 year = {2019},
 title = {Towards better interpretability in deep q-networks},
 venue = {Proceedings of the AAAI Conference on …},
 volume = {33}
}

@article{besold2018and,
 abstract = {The increasing incorporation of Artificial Intelligence in the form of automated systems into decision-making procedures highlights not only the importance of decision theory for automated systems but also the need for these decision procedures to be explainable to the people involved in them. Traditional realist accounts of explanation, wherein explanation is a relation that holds (or does not hold) eternally between an explanans and an explanandum, are not adequate to account for the notion of explanation required for artificial},
 author = {Besold, Tarek R and Uckelman, Sara L},
 journal = {arXiv preprint arXiv:1808.07074},
 year = {2018},
 title = {The what, the why, and the how of artificial explanations in automated decision-making},
 venue = {arXiv preprint arXiv:1808.07074}
}

@article{chang2018explaining,
 author = {Chang, Chun-Hao and Creager, Elliot and Goldenberg, Anna and Duvenaud, David},
 journal = {arXiv preprint arXiv:1807.08024},
 year = {2018},
 title = {Explaining image classifiers by adaptive dropout and generative in-filling},
 venue = {NA},
 volume = {2}
}

@article{charles2018interpreting,
 abstract = {Theoretical understanding of deep learning is one of the most important tasks facing the statistics and machine learning communities. While deep neural networks (DNNs) originated as engineering methods and models of biological networks in neuroscience and psychology, they have quickly become a centerpiece of the machine learning toolbox. Unfortunately, DNN adoption powered by recent successes combined with the open-source nature of the machine learning community, has outpaced our theoretical understanding. We},
 author = {Charles, Adam S},
 journal = {arXiv preprint arXiv:1806.00148},
 year = {2018},
 title = {Interpreting deep learning: The machine learning rorschach test?},
 venue = {arXiv preprint arXiv:1806.00148}
}

@article{dhurandhar2018explanations,
 abstract = {In this paper we propose a novel method that provides contrastive explanations justifying the classification of an input by a black box classifier such as a deep neural network. Given an input we find what should be% necessarily and minimally and sufficiently present (viz. important object pixels in an image) to justify its classification and analogously what should be minimally and necessarily\emph {absent}(viz. certain background pixels). We argue that such explanations are natural for humans and are used commonly in domains such as},
 author = {Dhurandhar, Amit and Chen, Pin-Yu and Luss, Ronny and Tu, Chun-Chen and Ting, Paishun and Shanmugam, Karthikeyan and Das, Payel},
 journal = {arXiv preprint arXiv:1802.07623},
 year = {2018},
 title = {Explanations based on the missing: Towards contrastive explanations with pertinent negatives},
 venue = {arXiv preprint arXiv …}
}

@article{elsayed2018adversarial,
 abstract = {Deep neural networks are susceptible to\emph {adversarial} attacks. In computer vision, well-crafted perturbations to images can cause neural networks to make mistakes such as confusing a cat with a computer. Previous adversarial attacks have been designed to degrade performance of models or cause machine learning models to produce specific outputs chosen ahead of time by the attacker. We introduce attacks that instead {\em reprogram} the target model to perform a task chosen by the attacker---without the attacker},
 author = {Elsayed, Gamaleldin F and Goodfellow, Ian and Sohl-Dickstein, Jascha},
 journal = {arXiv preprint arXiv:1806.11146},
 year = {2018},
 title = {Adversarial reprogramming of neural networks},
 venue = {arXiv preprint arXiv …}
}

@inproceedings{ge2018towards,
 abstract = {The capability of making explainable inferences regarding physical processes has long been desired. One fundamental physical process is object motion. Inferring what causes the motion of a group of objects can even be a challenging task for experts, eg, in forensic science. Most of the work in the literature rely on physics simulation to draw such inferences. The simulation requires a precise model of the underlying domain to work well and is essentially a black-box from which one can hardly obtain any useful explanation. By},
 author = {Ge, Xiaoyu and Renz, Jochen and Hua, Hua},
 booktitle = {Sixteenth International Conference on Principles of Knowledge Representation and Reasoning},
 year = {2018},
 title = {Towards explainable inference about object motion using qualitative reasoning},
 venue = {Sixteenth International Conference on Principles of …}
}

@misc{gilpin2018explaining,
 author = {Gilpin, Leilani H and Bau, David and Yuan, Ben Z and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
 year = {2018},
 title = {Explaining Explanations: An Approach to Evaluating Interpretability of Machine Learning.(2018)},
 venue = {NA}
}

@article{grosz2018century,
 abstract = {The Stanford One Hundred Year Study on Artificial Intelligence, a project that launched in December  2014, is designed to be a century-long periodic assessment of the field of artificial intelligence  (AI) and its influences on people, their communities, and society. Colloquially referred to as  “AI100,” the project issued its first report in September 2016. A standing committee of AI scientists  and scholars in the humanities and social sciences working with the Stanford faculty director  of AI100 oversees the project and the design of its activities. A little more than two years after},
 author = {Grosz, Barbara J and Stone, Peter},
 journal = {Communications of the ACM},
 number = {12},
 pages = {68--73},
 year = {2018},
 publisher = {ACM New York, NY, USA},
 title = {A century-long commitment to assessing artificial intelligence and its impact on society},
 venue = {Communications of the ACM},
 volume = {61}
}

@article{guidotti2018survey,
 abstract = {In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific},
 author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
 journal = {ACM computing surveys (CSUR)},
 number = {5},
 pages = {1--42},
 year = {2018},
 publisher = {ACM New York, NY, USA},
 title = {A survey of methods for explaining black box models},
 venue = {ACM computing …},
 volume = {51}
}

@article{guo2020explainable,
 abstract = {to, and we need machines to have that equal capability in order to ensure trust and a legal pathway toward improving safety and  The recent advances in 6G human-brain interfacing [1] for tactile control and shared intelligence present a futuristic framework for XAI},
 author = {Guo, Weisi},
 journal = {IEEE Communications Magazine},
 number = {6},
 pages = {39--45},
 year = {2020},
 publisher = {IEEE},
 title = {Explainable artificial intelligence for 6G: Improving trust between human and machine},
 venue = {IEEE Communications Magazine},
 volume = {58}
}

@article{harbecke2018learning,
 abstract = {In the last decade, deep neural classifiers achieved state-of-the-art results in many domains,  among others in vision and language. Due to the com- plexity of a deep neural model,  however, it is dif- ficult to explain its decisions. Understanding its decision process potentially  allows to improve the model and may reveal new knowledge about the input. Recently, Kindermans  et al. (2018) claimed that “popular explanation approaches for neural net- works (...) do not provide  the correct explana- tion, even for a simple linear model.” They show that in a linear model, the},
 author = {Harbecke, David and Schwarzenberg, Robert and Alt, Christoph},
 journal = {arXiv preprint arXiv:1808.04127},
 year = {2018},
 title = {Learning explanations from language data},
 venue = {arXiv preprint arXiv:1808.04127}
}

@article{hind2018increasing,
 abstract = {The accuracy and reliability of machine learning algorithms are an important concern for suppliers of artificial intelligence (AI) services, but considerations beyond accuracy, such as safety, security, and provenance, are also critical elements to engender consumers' trust in a service. In this paper, we propose a supplier's declaration of conformity (SDoC) for AI services to help increase trust in AI services. An SDoC is a transparent, standardized, but often not legally required, document used in many industries and sectors to describe the},
 author = {Hind, Michael and Mehta, Sameep and Mojsilovic, Aleksandra and Nair, Ravi and Ramamurthy, Karthikeyan Natesan and Olteanu, Alexandra and Varshney, Kush R},
 journal = {arXiv preprint arXiv:1808.07261},
 pages = {2813--2869},
 year = {2018},
 title = {Increasing trust in AI services through supplier’s declarations of conformity},
 venue = {arXiv preprint arXiv …},
 volume = {18}
}

@inproceedings{hoffman2018explaining,
 abstract = {What makes for an explanation of “black box” AI systems such as Deep Nets? We reviewed the pertinent literatures on explanation and derived key ideas. This set the stage for our empirical inquiries, which include conceptual cognitive modeling, the analysis of a corpus of cases of" naturalistic explanation" of computational systems, computational cognitive modeling, and the development of measures for performance evaluation. The purpose of our work is to contribute to the program of research on “Explainable AI.” In this report we focus},
 author = {Hoffman, Robert R and Klein, Gary and Mueller, Shane T},
 booktitle = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
 number = {1},
 organization = {SAGE Publications Sage CA: Los Angeles, CA},
 pages = {197--201},
 year = {2018},
 title = {Explaining explanation for “explainable ai”},
 venue = {Proceedings of the Human …},
 volume = {62}
}

@article{hohman2018visual,
 abstract = {Deep learning has recently seen rapid development and received significant attention due to its state-of-the-art performance on previously-thought hard problems. However, because of the internal complexity and nonlinear structure of deep neural networks, the underlying decision making processes for why these models are achieving such performance are challenging and sometimes mystifying to interpret. As deep learning spreads across domains, it is of paramount importance that we equip users of deep learning with tools for},
 author = {Hohman, Fred and Kahng, Minsuk and Pienta, Robert and Chau, Duen Horng},
 journal = {IEEE transactions on visualization and computer graphics},
 number = {8},
 pages = {2674--2693},
 year = {2018},
 publisher = {IEEE},
 title = {Visual analytics in deep learning: An interrogative survey for the next frontiers},
 venue = {IEEE transactions on …},
 volume = {25}
}

@article{honegger2018shedding,
 abstract = {From self-driving vehicles and back-flipping robots to virtual assistants who book our next appointment at the hair salon or at that restaurant for dinner-machine learning systems are becoming increasingly ubiquitous. The main reason for this is that these methods boast},
 author = {Honegger, Milo},
 journal = {arXiv preprint arXiv:1808.05054},
 year = {2018},
 title = {Shedding light on black box machine learning algorithms: Development of an axiomatic framework to assess the quality of methods that explain individual predictions},
 venue = {arXiv preprint arXiv:1808.05054}
}

@article{hossain2019comprehensive,
 abstract = {Generating a description of an image is called image captioning. Image captioning requires recognizing the important objects, their attributes, and their relationships in an image. It also needs to generate syntactically and semantically correct sentences. Deep-learning-based techniques are capable of handling the complexities and challenges of image captioning. In this survey article, we aim to present a comprehensive review of existing deep-learning-based image captioning techniques. We discuss the foundation of the techniques to analyze},
 author = {Hossain, MD Zakir and Sohel, Ferdous and Shiratuddin, Mohd Fairuz and Laga, Hamid},
 journal = {ACM Computing Surveys (CsUR)},
 number = {6},
 pages = {1--36},
 year = {2019},
 publisher = {ACM New York, NY, USA},
 title = {A comprehensive survey of deep learning for image captioning},
 venue = {ACM Computing Surveys …},
 volume = {51}
}

@inproceedings{hu2018explainable,
 abstract = {In complex inferential tasks like question answering, machine learning models must confront two challenges: the need to implement a compositional reasoning process, and, in many applications, the need for this reasoning process to be interpretable to assist users in both development and prediction. Existing models designed to produce interpretable traces of their decision-making process typically require these traces to be supervised at training time. In this paper, we present a novel neural modular approach that performs compositional},
 author = {Hu, Ronghang and Andreas, Jacob and Darrell, Trevor and Saenko, Kate},
 booktitle = {Proceedings of the European conference on computer vision (ECCV)},
 pages = {53--69},
 year = {2018},
 title = {Explainable neural computation via stack neural module networks},
 venue = {Proceedings of the …}
}

@article{hu2018locally,
 abstract = {Supervised Machine Learning (SML) algorithms such as Gradient Boosting, Random Forest, and Neural Networks have become popular in recent years due to their increased predictive performance over traditional statistical methods. This is especially true with large data sets (millions or more observations and hundreds to thousands of predictors). However, the complexity of the SML models makes them opaque and hard to interpret without additional tools. There has been a lot of interest recently in developing global and local diagnostics for},
 author = {Hu, Linwei and Chen, Jie and Nair, Vijayan N and Sudjianto, Agus},
 journal = {arXiv preprint arXiv:1806.00663},
 year = {2018},
 title = {Locally interpretable models and effects based on supervised partitioning (LIME-SUP)},
 venue = {arXiv preprint arXiv:1806.00663}
}

@inproceedings{schwalbe2020survey,
 abstract = {Keil, D., Herbert, SL, Fisac, JF, Deglurkar, S., Tomlin, CJ: Planning, fast and slow: A framework for adaptive real-time safe trajectory planning  2016) [19] Huang, X., Kroening, D., Kwiatkowska, M., Ruan, W., Sun, Y., Thamo, E., Wu, M., Yi, X.: Safety and trustworthiness of deep},
 author = {Schwalbe, Gesina and Schels, Martin},
 booktitle = {10th European Congress on Embedded Real Time Software and Systems (ERTS 2020)},
 year = {2020},
 title = {A survey on methods for the safety assurance of machine learning based systems},
 venue = {10th European Congress on …}
}

@inproceedings{iyer2018transparency,
 abstract = {Autonomous AI systems will be entering human society in the near future to provide services and work alongside humans. For those systems to be accepted and trusted, the users should be able to understand the reasoning process of the system, ie the system should be transparent. System transparency enables humans to form coherent explanations of the system's decisions and actions. Transparency is important not only for user trust, but also for software debugging and certification. In recent years, Deep Neural Networks have made},
 author = {Iyer, Rahul and Li, Yuezhang and Li, Huao and Lewis, Michael and Sundar, Ramitha and Sycara, Katia},
 booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
 pages = {144--150},
 year = {2018},
 title = {Transparency and explanation in deep reinforcement learning neural networks},
 venue = {Proceedings of the 2018 …}
}

@article{karim2018machine,
 abstract = {The term" interpretability" is oftenly used by machine learning researchers each with their own intuitive understanding of it. There is no universal well agreed upon definition of interpretability in machine learning. As any type of science discipline is mainly driven by the set of formulated questions rather than by different tools in that discipline, eg astrophysics is the discipline that learns the composition of stars, not as the discipline that use the spectroscopes. Similarly, we propose that machine learning interpretability should be a},
 author = {Karim, Abdul and Mishra, Avinash and Newton, MA and Sattar, Abdul},
 journal = {arXiv preprint arXiv:1807.06722},
 year = {2018},
 title = {Machine Learning Interpretability: A Science rather than a tool},
 venue = {arXiv preprint arXiv:1807.06722}
}

@inproceedings{kleinerman2018providing,
 abstract = {Automated platforms which support users in finding a mutually beneficial match, such as online dating and job recruitment sites, are becoming increasingly popular. These platforms often include recommender systems that assist users in finding a suitable match. While recommender systems which provide explanations for their recommendations have shown many benefits, explanation methods have yet to be adapted and tested in recommending suitable matches. In this paper, we introduce and extensively evaluate the use of" reciprocal},
 author = {Kleinerman, Akiva and Rosenfeld, Ariel and Kraus, Sarit},
 booktitle = {Proceedings of the 12th ACM conference on recommender systems},
 pages = {22--30},
 year = {2018},
 title = {Providing explanations for recommendations in reciprocal environments},
 venue = {… of the 12th ACM conference on …}
}

@article{lage2018human,
 abstract = {We often desire our models to be interpretable as well as accurate. Prior work on optimizing models for interpretability has relied on easy-to-quantify proxies for interpretability, such as sparsity or the number of operations required. In this work, we optimize for interpretability by directly including humans in the optimization loop. We develop an algorithm that minimizes the number of user studies to find models that are both predictive and interpretable and demonstrate our approach on several data sets. Our human subjects results show trends},
 author = {Lage, Isaac and Ross, Andrew Slavin and Kim, Been and Gershman, Samuel J and Doshi-Velez, Finale},
 journal = {Advances in neural information processing systems},
 year = {2018},
 publisher = {NIH Public Access},
 title = {Human-in-the-loop interpretability prior},
 venue = {Advances in neural …},
 volume = {31}
}

@article{miller2019explanation,
 abstract = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer},
 author = {Miller, Tim},
 journal = {Artificial intelligence},
 pages = {1--38},
 year = {2019},
 publisher = {Elsevier},
 title = {Explanation in artificial intelligence: Insights from the social sciences},
 venue = {Artificial intelligence},
 volume = {267}
}

@article{noothigattu2018interpretable,
 abstract = {Autonomous cyber-physical agents and systems play an increasingly large role in our lives. To ensure that agents behave in ways aligned with the values of the societies in which they operate, we must develop techniques that allow these agents to not only maximize their reward in an environment, but also to learn and follow the implicit constraints of society. These constraints and norms can come from any number of sources including regulations, business process guidelines, laws, ethical principles, social norms, and moral values. We},
 author = {Noothigattu, Ritesh and Bouneffouf, Djallel and Mattei, Nicholas and Chandra, Rachita and Madan, Piyush and Varshney, Kush and Campbell, Murray and Singh, Moninder and Rossi, Francesca},
 journal = {arXiv preprint arXiv:1809.08343},
 year = {2018},
 title = {Interpretable multi-objective reinforcement learning through policy orchestration},
 venue = {arXiv preprint arXiv …}
}

@inproceedings{odena2019tensorfuzz,
 abstract = {Neural networks are difficult to interpret and debug. We introduce testing techniques for neural networks that can discover errors occurring only for rare inputs. Specifically, we develop coverage-guided fuzzing (CGF) methods for neural networks. In CGF, random mutations of inputs are guided by a coverage metric toward the goal of satisfying user-specified constraints. We describe how approximate nearest neighbor (ANN) algorithms can provide this coverage metric for neural networks. We then combine these methods with},
 author = {Odena, Augustus and Olsson, Catherine and Andersen, David and Goodfellow, Ian},
 booktitle = {International Conference on Machine Learning},
 organization = {PMLR},
 pages = {4901--4911},
 year = {2019},
 title = {Tensorfuzz: Debugging neural networks with coverage-guided fuzzing},
 venue = {… on Machine Learning}
}

@article{ouarti2018out,
 abstract = {Deep neural networks are powerful machine learning approaches that have exhibited excellent results on many classification tasks. However, they are considered as black boxes and some of their properties remain to be formalized. In the context of image recognition, it is still an arduous task to understand why an image is recognized or not. In this study, we formalize some properties shared by eight state-of-the-art deep neural networks in order to grasp the principles allowing a given deep neural network to classify an image. Our results},
 author = {Ouarti, Nizar and Carmona, David},
 journal = {arXiv preprint arXiv:1808.04433},
 year = {2018},
 title = {Out of the Black Box: Properties of deep neural networks and their applications},
 venue = {arXiv preprint arXiv:1808.04433}
}

@inproceedings{park2019infossm,
 abstract = {The goal of system identification is to learn about underlying physics dynamics behind the time-series data. To model the probabilistic and nonparametric dynamics model, Gaussian process (GP) have been widely used; GP can estimate the uncertainty of prediction and avoid over-fitting. Traditional GPSSMs, however, are based on Gaussian transition model, thus often have difficulty in describing a more complex transition model, eg aircraft motions. To resolve the challenge, this paper proposes a framework using multiple GP transition},
 author = {Park, Young-Jin and Choi, Han-Lim},
 booktitle = {AIAA Scitech 2019 Forum},
 pages = {0681},
 year = {2019},
 title = {InfoSSM: Interpretable Unsupervised Learning of Nonparametric State-Space Model for Multi-modal Dynamics},
 venue = {AIAA Scitech 2019 Forum}
}

@article{petsiuk2018rise,
 abstract = {Deep neural networks are being used increasingly to automate data analysis and decision making, yet their decision-making process is largely unclear and is difficult to explain to the end users. In this paper, we address the problem of Explainable AI for deep neural networks that take images as input and output a class probability. We propose an approach called RISE that generates an importance map indicating how salient each pixel is for the model's prediction. In contrast to white-box approaches that estimate pixel importance using},
 author = {Petsiuk, Vitali and Das, Abir and Saenko, Kate},
 journal = {arXiv preprint arXiv:1806.07421},
 year = {2018},
 title = {Rise: Randomized input sampling for explanation of black-box models},
 venue = {arXiv preprint arXiv:1806.07421}
}

@incollection{ras2018explanation,
 abstract = {Issues regarding explainable AI involve four components: users, laws and regulations, explanations and algorithms. Together these components provide a context in which explanation methods can be evaluated regarding their adequacy. The goal of this chapter is to bridge the gap between expert users and lay users. Different kinds of users are identified and their concerns revealed, relevant statements from the General Data Protection Regulation are analyzed in the context of Deep Neural Networks (DNNs), a taxonomy for the},
 author = {Ras, Gabri{\"e}lle and van Gerven, Marcel and Haselager, Pim},
 booktitle = {Explainable and interpretable models in computer vision and machine learning},
 pages = {19--36},
 year = {2018},
 publisher = {Springer},
 title = {Explanation methods in deep learning: Users, values, concerns and challenges},
 venue = {Explainable and interpretable models …}
}

@article{seo2019regional,
 abstract = {Recently, many methods to interpret and visualize deep neural network predictions have been proposed, and significant progress has been made. However, a more class-discriminative and visually pleasing explanation is required. Thus, this paper proposes a region-based approach that estimates feature importance in terms of appropriately segmented regions. By fusing the saliency maps generated from multi-scale segmentations, a more class-discriminative and visually pleasing map is obtained. This paper incorporates},
 author = {Seo, Dasom and Oh, Kanghan and Oh, Il-Seok},
 journal = {IEEE Access},
 pages = {8572--8582},
 year = {2019},
 publisher = {IEEE},
 title = {Regional multi-scale approach for visually pleasing explanations of deep neural networks},
 venue = {IEEE Access},
 volume = {8}
}

@article{takahashi2018interpretable,
 abstract = {Embedding models for entities and relations are extremely useful for recovering missing facts in a knowledge base. Intuitively, a relation can be modeled by a matrix mapping entity vectors. However, relations reside on low dimension sub-manifolds in the parameter space of arbitrary matrices---for one reason, composition of two relations $\boldsymbol {M} _1,\boldsymbol {M} _2 $ may match a third $\boldsymbol {M} _3 $(eg composition of relations currency_of_country and country_of_film usually matches currency_of_film_budget},
 author = {Takahashi, Ryo and Tian, Ran and Inui, Kentaro},
 journal = {arXiv preprint arXiv:1805.09547},
 year = {2018},
 title = {Interpretable and compositional relation learning by joint training with an autoencoder},
 venue = {arXiv preprint arXiv:1805.09547}
}

@article{tomsett2018interpretable,
 abstract = {Several researchers have argued that a machine learning system's interpretability should be defined in relation to a specific agent or task: we should not ask if the system is interpretable, but to whom is it interpretable. We describe a model intended to help answer this question, by identifying different roles that agents can fulfill in relation to the machine learning system. We illustrate the use of our model in a variety of scenarios, exploring how an agent's role influences its goals, and the implications for defining interpretability. Finally, we make},
 author = {Tomsett, Richard and Braines, Dave and Harborne, Dan and Preece, Alun and Chakraborty, Supriyo},
 journal = {arXiv preprint arXiv:1806.07552},
 year = {2018},
 title = {Interpretable to whom? A role-based model for analyzing interpretable machine learning systems},
 venue = {arXiv preprint arXiv …}
}

@article{vaughan2018explainable,
 abstract = {Machine Learning algorithms are increasingly being used in recent years due to their flexibility in model fitting and increased predictive performance. However, the complexity of the models makes them hard for the data analyst to interpret the results and explain them without additional tools. This has led to much research in developing various approaches to understand the model behavior. In this paper, we present the Explainable Neural Network (xNN), a structured neural network designed especially to learn interpretable features},
 author = {Vaughan, Joel and Sudjianto, Agus and Brahimi, Erind and Chen, Jie and Nair, Vijayan N},
 journal = {arXiv preprint arXiv:1806.01933},
 year = {2018},
 title = {Explainable neural networks based on additive index models},
 venue = {arXiv preprint arXiv …}
}

@article{velivckovic2018deep,
 abstract = {We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level},
 author = {Veli{\v{c}}kovi{\'c}, Petar and Fedus, William and Hamilton, William L and Li{\`o}, Pietro and Bengio, Yoshua and Hjelm, R Devon},
 journal = {arXiv preprint arXiv:1809.10341},
 year = {2018},
 title = {Deep graph infomax},
 venue = {arXiv preprint arXiv …}
}

@article{wagstaff2018interpretable,
 abstract = {Automated detection of new, interesting, unusual, or anomalous images within large data sets has great value for applications from surveillance (eg, airport security) to science (observations that don't fit a given theory can lead to new discoveries). Many image data analysis systems are turning to convolutional neural networks (CNNs) to represent image content due to their success in achieving high classification accuracy rates. However, CNN representations are notoriously difficult for humans to interpret. We describe a new strategy},
 author = {Wagstaff, Kiri L and Lee, Jake},
 journal = {arXiv preprint arXiv:1806.08340},
 year = {2018},
 title = {Interpretable discovery in large image data sets},
 venue = {arXiv preprint arXiv:1806.08340}
}

@article{xiang2018verification,
 abstract = {This survey presents an overview of verification techniques for autonomous systems, with a focus on safety-critical autonomous cyber-physical systems (CPS) and subcomponents thereof. Autonomy in CPS is enabling by recent advances in artificial intelligence (AI) and machine learning (ML) through approaches such as deep neural networks (DNNs), embedded in so-called learning enabled components (LECs) that accomplish tasks from classification to control. Recently, the formal methods and formal verification community has},
 author = {Xiang, Weiming and Musau, Patrick and Wild, Ayana A and Lopez, Diego Manzanas and Hamilton, Nathaniel and Yang, Xiaodong and Rosenfeld, Joel and Johnson, Taylor T},
 journal = {arXiv preprint arXiv:1810.01989},
 year = {2018},
 title = {Verification for machine learning, autonomy, and neural networks survey},
 venue = {arXiv preprint arXiv …}
}

@article{zhang2018visual,
 abstract = {This paper reviews recent studies in understanding neural-network representations and learning neural networks with interpretable/disentangled middle-layer representations. Although deep neural networks have exhibited superior performance in various tasks, the interpretability is always the Achilles' heel of deep neural networks. At present, deep neural networks obtain high discrimination power at the cost of low interpretability of their black-box representations. We believe that high model interpretability may help people to break},
 author = {Zhang, Quanshi and Zhu, Song-Chun},
 journal = {arXiv preprint arXiv:1802.00614},
 year = {2018},
 title = {Visual interpretability for deep learning: a survey},
 venue = {arXiv preprint arXiv:1802.00614}
}

@inproceedings{zhang2018examining,
 abstract = {Given a pre-trained CNN without any testing samples, this paper proposes a simple yet effective method to diagnose feature representations of the CNN. We aim to discover representation flaws caused by potential dataset bias. More specifically, when the CNN is trained to estimate image attributes, we mine latent relationships between representations of different attributes inside the CNN. Then, we compare the mined attribute relationships with ground-truth attribute relationships to discover the CNN's blind spots and failure modes due},
 author = {Zhang, Quanshi and Wang, Wenguan and Zhu, Song-Chun},
 booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
 number = {1},
 year = {2018},
 title = {Examining cnn representations with respect to dataset bias},
 venue = {Proceedings of the AAAI Conference on …},
 volume = {32}
}

@inproceedings{zhang2018interpretable,
 abstract = {This paper proposes a method to modify a traditional convolutional neural network (CNN) into an interpretable CNN, in order to clarify knowledge representations in high conv-layers of the CNN. In an interpretable CNN, each filter in a high conv-layer represents a specific},
 author = {Zhang, Quanshi and Wu, Ying Nian and Zhu, Song-Chun},
 booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
 pages = {8827--8836},
 year = {2018},
 title = {Interpretable convolutional neural networks},
 venue = {Proceedings of the IEEE …}
}

@article{zhou2018interpreting,
 abstract = {The success of recent deep convolutional neural networks (CNNs) depends on learning hidden representations that can summarize the important factors of variation behind the data. In this work, we describe Network Dissection, a method that interprets networks by providing meaningful labels to their individual units. The proposed method quantifies the interpretability of CNN representations by evaluating the alignment between individual hidden units and visual semantic concepts. By identifying the best alignments, units are},
 author = {Zhou, Bolei and Bau, David and Oliva, Aude and Torralba, Antonio},
 journal = {IEEE transactions on pattern analysis and machine intelligence},
 number = {9},
 pages = {2131--2145},
 year = {2018},
 publisher = {IEEE},
 title = {Interpreting deep visual representations via network dissection},
 venue = {IEEE transactions on …},
 volume = {41}
}

@inproceedings{biran2017explanation,
 author = {Biran, O and Cotton, C},
 organization = {IJCAI},
 year = {2017},
 title = {Explanation and justification in ml: A survey},
 venue = {NA}
}

@article{chakraborti2017ai,
 abstract = {Among the many anticipated roles for robots in the future is that of being a human teammate. Aside from all the technological hurdles that have to be overcome with respect to hardware and control to make robots fit to work with humans, the added complication here is that humans have many conscious and subconscious expectations of their teammates-indeed, we argue that teaming is mostly a cognitive rather than physical coordination activity. This introduces new challenges for the AI and robotics community and requires fundamental},
 author = {Chakraborti, Tathagata and Kambhampati, Subbarao and Scheutz, Matthias and Zhang, Yu},
 journal = {arXiv preprint arXiv:1707.04775},
 year = {2017},
 title = {AI challenges in human-robot cognitive teaming},
 venue = {arXiv preprint arXiv …}
}

@article{goodman2017european,
 abstract = {We summarize the potential impact that the European Union's new General Data Protection Regulation will have on the routine use of machine learning algorithms. Slated to take effect as law across the EU in 2018, it will restrict automated individual decision-making (that is, algorithms that make decisions based on user-level predictors) which “significantly affect” users. The law will also effectively create a “right to explanation,” whereby a user can ask for an explanation of an algorithmic decision that was made about them. We argue that while},
 author = {Goodman, Bryce and Flaxman, Seth},
 journal = {AI magazine},
 number = {3},
 pages = {50--57},
 year = {2017},
 title = {European Union regulations on algorithmic decision-making and a “right to explanation”},
 venue = {AI magazine},
 volume = {38}
}

@article{lipton2018mythos,
 abstract = {INTRODUCTION Until recently, humans had a monopoly on agency in society. If you applied  for a job, loan, or bail, a human decided your fate. If you went to the hospital, a human would  attempt to categorize your malady and recommend treatment. For consequential decisions such  as these, you might demand an explanation from the decision-making agent. If your loan application  is denied, for example, you might want to understand the agent's reasoning in a bid to strengthen  your next application. If the decision was based on a flawed premise, you might contest this premise},
 author = {Lipton, Zachary C},
 journal = {Queue},
 number = {3},
 pages = {31--57},
 year = {2018},
 publisher = {ACM New York, NY, USA},
 title = {The Mythos of Model Interpretability: In machine learning, the concept of interpretability is both important and slippery.},
 venue = {Queue},
 volume = {16}
}

@inproceedings{lundberg2017unified,
 abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and},
 author = {Lundberg, Scott M and Lee, Su-In},
 booktitle = {Proceedings of the 31st international conference on neural information processing systems},
 pages = {4768--4777},
 year = {2017},
 title = {A unified approach to interpreting model predictions},
 venue = {… of the 31st international conference on neural …}
}

@inproceedings{zhang2018interpreting,
 abstract = {This paper learns a graphical model, namely an explanatory graph, which reveals the knowledge hierarchy hidden inside a pre-trained CNN. Considering that each filter in a conv-layer of a pre-trained CNN usually represents a mixture of object parts, we propose a simple yet efficient method to automatically disentangles different part patterns from each filter, and construct an explanatory graph. In the explanatory graph, each node represents a part pattern, and each edge encodes co-activation relationships and spatial relationships},
 author = {Zhang, Quanshi and Cao, Ruiming and Shi, Feng and Wu, Ying Nian and Zhu, Song-Chun},
 booktitle = {Thirty-Second AAAI Conference on Artificial Intelligence},
 year = {2018},
 title = {Interpreting cnn knowledge via an explanatory graph},
 venue = {Thirty-Second AAAI Conference …}
}

@article{riguzzi2017survey,
 abstract = {In this paper, we survey these three proposals and experimentally evaluate them  linearly increases with the number of individuals of the domain for approaches exploiting lifted variable elimination  it is constant in case of WFOMC, thus indicating that the latter is able to lift a larger},
 author = {Riguzzi, Fabrizio and Bellodi, Elena and Zese, Riccardo and Cota, Giuseppe and Lamma, Evelina},
 journal = {International Journal of Approximate Reasoning},
 pages = {313--333},
 year = {2017},
 publisher = {Elsevier},
 title = {A survey of lifted inference approaches for probabilistic logic programming under the distribution semantics},
 venue = {International Journal of …},
 volume = {80}
}

@inproceedings{amershi2010examining,
 abstract = {End-user interactive concept learning is a technique for interacting with large unstructured datasets, requiring insights from both human-computer interaction and machine learning. This note re-examines an assumption implicit in prior interactive machine learning research, that interaction should focus on the question" what class is this object?". We broaden interaction to include examination of multiple potential models while training a machine learning system. We evaluate this approach and find that people naturally adopt revision in},
 author = {Amershi, Saleema and Fogarty, James and Kapoor, Ashish and Tan, Desney},
 booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
 pages = {1357--1360},
 year = {2010},
 title = {Examining multiple potential models in end-user interactive concept learning},
 venue = {Proceedings of the SIGCHI …}
}

