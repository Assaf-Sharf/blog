---
title: "Kafka Streams 101"
date: 2021-09-15
header:
  image: "/images/blog cover.jpg"
tags: []
excerpt: ""
---

Apache Kafka nowadays is clearly the leading technology concerning message brokers. It's scalable, resilient, and easy to use. Moreover, it leverages a bunch of interesting client libraries that offer a vast set of additional feature. One of this libraries is _kafka-streams_. 

Kafka streams brings a completely full stateful streaming system based directly on top of Kafka. Moreover, it introduces many interesting concepts, like the duality between topics and database tables. Implementing such concepts, kafka streams provide us many useful operation on topics, such as joining messages, grouping capabilities, and so on.

Because the kafka-streams library is very large and quite complex, this article will introduce only its main features, such use the architecture, the types `KStream`, `KTable`, and `GlobalKTable`, and some information about the _state store_.

## 1. Set up

As we said, the Kafka streams are implemented using a set of client libraries. In addition, we will use the Circe library to deal with JSON messages. Using Scala as the language to make some experiments, we have to declare the following dependencies in the `build.sbt` file:

```sbt
libraryDependencies ++= Seq(
  "org.apache.kafka" %  "kafka-clients"        % "2.8.0",
  "org.apache.kafka" %  "kafka-streams"        % "2.8.0",
  "org.apache.kafka" %% "kafka-streams-scala"  % "2.8.0",
  "io.circe"         %% "circe-core"           % "0.14.1",
  "io.circe"         %% "circe-generic"        % "0.14.1",
  "io.circe"         %% "circe-parser"         % "0.14.1"
)
```

Among the dependencies, we find the `kafka-streams-scala` libraries, which is a Scala wrapper built around the Java `kafka-streams` library. In fact, using implicit resolution, the tailored Scala library avoid some boilerplate code.

We will use version 2.8.0, of Kafka, the latest stable version at the moment. As we've done in the article [ZIO Kafka: A Practical Streaming Tutorial](https://blog.rockthejvm.com/zio-kafka/), we will start the Kafka broker using a Docker container. So, the `docker-compose.yml` file describing the container is the following:

```yaml
version: '2'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:6.2.0
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:6.2.0
    hostname: broker
    container_name: broker
    depends_on:
      - zookeeper
    ports:
      - "29092:29092"
      - "9092:9092"
      - "9101:9101"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: localhost
```

Please, refer to the above article for further details on starting the Kafka broker inside Docker.

As usual, we need a use case to work with. Imagine we have an e-commerce site and want to use Kafka to implement some part of the orders' workflow:

```scala
type UserId = String
type Product = String
type OrderId = String

case class Order(orderId: OrderId, user: UserId, products: List[Product], amount: Double)
```

Since this information will use Kafka messages and topics, we need to set up our application, creating the topic in the Kafka broker storing orders. We call the topic `orders-by-user`. As we did for the article "ZIO Kafka: A Practical Streaming Tutorial", we use the clients libraries contained in the Docker image:

```shell
kafka-topics \
  --bootstrap-server localhost:9092 \
  --topic orders-by-user \
  --create
```

To improve the readability of the code we will write, we define also a constant containing the name of the topic:

```scala
final val OrdersByUserTopic = "orders-by-user"
```

As the name of the topic suggests, its messages will have the `UserId` as keys. 

## 2. Basics

As we said, the kafka streams library is a client library, and it allows handling workflows that read from a topic and write to another topic as a stream.

As we should know, we build streaming applications around three concepts: sources, flows (or pipes), and sinks. Often, we represent streams as a series of token, generated by a source, transformed by flows and consumed by sinks:

TODO: Insert a graphic representation of a stream

A source is where the execution starts, and information is created. Sources generate token, and in kafka streams they are represented by a topic receiving messages. 

A flow is nothing more than a transformation applied to every token. In functional programming, we represent flows using function such as `map`, `filter`, `flatMap`, and so on.

Last but not least, a sink is where the token are consumed. After a sink, the token doesn't exist anymore. In kafka streams, sinks can consume token to a Kafka topics, or use anything other technology to consume them (i.e., the standard output, a database, etc.)

In kafka streams jargon, both sources, flows, and sinks are called _stream processors_. A streaming application is nothing more than a graph where each node is a processor, and edges are called _streams_. We can call such graph a _topology_.

TODO: Image of a topology

So, with these bullets in our Kafka gun, let's proceed dive a little deeper in how we can implement our use case using the kafka-streams library.

## 3. Creating the Topology

First thing, we need to define the topology of our streaming application. Fortunately, it's an esy job, as we can use the builder provided by the library:

```scala
val builder = new StreamsBuilder
```

Then, we need to define our source processor. The source, will read incoming messages from the Kafka topic `orders-by-user`, we've just defined. Differently from other streaming libraries, such as Akka Streams, the kafka-streams library doesn't define specific types for sources, pipes, and sinks:

```scala
val usersOrdersStreams: KStream[UserId, Order] = builder.stream[UserId, Order](OrdersByUserTopic)
```

TODO


