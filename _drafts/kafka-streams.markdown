---
title: "Kafka Streams 101"
date: 2021-09-15
header:
  image: "/images/blog cover.jpg"
tags: []
excerpt: ""
---

Apache Kafka nowadays is clearly the leading technology concerning message brokers. It's scalable, resilient, and easy to use. Moreover, it leverages a bunch of interesting client libraries that offer a vast set of additional feature. One of this libraries is _kafka-streams_. 

Kafka streams brings a completely full stateful streaming system based directly on top of Kafka. Moreover, it introduces many interesting concepts, like the duality between topics and database tables. Implementing such concepts, kafka streams provide us many useful operation on topics, such as joining messages, grouping capabilities, and so on.

Because the kafka-streams library is very large and quite complex, this article will introduce only its main features, such use the architecture, the types `KStream`, `KTable`, and `GlobalKTable`, and some information about the _state store_.

## 1. Set up

As we said, the Kafka streams are implemented using a set of client libraries. In addition, we will use the Circe library to deal with JSON messages. Using Scala as the language to make some experiments, we have to declare the following dependencies in the `build.sbt` file:

```sbt
libraryDependencies ++= Seq(
  "org.apache.kafka" %  "kafka-clients"        % "2.8.0",
  "org.apache.kafka" %  "kafka-streams"        % "2.8.0",
  "org.apache.kafka" %% "kafka-streams-scala"  % "2.8.0",
  "io.circe"         %% "circe-core"           % "0.14.1",
  "io.circe"         %% "circe-generic"        % "0.14.1",
  "io.circe"         %% "circe-parser"         % "0.14.1"
)
```

Among the dependencies, we find the `kafka-streams-scala` libraries, which is a Scala wrapper built around the Java `kafka-streams` library. In fact, using implicit resolution, the tailored Scala library avoid some boilerplate code.

We will use version 2.8.0, of Kafka, the latest stable version at the moment. As we've done in the article [ZIO Kafka: A Practical Streaming Tutorial](https://blog.rockthejvm.com/zio-kafka/), we will start the Kafka broker using a Docker container. So, the `docker-compose.yml` file describing the container is the following:

```yaml
version: '2'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:6.2.0
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:6.2.0
    hostname: broker
    container_name: broker
    depends_on:
      - zookeeper
    ports:
      - "29092:29092"
      - "9092:9092"
      - "9101:9101"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: localhost
```

Please, refer to the above article for further details on starting the Kafka broker inside Docker.

As usual, we need a use case to work with. Imagine we have an e-commerce site and want to use Kafka to implement some part of the orders' workflow:

```scala
type UserId = String
type Product = String
type OrderId = String

case class Order(orderId: OrderId, user: UserId, products: List[Product], amount: Double)
```

Since this information will use Kafka messages and topics, we need to set up our application, creating the topic in the Kafka broker storing orders. We call the topic `orders-by-user`. As we did for the article "ZIO Kafka: A Practical Streaming Tutorial", we use the clients libraries contained in the Docker image:

```shell
kafka-topics \
  --bootstrap-server localhost:9092 \
  --topic orders-by-user \
  --create
```

To improve the readability of the code we will write, we define also a constant containing the name of the topic:

```scala
final val OrdersByUserTopic = "orders-by-user"
```

As the name of the topic suggests, its messages will have the `UserId` as keys. 

## 2. Basics

As we said, the kafka streams library is a client library, and it allows handling workflows that read from a topic and write to another topic as a stream.

As we should know, we build streaming applications around three concepts: sources, flows (or pipes), and sinks. Often, we represent streams as a series of token, generated by a source, transformed by flows and consumed by sinks:

TODO: Insert a graphic representation of a stream

A source is where the execution starts, and information is created. Sources generate token, and in kafka streams they are represented by a topic receiving messages. 

A flow is nothing more than a transformation applied to every token. In functional programming, we represent flows using function such as `map`, `filter`, `flatMap`, and so on.

Last but not least, a sink is where the token are consumed. After a sink, the token doesn't exist anymore. In kafka streams, sinks can consume token to a Kafka topics, or use anything other technology to consume them (i.e., the standard output, a database, etc.)

In kafka streams jargon, both sources, flows, and sinks are called _stream processors_. A streaming application is nothing more than a graph where each node is a processor, and edges are called _streams_. We can call such graph a _topology_.

TODO: Image of a topology

So, with these bullets in our Kafka gun, let's proceed dive a little deeper in how we can implement our use case using the kafka-streams library.

## 3. Creating the Topology

First thing, we need to define the topology of our streaming application. Fortunately, it's an esy job, as we can use the builder provided by the library:

```scala
val builder = new StreamsBuilder
```

Then, we need to define our source processor. The source, will read incoming messages from the Kafka topic `orders-by-user`, we've just defined. Differently from other streaming libraries, such as Akka Streams, the kafka-streams library doesn't define specific types for sources, pipes, and sinks:

```scala
val usersOrdersStreams: KStream[UserId, Order] = builder.stream[UserId, Order](OrdersByUserTopic)
```

There are a lot of things going on the above code. First, we introduced the first notable citizen of the kafka stream library: the `KStream[K, V]` type. We can imagine a `KStream` as a regular stream of Kafka messages. Each message as a key of type `K` and a value of type `V`. 

Moreover, the API to build a new stream seems to be very straightforward because there are a lot of "implicit magic" under the hood. In fact, the complete signature of the methods is

```scala
// Scala kafka-stream library
def stream[K, V](topic: String)(implicit consumed: Consumed[K, V]): KStream[K, V]
```

You may wonder what the heck is a `Consumed[K, V]` is. Well, it's the Java way to provide to the stream a `Serde` for the key and for the value of the Kafka message. And, what's a `Serde`? The `Serde` word stands for `Serializer` and `Deserializer` and an instance of a `Serde` provides the logic to read and write a message from and to a Kafka topic.

So, if we have a `Serde[R]` instance, we can deserialize and serialize messages of the type `R`. In this article we will use JSON format for the payload of Kafka messages. In Scala, one of the most used libraries to marshall and unmarshall JSON into objects is Circe. We already talk about Circe in the post [Unleashing the Power of HTTP Apis: The Http4s Library](https://blog.rockthejvm.com/http4s-tutorial/), when we used it together with the Http4s library.

This time, we use Circe to create a `Serde` instance. The Scala kafka streams library comes with a lot of `Serde` instances for all the primitive types:

```scala
// Scala kafka-stream library
object Serdes {
  implicit def stringSerde: Serde[String]
  implicit def longSerde: Serde[Long]
  implicit def javaLongSerde: Serde[java.lang.Long]
  implicit def byteArraySerde: Serde[Array[Byte]]
  implicit def bytesSerde: Serde[org.apache.kafka.common.utils.Bytes]
  implicit def byteBufferSerde: Serde[ByteBuffer]
  implicit def shortSerde: Serde[Short]
  implicit def javaShortSerde: Serde[java.lang.Short]
  implicit def floatSerde: Serde[Float]
  implicit def javaFloatSerde: Serde[java.lang.Float]
  implicit def doubleSerde: Serde[Double]
  implicit def javaDoubleSerde: Serde[java.lang.Double]
  implicit def intSerde: Serde[Int]
  implicit def javaIntegerSerde: Serde[java.lang.Integer]
  implicit def uuidSerde: Serde[UUID] = JSerdes.UUID()
  // ...
}
```

In addition, the `Serdes` object defines the function `fromFn`, which we can use to build our custom instance of `Serde`:

```scala
// Scala kafka-stream library
def fromFn[T >: Null](serializer: T => Array[Byte], deserializer: Array[Byte] => Option[T]): Serde[T]
```

Wiring all the information together, we can use the above function to create a `Serde` using Circe:

```scala
def serde[A >: Null : Decoder : Encoder]: Serde[A] = {
  val serializer = (a: A) => a.asJson.noSpaces.getBytes
  val deserializer = (aAsBytes: Array[Byte]) => {
    val aAsString = new String(aAsBytes)
    val aOrError = decode[A](aAsString)
    aOrError match {
      case Right(a) => Option(a)
      case Left(error) =>
        println(s"There was an error converting the message $aOrError, $error")
        Option.empty
    }
  }
  Serdes.fromFn[A](serializer, deserializer)
}
```

The `serde` function constraints the type `A` to have Circe `Decoder` and `Encoder` implicitly defined in the scope. Then, it uses the type class `Encoder[A]` to create a JSON string: 

```scala
a.asJson
``` 

Moreover, the function uses the type class `Decoder[A]` to parse a JSON string into an object:

```scala
decode[A](aAsString)
```

Fortunately, we can autogenerate Circe `Encoder` and `Decoder` type classes using the import of the resource `import io.circe.generic.auto._`.