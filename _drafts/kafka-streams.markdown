---
title: "Kafka Streams 101"
date: 2021-09-15
header:
  image: "/images/blog cover.jpg"
tags: []
excerpt: ""
---

Apache Kafka nowadays is clearly the leading technology concerning message brokers. It's scalable, resilient, and easy to use. Moreover, it leverages a bunch of interesting client libraries that offer a vast set of additional feature. One of this libraries is _kafka-streams_. 

Kafka streams brings a completely full stateful streaming system based directly on top of Kafka. Moreover, it introduces many interesting concepts, like the duality between topics and database tables. Implementing such concepts, kafka streams provide us many useful operation on topics, such as joining messages, grouping capabilities, and so on.

Because the kafka-streams library is very large and quite complex, this article will introduce only its main features, such use the architecture, the types `KStream`, `KTable`, and `GlobalKTable`, and some information about the _state store_.

## 1. Set up

As we said, the Kafka streams are implemented using a set of client libraries. In addition, we will use the Circe library to deal with JSON messages. Using Scala as the language to make some experiments, we have to declare the following dependencies in the `build.sbt` file:

```sbt
libraryDependencies ++= Seq(
  "org.apache.kafka" %  "kafka-clients"        % "2.8.0",
  "org.apache.kafka" %  "kafka-streams"        % "2.8.0",
  "org.apache.kafka" %% "kafka-streams-scala"  % "2.8.0",
  "io.circe"         %% "circe-core"           % "0.14.1",
  "io.circe"         %% "circe-generic"        % "0.14.1",
  "io.circe"         %% "circe-parser"         % "0.14.1"
)
```

Among the dependencies, we find the `kafka-streams-scala` libraries, which is a Scala wrapper built around the Java `kafka-streams` library. In fact, using implicit resolution, the tailored Scala library avoid some boilerplate code.

We will use version 2.8.0, of Kafka, the latest stable version at the moment. As we've done in the article [ZIO Kafka: A Practical Streaming Tutorial](https://blog.rockthejvm.com/zio-kafka/), we will start the Kafka broker using a Docker container. So, the `docker-compose.yml` file describing the container is the following:

```yaml
version: '2'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:6.2.0
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:6.2.0
    hostname: broker
    container_name: broker
    depends_on:
      - zookeeper
    ports:
      - "29092:29092"
      - "9092:9092"
      - "9101:9101"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: localhost
```

Please, refer to the above article for further details on starting the Kafka broker inside Docker.

As usual, we need a use case to work with. Imagine we have an e-commerce site and want to use Kafka to implement some part of the orders' workflow:

```scala
type UserId = String
type Product = String
type OrderId = String

case class Order(orderId: OrderId, user: UserId, products: List[Product], amount: Double)
```

Since this information will use Kafka messages and topics, we need to set up our application, creating the topic in the Kafka broker storing orders. We call the topic `orders-by-user`. As we did for the article "ZIO Kafka: A Practical Streaming Tutorial", we use the clients libraries contained in the Docker image:

```shell
kafka-topics \
  --bootstrap-server localhost:9092 \
  --topic orders-by-user \
  --create
```

To improve the readability of the code we will write, we define also a constant containing the name of the topic:

```scala
final val OrdersByUserTopic = "orders-by-user"
```

As the name of the topic suggests, its messages will have the `UserId` as keys. 

## 2. Basics

As we said, the kafka streams library is a client library, and it allows handling workflows that read from a topic and write to another topic as a stream.

As we should know, we build streaming applications around three concepts: sources, flows (or pipes), and sinks. Often, we represent streams as a series of token, generated by a source, transformed by flows and consumed by sinks:

TODO: Insert a graphic representation of a stream

A source is where the execution starts, and information is created. Sources generate token, and in kafka streams they are represented by a topic receiving messages. 

A flow is nothing more than a transformation applied to every token. In functional programming, we represent flows using function such as `map`, `filter`, `flatMap`, and so on.

Last but not least, a sink is where the token are consumed. After a sink, the token doesn't exist anymore. In kafka streams, sinks can consume token to a Kafka topics, or use anything other technology to consume them (i.e., the standard output, a database, etc.)

In kafka streams jargon, both sources, flows, and sinks are called _stream processors_. A streaming application is nothing more than a graph where each node is a processor, and edges are called _streams_. We can call such graph a _topology_.

TODO: Image of a topology

So, with these bullets in our Kafka gun, let's proceed dive a little deeper in how we can implement our use case using the kafka-streams library.

## 3. Messages Serialization and Deserialization

If we want to create any structure on top of Kafka topics, such as stream, we need a standard way to serialize objects into a topic, and to deserialize messages from topic to objects. While the Kafka library defines serializers and deserializers as different types, the Kafka stream  library uses the so call `Serde` type.

what's a `Serde`? The `Serde` word stands for `Serializer` and `Deserializer` and an instance of a `Serde` provides the logic to read and write a message from and to a Kafka topic.

So, if we have a `Serde[R]` instance, we can deserialize and serialize messages of the type `R`. In this article we will use JSON format for the payload of Kafka messages. In Scala, one of the most used libraries to marshall and unmarshall JSON into objects is Circe. We already talk about Circe in the post [Unleashing the Power of HTTP Apis: The Http4s Library](https://blog.rockthejvm.com/http4s-tutorial/), when we used it together with the Http4s library.

This time, we use Circe to create a `Serde` instance. The Scala kafka streams library comes with a lot of `Serde` instances for all the primitive types:

```scala
// Scala kafka-stream library
object Serdes {
  implicit def stringSerde: Serde[String]
  implicit def longSerde: Serde[Long]
  implicit def javaLongSerde: Serde[java.lang.Long]
  implicit def byteArraySerde: Serde[Array[Byte]]
  implicit def bytesSerde: Serde[org.apache.kafka.common.utils.Bytes]
  implicit def byteBufferSerde: Serde[ByteBuffer]
  implicit def shortSerde: Serde[Short]
  implicit def javaShortSerde: Serde[java.lang.Short]
  implicit def floatSerde: Serde[Float]
  implicit def javaFloatSerde: Serde[java.lang.Float]
  implicit def doubleSerde: Serde[Double]
  implicit def javaDoubleSerde: Serde[java.lang.Double]
  implicit def intSerde: Serde[Int]
  implicit def javaIntegerSerde: Serde[java.lang.Integer]
  implicit def uuidSerde: Serde[UUID] = JSerdes.UUID()
  // ...
}
```

In addition, the `Serdes` object defines the function `fromFn`, which we can use to build our custom instance of `Serde`:

```scala
// Scala kafka-stream library
def fromFn[T >: Null](serializer: T => Array[Byte], deserializer: Array[Byte] => Option[T]): Serde[T]
```

Wiring all the information together, we can use the above function to create a `Serde` using Circe:

```scala
def serde[A >: Null : Decoder : Encoder]: Serde[A] = {
  val serializer = (a: A) => a.asJson.noSpaces.getBytes
  val deserializer = (aAsBytes: Array[Byte]) => {
    val aAsString = new String(aAsBytes)
    val aOrError = decode[A](aAsString)
    aOrError match {
      case Right(a) => Option(a)
      case Left(error) =>
        println(s"There was an error converting the message $aOrError, $error")
        Option.empty
    }
  }
  Serdes.fromFn[A](serializer, deserializer)
}
```

The `serde` function constraints the type `A` to have Circe `Decoder` and `Encoder` implicitly defined in the scope. Then, it uses the type class `Encoder[A]` to create a JSON string:

```scala
a.asJson
``` 

Moreover, the function uses the type class `Decoder[A]` to parse a JSON string into an object:

```scala
decode[A](aAsString)
```

Fortunately, we can autogenerate Circe `Encoder` and `Decoder` type classes importing `io.circe.generic.auto._`.

Now that we understand the types the library uses to write and read from a Kafka topic, and that we create some utility functions to deal with such types, we can go on and understand how to build our first stream topology.

## 4. Creating the Topology

First thing, we need to define the topology of our streaming application. We will use the _Stream DSL` to define it. This DSL, built on top of the low level [Processor API](https://docs.confluent.io/platform/current/streams/developer-guide/processor-api.html#streams-developer-guide-processor-api), is easier to use and master, having a declarative approach. Using the Stream DSL we don't have to deal with stream processor nodes directly. The Kafka stream library will create the best processors' topology reflecting the operation with need. 

So, first, we need an instance of the builder type provided by the library:

```scala
val builder = new StreamsBuilder
```

The builder lets us creating the basic type of the Stream DSL, which are the`KStream`, `Ktable`, and `GlobalKTable` types. Let's see how.

### 4.1. Building a `KStream`

First, we need to define our source. The source, will read incoming messages from the Kafka topic `orders-by-user`, we've just defined. Differently from other streaming libraries, such as Akka Streams, the kafka-streams library doesn't define specific types for sources, pipes, and sinks:

```scala
val usersOrdersStreams: KStream[UserId, Order] = builder.stream[UserId, Order](OrdersByUserTopic)
```

There are a lot of things going on the above code. First, we introduced the first notable citizen of the kafka stream library: the `KStream[K, V]` type. We can imagine a `KStream` as a regular stream of Kafka messages. Each message as a key of type `K` and a value of type `V`. 

Moreover, the API to build a new stream seems to be very straightforward because there are a lot of "implicit magic" under the hood. In fact, the complete signature of the methods is

```scala
// Scala kafka-stream library
def stream[K, V](topic: String)(implicit consumed: Consumed[K, V]): KStream[K, V]
```

You may wonder what the heck is a `Consumed[K, V]` is. Well, it's the Java way to provide to the stream a `Serde` for the key and for the value of the Kafka message. Having previously defined the `serde` function, we can build a `Serde` for our `Order` class in a straightforward way. We usually put such classes in the companion object:

```scala
object Order {
  implicit val orderSerde: Serde[Order] = serde[Order]
}
```

We can go even further on the implicit generation of the `Serde` class. In fact, if we define the previous `serde` function as `implicit`, the Scala compiler will generate automatically the `orderSerde`, since the `Order` class fulfills all the needed context bounds.

So, just as a recap, the following implicit resolution takes place:

```
Order => Decoder[Order] / Encoder[Order] => Serde[Order] => Consume[Order]
```

Why do we need `Serde` types to be implicit? The main reason is that the Scala kafka stream provides the object `ImplicitConversions`. Inside this `object`, we find a lot of useful conversion functions that, given `Serde` objects for the key and the values of a Kafka message, let us define a lot of other types, such as the above `Consumed`. Again, all these conversions save us writing a lot of boilerplate code, which we should have written in Java, for example.

After the definition of the needed `Serde` types we can return to the definition of our streams. As we said, a `KStream[K, V]` represents a stream of Kafka messages. This type defines many useful functions on it, such as `filter`, `map`, `flatMap`, etc. Say, for example, that we want to filter all the orders with an amount greater than 1,000.00 euro. We can use the `filter` function (the library also provides a useful function `filterNot`):

```scala
val expensiveOrders: KStream[UserId, Order] = usersOrdersStreams.filter { (userId, order) =>
  order.amount >= 1000
}
```

Let's say, instead, that we want to extract a stream of all the ordered products, maintaining the
`UserId` a the message key. Since we want to map only the values of the Kafka messages, we can use the `mapValue` function:

```scala
val purchasedListOfProductsStream: KStream[UserId, List[Product]] = usersOrdersStreams.mapValues { order => 
  order.products
}
```

Going further, we can obtain a `KStream[UserId, Product]` instead, just using the `flatMapValues` function:

```scala
val purchasedProductsStream: KStream[UserId, Product] = usersOrdersStreams.flatMapValues { order =>
  order.products
}
```

Returning to our topology, we can represent each function we applied to the above `KStream` as an edge connecting the source `KStream` and the resulting `KStream`. So, until now, we saw an example of source processor, i.e. the `KStream` created directly using the `StreamBuilder`, and many examples of stream processor. Now, it's time to introduce sink processors.

As the name said, a sink processor represents a terminal node of our stream topology. We can apply no other function to the stream after a sink is reached. Two examples of sink processors are the `foreach` and `to` methods.

The `foreach` method applies to a stream a given function:

```scala
// Scala kafka-stream library
def foreach(action: (K, V) => Unit): Unit
```

As an example, imagine we want to print all the product purchased by a user. We can call the ` foreach` method directly on the `purchasedProductsStream` stream:

```scala
purchasedProductsStream.foreach { (userId, product) => 
  println(s"The user $userId purchased the product $product")
}
```

Another interesting sink processor is the `to` method, which persist the messages of the stream into a new topic:

```scala
expensiveOrders.to("suspicious-orders")
```

In the above example, we are writing all the order with an amount greater than 1,000 Euro in a dedicated topic, probably to perform some kind of fraud analysis on them. Also, this time the Scala kafka stream library saves us to type a lot of code. In fact, the complete signature of the `to` method is the following:

```scala
// Scala kafka-stream library
def to(topic: String)(implicit produced: Produced[K, V]): Unit
```

Again, the implicit instance of the `Produced` type, which is a wrapper around key and value `Serde` is produced automatically by the functions in the `ImplicitConversions` object, plus our `serde` implicit function.

### 4.2. Building `KTable` and `GlobalKTable` Processors

The Kafka stream libraries offers two more kind of processors: `KTable`, and `GlobalKTable`. We build both processors on top of a _compacted topic_. We can think of a compacted topic as a table, indexed by the messages' key. Messages are not deleted by the broker using a time to live policy. Every time a new message arrives, a "row" it's added to the "table" if the key were not present, or the value associated with the key is updated otherwise. To delete a "row" from the "table", we just send to the topic a `null` value associated with the selected key.

To make a topic compacted, we need to specify it during its creation:

```shell
kafka-topics \
  --bootstrap-server localhost:9092 \
  --topic discount-profiles-by-user \
  --create \
  --config "cleanup.policy=compact"
```

The above topic will be the starting point to extend our Kafka stream application. In fact, the messages in it has a `UserId` as key, and a discount profile as value. A discount profile tells for each user which is the discount the e-commerce site could apply to the orders of a user. For sake of simplicity, we represent profiles as simple `String`:

```scala
type Profile = String
```

Which is the difference between the two? Well, the difference is that a `KTable` is partitioned between the nodes of the Kafka cluster. However, every node of the cluster receive a full copy of the messages of a `GlobalKTable`. So, be careful with `GlobalKTable`.

Creating a `KTable` or a `GlobalKTable` it's easy. As an example, let's create a `KTable` on top of the `discount-profiles-by-user` topic. Returning to our example, as the users' number of our e-commerce might be high, we need to partition the information among the nodes of the Kafka cluster. So, let's create the `KTable`:

```scala
final val DiscountProfilesByUserTopic = "discount-profiles-by-user"

val userProfilesTable: KTable[UserId, Profile] =
  builder.table[UserId, Profile](DiscountProfilesByUserTopic)
```

As you can imagine, there is more behind the scene than what we can see. Again, using the chain of implicit conversions, the Scala Kafka stream library is creating for us an instance of the `Consumed` class, which is mainly used to pass `Serde` instances around. In this particular case, we are using the `Serdes.stringSerde` implicit object, both for the key and for the value of the topic.

The methods defined on the `KTable` type are more or less the same as those defined on a `KStream`. In addition, a `KTable` can be easily converted into a `KStream` using the following method (or one of its variants):

```scala
// Scala kafka-stream library
def toStream: KStream[K, V]
```

As we can imagine, creating a `GlobalKTable` is easy as well, we only need a compacted topic containing a number of keys that is affordable for each the cluster node:

```shell
kafka-topics \
  --bootstrap-server localhost:9092 \
  --topic discounts \
  --create \
  --config "cleanup.policy=compact"
```

We can think the  number of different instances of discount `Profile` is very low. So. let's create a `GlobalKTable` on top of a topic mapping each discount profile to an effective discount. First, we define the type modelling a discount:

```scala
final val DiscountsTopic = "discounts"

case class Discount(profile: Profile, amount: Double)
```

Then, we can create an instance of the needed `GlobalKTable`:

```scala
val discountProfilesGTable: GlobalKTable[Profile, Discount] = 
  builder.globalTable[Profile, Discount](DiscountsTopic)
```

Again, under the hood, an instance of a `Consumed` object is created by the library.

The `GlobalKTable` type doesn't define any interesting method. So, why should we ever create an instance of a `GlobalKTable`? The answer to this legitimate question allows us to introduce the next big feature of Kafka stream: Joins.


