---
layout: single
title:  "핸즈온 머신러닝 - 8"
categories : scikit-learn
tag : [scikit-learn, machine-learning, python]
toc: true
toc_sticky: true

---

![header](https://capsule-render.vercel.app/api?type=waving&color=a2dcec&height=300&section=header&text=핸즈온 머신러닝 - 8&fontSize=40&animation=fadeIn&fontAlignY=38&fontColor=FFFFFF)

- 참고 : [핸즈온 머신러닝 2판](http://www.kyobobook.co.kr/product/detailViewKor.laf?mallGb=KOR&ejkGb=KOR&barcode=9791162242964)

------------------------------------------------------

&nbsp;

## 차원 축소

- 머신러닝 문제는 훈련 샘플 각각이 수천 수백개의 특성을 가지는 경우가 존재

- 많은 특성은 훈련을 느리게 할수 있고 좋은 솔루션을 찾기 어렵게 만듬

  >  이런 문제를 **차원의 저주** 라고함	

- 차원을 축소하면 일부 정보를 소실하지만 잃는 정보가 많지 않음
- 차원을 축소하여 훈련 속도 증가, 데이터 시각화 에도 유용함
- 차원 축소에서 사용되는 두 가지 접근 방법 (투영, 매니폴드 학습) 존재

&nbsp;

### 차원의 저주

- 현실은 3차원 세계임으로 고차원 공간을 직관적으로 파악하기 어려움
  - 훈련 세트의 차원이 클수록 과대적합의 위험성이 커짐
- 차원의 저주를 해결하는 방법중 하나는 훈련 샘플의 밀도가 충분히 높아질 때까지 훈련 세트를 키우는 방법
  - 이 방법은 실제로 엄청난 수의 데이터 샘플을 추가 수집해야 함으로 불가능

&nbsp;

### 차원 축소를 위한 접근 방법

- 차원을 감소시키는 두 가지 주요한 접근법인 투영, 매니폴드 학습이 존재

&nbsp;

**투영**

- 모든 훈련 샘플이 고차원 안의 저차원 **부분공간**에 놓여 있음
- 균일하게 펴져있지 않음

- 투영은 높은 차원의 데이터를 낮은 차원의 데이터로 옮길때 균일하게 펼치는 것

&nbsp;

**매니폴드 학습**

- 많은 차원 축소 알고리즘이 훈련 샘플이 놓여 있는 매니폴드를 모델링하는 식으로 작동
  - 대부분 실제 고차원 데이터셋이 더 낮은 저차원 매니폴드에 가깝게 놓여 있다는 매니폴드 가정 또는 매니폴드 가설에 근거

&nbsp;

> 모델을 훈련하기 전에 훈련 세트의 차원을 감소시키면 훈련 속도는 증가하지만 항상 더 좋은 솔루션은 아님, 데이터셋에 달려있음

&nbsp;

### PCA

- 데이터에 가장 가까운 초평면을 정의한 다음, 데이터를 평면에 투영시키는 방법

&nbsp;

**분산 보존**

- 저 차원의 초평면에 훈련 세트를 투영하기 전에 먼저 적절한 초평면을 선택
- 분산이 최대로 보존되는 축을 선택하는 것이 정보가 가장 적게 손실되므로 합리적인 방법

&nbsp;

**주성분**

- PCA는 훈련 세트에서 분산이 최대인 축을 찾음, 첫 번째 축에 직교하고 남은 분산을 최대한 보존하는 두 번째 축을 찾음
- 위와 같은 방법으로 세 번째, 네 번째, .... , i 번째 축을 찾게 되는데 이때 i번째 축을 i번째 **주성분**이라고 함

- 훈련 세트의 주성분을 찾는 방법으로는 특이값 분해(SVD)라는 표준 행렬 분해 기술을 사용

- 훈련 세트 X를 세 개 행렬의 행렬곱 표현식

  ![image-20221102135954789](/images/2022-11-01-hands_on_8/image-20221102135954789.png)

- SVD 함수를 사용해 훈련 세트의 모든 주성분을 구한 후 두 개의 PC를 구하는 예제

  - PCA는 데이터셋의 평균이 0이라고 가정
  - 그러므로 데이터를 원점에 맞추고 사용

  ```python
  X_centered = X - X.mean(axis=0)
  U, s, Vt = np.linalg.svd(X_centered)
  c1 = Vt.T[:, 0]
  c2 = Vt.T[:, 1]
  ```

&nbsp;

**d차원으로 투영하기**

- 주성분을 모두 추출시 처음 d개의 주성분으로 정의한 초평면에 투영하여 데이터셋의 차원을 d차원으로 축소시킬 수 있음

  - 해당 초평면은 분산을 가능한 한 최대로 보존하는 투영을 보장

- 첫 두 개의 주성분으로 정희된 평면에 훈련 세트를 투영하는 예제

  ```python
  W2 = Vt.T[:, :2]
  X2D = X_centered.dot(W2)
  ```

&nbsp;

**사이킷런 사용하기**

- 앞서 설명한 SVD 분해 방법을 사용하여 구현

- 사이킷런을 이용한 PCA 예제

  ```python
  from sklearn.decomposition import PCA
  
  pca = PCA(n_components=2)
  X2D = pca.fit_transform(X)
  ```

&nbsp;

**설명된 분산의 비율**

- explained_variance_ratio_ 변수에 저장된 주성분의 **설명된 분산의 비율**도 유용한 정보

- 이 비율은 각 주성분의 축을 따라 있는 데이터셋의 분산 비율을 의미

- 분산 비율 예제

  ```python
  pca.explained_variance_ratio_
  >> array([0.84248607, 0.14631839])
  ```

  - 위 정보로 데이터셋 분산의 84.2%가 첫 번째 PC를 따라 놓여 있고 나머지 14.6%가 두 번째 PC를 따라 놓여 있다는 것을 의미 이를 통해 세 번째 PC에는 1.2% 미만이 남아있음

&nbsp;

**적절한 차원 수 선택하기**

- 차원을 축소하지 않고 PCA를 계산한 뒤 훈련 세트의 분산을 95%유지하는 예제

  ```python
  from sklearn.datasets import fetch_openml
  
  mnist = fetch_openml('mnist_784', version=1, as_frame=False)
  mnist.target = mnist.target.astype(np.uint8)
  
  from sklearn.model_selection import train_test_split
  
  X = mnist["data"]
  y = mnist["target"]
  
  X_train, X_test, y_train, y_test = train_test_split(X, y)
  
  pca = PCA()
  pca.fit(X_train)
  cumsum = np.cumsum(pca.explained_variance_ratio_)
  d = np.argmax(cumsum >= 0.95) + 1
  
  d
  >> 154
  ```

  - 적절한 차원수를 찾으면 해당 d를 components로 설정해 PCA 다시 실행

  - 유지하려는 주성분의 수를 지정하는 것 보다 보존하려는 분산의 비율을 n_components에 0.0 ~ 1.0 사이로 설정하는 것이 좋음

    ```python
    pca = PCA(n_components=0.95)
    X_reduced = pca.fit_transform(X_train)
    
    pca.n_components_
    >> 154
    ```

    

&nbsp;
