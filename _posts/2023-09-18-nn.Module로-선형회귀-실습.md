---
layout: single
title: "nn.Module을 이용하여 선형 회귀 구현해보기"
categories: DeepLearning
tag: [DeepLearning, coding, python, LinearRegression, pytorch]
toc: true 
---
# nn.Module로 선형 회귀를 구현해보기

이번에는 nn.Module을 이용해서 선형 회귀를 구현해보겠습니다.

**???:이거 도대체 왜씀~**

매번 선형 회귀를 할 때마다 우리는 선형 회귀 가설을 직접 식을 세워줘야 합니다. 하지만 이 모델을 쓰게 된다면 우리는 직접 선형 회귀 가설을 세우지 않더라도 pytorch에서 제공하는 선형회귀 모델을 이용해서 바로 쓸 수 있기 때문에 편합니다.

또한 우리가 배웠던 loss 함수 mse(mean squared Error도 따로 선언하지 않아도 F.mse_loss()메서드로 쉽게 바로 사용할 수 있습니다.

그럼 먼저 nn.Module을 사용하는 방법을 알아보도록 합시다.

---

- **선형 회귀 사용하기**

![Untitled](https://github.com/jusunglee-ai/jusunglee-ai.github.io/assets/125032849/2e8cdd19-f07a-4478-9f8e-4e77e02372e7)

import torch.nn as nn으로 라이브러리를 호출 한 뒤 nn.Linear()함수를 사용할 수 있습니다.

nn.Linear함수에 구조는 위의 사진처럼 nn.Linear(input_dim,output_dim)으로 이루어져 있으며 input_dim에는 input data 즉 독립 변수 x의 feature의 개수를 넣어주시면 되고 output_dim에는 ouput data 즉 종속 변수 y의 feature의 개수를 넣어주시면 됩니다. 

- **loss함수로 MSE사용하기**

![Untitled 1](https://github.com/jusunglee-ai/jusunglee-ai.github.io/assets/125032849/8f0231b6-c38b-4d2e-bd22-4c4089de3483)

사용 방법은 라이브러리 호출 뒤 F.mse_loss를 사용해주면 됩니다.

여기서 prediction은 예상 값 즉 hypothesis의 결과 값 모델이 예측한 값을 넣어주시면 되고, y_train은 실제 데이터 종속 변수 y의 값을 넣어주시면 됩니다.

---

그럼 새롭게 사용할 함수들을 설명했으니 직접 실습 해봅시다.

![Untitled 2](https://github.com/jusunglee-ai/jusunglee-ai.github.io/assets/125032849/555a4cec-5b1f-4236-85a3-8984c05649a6)

먼저 input으로 들어갈 x데이터와 output인 y데이터를 선언해서 넣어줍시다.

model은 pytorch에서 제공하는 선형회귀 모델을 쓰기 위해 nn.Linear(1,1)을 넣어줍시다.

여기서 1,1을 넣는 이유는 앞에서 말했다 싶이 독립 변수 x가 1x3크기의 feature가 1개인 데이터고 종속 변수 y도 1x3크기의 feature가 1개인 데이터 이기 때문에 각 feature의 개수인 1,1을 넣어준겁니다.

그리고 print(list(model.parameters()))를 통해 해당 선형 회귀 모델의 weight와 bias값을 확인 해줍시다.

여기서 parameter의 결과 값이 2개가 나오게 되는데 1번째 parameter값이 weight값 2번째 parameter 결과 값이 bias값입니다.

그리고 두 값 모두 학습하면서 조정이 되어야 할 대상이기 때문에 자동 미분 requires_grad=True가 되어있습니다.

---

- **optimizer 설정**

![Untitled 3](https://github.com/jusunglee-ai/jusunglee-ai.github.io/assets/125032849/0da304c3-29b5-45df-97a6-4fc5c0fe076f)

당연히 optimizer는 이번에도 gradient descent를 사용할 것 입니다.

![Untitled 4](https://github.com/jusunglee-ai/jusunglee-ai.github.io/assets/125032849/fb51fbe2-7c3a-4177-af43-903a5911070a)

epochs 즉 총 학습 횟수는 전과 똑같이 2000으로 하고 학습 시켜보도록하겠습니다.

![Untitled 5](https://github.com/jusunglee-ai/jusunglee-ai.github.io/assets/125032849/a783e4bd-5614-4218-89cb-412e9f09dfcb)

현재 loss값이 0으로 완전하게 적절한 weight와 bias값을 찾아서 설정하고 예측한 것 같습니다.

- **임의의 예측 값 넣어보기**

그럼 이 다음에는 우리가 한번 임의의 데이터를 주고 예측해보라고 해봅시다.

![Untitled 6](https://github.com/jusunglee-ai/jusunglee-ai.github.io/assets/125032849/b253338a-7f07-4ccf-b0a3-bb34d7177ae5)

x데이터로는 4로 주었고 이를 예측하였을 때 7.9989라는 값이 나왔습니다

완전히 8이라는 값이 나오지 않았지만 7.9989라는 값은 8과 거의 일치하기 때문에 잘 예측했다고 판단을 할 수  있습니다.

그럼 현재 모델의 weight와 bias의 값이 어떻게 되었는지 확인 해보도록 하겠습니다.

![Untitled 7](https://github.com/jusunglee-ai/jusunglee-ai.github.io/assets/125032849/09e1356e-2f8c-4a13-a25d-11b634883827)

weight가 1.9994, bias가 0.0014가 나왔습니다.

우리가 보았을 때 해당 데이터에 알맞은 선형 회귀 함수는 y=2x라는 함수가 가장 해당 데이터에 알맞는 함수라고 생각하실 겁니다.

그럼 이 함수에서 weight는 2, bias는 0이라는 값이 나오겠죠 그럼 모델이 학습하면서 조정한 weight와 bias와 비교해보면 굉장히 유사한 값이 나왔으며 적절한 weight와 bias를 찾아 학습이 잘 되었다는 것을 알 수 있습니다.

---

- **nn.module을 이용하여 다중 선형 회귀 구현을 해보자**

이번에는 nn.module을 이용해서 다중 선형 회귀를 구현 해보도록 하겠습니다.

개념과 방식은 우리가 앞서했던 단순 선형 회귀와 방법이 똑같습니다.

![Untitled 8](https://github.com/jusunglee-ai/jusunglee-ai.github.io/assets/125032849/b81010b0-1816-4431-98dd-e92cd197af48)

똑같이 x데이터와 y데이터를 선언을 해줍니다.

우리가 앞서 다중 선형 회귀 개념을 배울 때 사용 했던 데이터와 똑같습니다.

![Untitled 9](https://github.com/jusunglee-ai/jusunglee-ai.github.io/assets/125032849/286179e5-01c4-4482-b314-ef0cbb7736c6)

nn.Linear(3,1)에서는 독립 변수 x데이터의 feature가 3개고 종속 변수 y데이터의 feature가 1개 이기 때문에 3,1을 넣어 주었습니다. 그리고 weight와 bias도 확인 해주도록 합시다.

앞에서 말했듯이 1번째 parameter가 weight, 2번째 parameter가 bias입니다. weight의 개수가 3개 이기 때문에 순서대로 w1, w2, w3가 잘 설정 되었는 것을 확인 할 수 있구요 bias도 1개 설정 된 것을 확인 할 수 있습니다.

- **optimizer-gradient descent**

![Untitled 10](https://github.com/jusunglee-ai/jusunglee-ai.github.io/assets/125032849/2a0bd075-a4c4-427b-838e-fde32b6da427)

옵티마이저는 grdient descent를 사용할 예정입니다.

또한 loss함수도 똑같이 mse를 사용할 예정이구요.

그 외에는 다 똑같습니다.

![Untitled 11](https://github.com/jusunglee-ai/jusunglee-ai.github.io/assets/125032849/685f2471-0cab-4bc3-b3fc-8973e774d765)

학습 결과를 보니 loss가 급격하게 줄었드는 과정을 확인 할 수 있고 초기 loss값에 비해 훈련 후 loss값이 굉장히 많이 줄었다는 것을 알 수 있습니다.

- **임의의 예측 값 넣어보기**

그럼 이번에도 임의 서로 다른 3개의 x를 넣어 보도록 하겠습니다.

![Untitled 12](https://github.com/jusunglee-ai/jusunglee-ai.github.io/assets/125032849/c0dab044-9a2d-466f-8015-c280177fe9c1)

임의로 넣은 값은 73, 80 ,75 이고 예측 값은 151.6319가 나왔네요

실제로 이 값을 넣었을 때의 y값은 152이며 굉장히 실제 데이터에 근접한 유사한 결과 값이 나왔음을 보아 학습이 잘 되어 weight와 bias가 잘 조정 되었다는 것을 알 수 있습니다.

그럼 실제로 조정된 weight와 bias에 대해 보도록 합시다.

![Untitled 13](https://github.com/jusunglee-ai/jusunglee-ai.github.io/assets/125032849/403ce4e9-38ff-41f0-826f-475a212b58a5)

이게 조정된 weight와 bias의 값입니다.

loss값과 임의의 데이터를 넣었을 때 나온 결과 값을 보면 weight와 bias가 적절하게 조절된 것 같습니다.
