---
layout: single        # 문서 형식
title: 'Attention is all you need 리뷰'         # 제목
categories: DL/NLP    # 카테고리
toc: true             # 글 목차
author_profiel: false # 홈페이지 프로필이 다른 페이지에도 뜨는지 여부
sidebar:              # 페이지 왼쪽에 카테고리 지정
    nav: "docs"       # sidebar의 주소 지정
#search: false # 블로그 내 검색 비활성화
---

# Abstract


# 1. Intorduction


# 2. Background


# 3. Model Architecture
## 3.1. Encoder & Decoder Stacks
### Encoder
### Decoder

## 3.2. Attention
### Scaled Dot-Product Attention
### Multi-Head Attention
### Applications of Attention in our Model

## 3.3. Position-wise Feed-Forward Networks
## 3.4. Embeddings and Softmax
## 3.5. Positional Encoding

# 4. Why Self-Attention
