---
layout: single
title:  "8/5 NLP"
categories: [Programming, python, NLP, NLTK, 자연어]
tag: [Programming, python, NLP, NLTK, 자연어]
toc: true
author_profile: false
sidebar:
    nav: "docs"
---

# 정수 인코딩

* 자연어를 컴퓨터가 이해할 수 있는 형태로 인코딩하는 과정



# 패딩(padding)

* 자연어를 컴퓨터가 이해할 수 있는 숫자 형태로 인코딩하는 과정에서 문장들의 길이를 모두 동일하게 맞추는 작업
* 컴퓨터는 길이가 동일한 문장을 하나의 행렬로 만들어 병렬적으로 계산할 수 있다.(계산 속도가 매우 빨라짐)

* 보통 0을 추가해 길이를 맞추는 제로패딩 방식을 사용한다.



# 원 핫 인코딩(One-Hot Encoding)

* 정수로 표현되었지만 실제로는 문자인 이 데이터를 기계가 인식할 수 있도록 바꿔주는 방법
* 정수 인코딩을 했을 때 단어에 숫자가 부여되는데, 이러한 숫자는 대소관계를 가지므로 인코딩방법으로 적합하지 않다.
* 원 핫 인코딩은 1차원 벡터의 형태로 인코딩하는 방식으로, 표현하고자 하는 단어의 자리는 1, 나머지는 0으로 채워넣은 벡터로 인코딩
* 하지만 단어와 문장의 개수가 많아지면, 필요한 공간이 엄청나게 늘어나고 0의 개수가 많아지므로 희소 데이터(sparse data)가 된다.



# 백오브 워즈(Bag of Words, BoW)

* bag은 원소의 순서를 고려하지 않고 중복 원소를 허용하는 집합을 의미한다.
* 단어의 등장 순서를 고려하지 않고, 문서 내 단어의 등장 빈도를 고려한다.



# 문서 단어 행렬(Document Term Matrix, DTM)

* 각 문서에 대한 BoW 표현 방법을 그대로 갖고 와서, 서로 다른 문서들의 BoW들을 결합한 표현방법
* 원 핫 인코딩과 마찬가지로 공간적 낭비를 일으킬 수 있다.



# TF-IDF

* 단어의 빈도(TF, term frequency)와 역 문서 빈도(Inverse Document Frequency)를 사용하여 DTM 내의 각 단어들 마다 중요한 정도를 가중치로 주는 방법이다.
* TF, IDF을 곱한 값으로 점수가 높을수록 해당 문서에서만 자주 등장하는 단어이다.
* tf(d, t) : 단어의 빈도, DTM에서 각 단어들이 가진 값
* df(t) : 특정 단어 t가 등장한 문서의 수, 문서에서 몇번 등장했는지는 고려하지 않는다.
* idf(d, t) = $\log(n \over {(1 + df(t))}) 