---
layout: single
title:  "핸즈온 머신러닝 - 5"
categories : scikit-learn
tag : [scikit-learn, machine-learning, python]
toc: true
toc_sticky: true

---

![header](https://capsule-render.vercel.app/api?type=waving&color=a2dcec&height=300&section=header&text=핸즈온 머신러닝 - 5&fontSize=40&animation=fadeIn&fontAlignY=38&fontColor=FFFFFF)

- 참고 : [핸즈온 머신러닝 2판](http://www.kyobobook.co.kr/product/detailViewKor.laf?mallGb=KOR&ejkGb=KOR&barcode=9791162242964)

------------------------------------------------------

&nbsp;



## 서포트 벡터 머신(SVM)

- 매우 강력하고 선형이나 비선형 분류, 회귀, 이상치 탐색에도 사용할 수 있는 다목적 머신러닝 모델
- 특히 복잡한 분류 문제에 잘 들어맞으며 작거나 중간 크기의 데이터셋에 적합

&nbsp;

## 선형 SVM 분류

- ![image-20221011172740196](/images/2022-10-11-hands_on_5/image-20221011172740196.png)

- 두 class가 직선을 기준으로 잘 나눠져 있음
  - 왼쪽 그래프는 세 개의 선형 분류기에서 만들어진 결정 경계
    - 점선으로 나타낸 결정경계는 클래스를 적절하게 분류하고 있지 않음

  - 오른쪽 그래프는 SVM 분류기의 결정경계
    - 직선이 두 개의 클래스를 나누고 있고 제일 가까운 훈련 샘플로 부터 가능한 멀리 떨어져 있음
    - 클래스 사이에 폭이 넓은 도로를 찾는 것과 같다고 하여 **라지 마진 분류** 라고 함
    - 점선 경계에 위치한 샘플을 서포트 벡터, 서포트 벡터 밖의 샘플을 추가해도 결과 동일


> SVM은 특성의 스케일에 민감함, 특성의 scale을 조절하면 결정 경계가 좋아짐

![image-20221011173232340](/images/2022-10-11-hands_on_5/image-20221011173232340.png)

&nbsp;

**소프트 마진 분류**

- 모든 샘플이 서포트 벡터 바깥쪽에 올바르게 분포 했다면 **하드 마진 분류** 라고 함
  - 하드 마진 분류의 두가지 문제점
    1. 데이터가 선형적으로 구분될 수 있어야 잘 작동
    2. 이상치에 민감


![image-20221011174400950](/images/2022-10-11-hands_on_5/image-20221011174400950.png)

- 위 그림에서 왼쪽 그래프에서는 하드 마진이 없고 오른쪽 그래프는 이상치 때문에 결정경계가 일반화 되기 어렵게 나타나 있습니다.

> 위와 같은 문제를 해결하기 위해 서포트 벡터 사이의 마진의 폭을 가능한 넓게 유지, 마진 오류 사이에 적절한 균형을 잡아야함 이를 **소프트 마진 분류** 라고 함

- sklearn 의 SVM 모델의 C 파라미터를 통해 조절이 가능, SVM이 과대적합이라면 C를 통해 규제를 추가할수 있음
  - ![image-20221011175026090](/images/2022-10-11-hands_on_5/image-20221011175026090.png)
  - LinearSVC는 규제에 편향을 포함, 훈련 세트에서 평균을 빼서 중앙에 맞춰야함
    - standardScaler를 적용해 해결
    - loss를 hinge로 지정, 훈련 샘플보다 특성이 많지 않다면 성능을 높이기 위해 dual 매개변수 False 지정

&nbsp;

## 비선형 SVM 분류

- 
