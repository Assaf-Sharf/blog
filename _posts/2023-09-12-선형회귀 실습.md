# 선형 회귀 실습

선형 회귀를 한번 코드로 직접 구현 해봅시다.

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20%E1%84%89%E1%85%B5%E1%86%AF%E1%84%89%E1%85%B3%E1%86%B8%20c4d03699f4f54c9494f230ff77170519/Untitled.png)

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20%E1%84%89%E1%85%B5%E1%86%AF%E1%84%89%E1%85%B3%E1%86%B8%20c4d03699f4f54c9494f230ff77170519/Untitled%201.png)

우리가 아까 다루었던 예시 시험 공부와 공부 대비 나오는 시험 점수들에 대한 데이터를 한번 만들어 봅시다.

x_train은 독립변수 x에 대한 훈련용 데이터

y_train은 종속 변수 y에 대한 훈련용 데이터입니다.

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20%E1%84%89%E1%85%B5%E1%86%AF%E1%84%89%E1%85%B3%E1%86%B8%20c4d03699f4f54c9494f230ff77170519/Untitled%202.png)

한번 훈련용 데이터의 크기와 형태에 대해 알아보도록 합시다.

x_train같은 경우는 3x1 크기의 2차원 텐서이며

y_train도 3x1 크기의 2차원 텐서인 것을 확인 할 수 있습니다.

제대로 우리가 의도한대로 텐서가 잘 만들어 졌네요

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20%E1%84%89%E1%85%B5%E1%86%AF%E1%84%89%E1%85%B3%E1%86%B8%20c4d03699f4f54c9494f230ff77170519/Untitled%203.png)

그 다음 manual_seed를 추가해줄거에요

이걸 왜 추가하냐면 이 코드를 다시 실행해도 똑같은 난수 값이 나오도록 하기 위해서 입니다.

### manual_seed에 대해 자세하게 궁금하면 여기로

Manual_seed는 파이토치에서 제공하는 메서드로 

numpy에서 제공하는 random.seed()메서드와 같은 역할을 합니다.

바로 랜덤으로 생성된 난수들을 seed값에 따라서 고정하는 역할을 합니다.

random.rand()메서드의 경우 프로그램을 매번 실행할 때 마다 서로 다른 랜덤한 난수를 생성 합니다. 

하지만 여기서 manual_seed 메서드를 통해 seed값을 정해버리면 매번 프로그램을 실행할 때마다 서로 다른 랜덤한 난수를 생성하는게 아닌 한번 생성한 랜덤 난수를 프로그램을 반복해서 실행해도 똑같은 난수가 나오도록 설정해준다는 것이다.

 

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20%E1%84%89%E1%85%B5%E1%86%AF%E1%84%89%E1%85%B3%E1%86%B8%20c4d03699f4f54c9494f230ff77170519/Untitled%204.png)

그리고 여기서의 seed값은 컴퓨터가 난수처럼 보이는 수열을 생성할 때 필요한 시작 값이다.

그렇기 때문에 seed를 고정해주면 매번 같은 수열을 생성하기 때문에 seed값을 고정해주면 매번 같은 난수가 생성이 되는 것이다.

---

- **weight 와 bias 선언**

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20%E1%84%89%E1%85%B5%E1%86%AF%E1%84%89%E1%85%B3%E1%86%B8%20c4d03699f4f54c9494f230ff77170519/Untitled%205.png)

그리고 weight를 정의해줌과 동시에 weight를 torch.zeros를 통해 0으로 채워진 텐서로 만들어 0으로 초기화 해줍니다.

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20%E1%84%89%E1%85%B5%E1%86%AF%E1%84%89%E1%85%B3%E1%86%B8%20c4d03699f4f54c9494f230ff77170519/Untitled%206.png)

bias도 마찬가지로 해줍니다.

- **hypothesis 선언**

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20%E1%84%89%E1%85%B5%E1%86%AF%E1%84%89%E1%85%B3%E1%86%B8%20c4d03699f4f54c9494f230ff77170519/Untitled%207.png)

hypothesis는 우리가 배웠던 단순 선형 회귀식으로 선언을 할 겁니다.

y=wx+b로요

- **loss function 선언**

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20%E1%84%89%E1%85%B5%E1%86%AF%E1%84%89%E1%85%B3%E1%86%B8%20c4d03699f4f54c9494f230ff77170519/Untitled%208.png)

loss function은 우리가 배웠던 loss function식 MSE와 같은 식으로 선언을 해줄겁니다.

- **Optimizer 선언**

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20%E1%84%89%E1%85%B5%E1%86%AF%E1%84%89%E1%85%B3%E1%86%B8%20c4d03699f4f54c9494f230ff77170519/Untitled%209.png)

optim.SGD를 통해 우리는 Optimizer를 Gradient Descent로 설정할 수 있습니다.

lr은 Learning rate를 의미 합니다.

zero_grad()는 해당 메서드가 실행이 될 때마다 gradient를 0으로 초기화 해줍니다. 이게 필요한 이유는 뒤에서 더 자세하게 다루도록 하겠읍니다.

적절한 Learning rate은 해당 모델과 데이터 등등 조건에 따라 다 다르기 때문에 정해진 Learning rate이란 없습니다.

그리고 backward()메서드를 통해 loss function을 미분하여 나온 접선의 기울기를 구할 수 있습니다.

optimizer.step():이 메서드를 이용하면 우리가 gradient descent 알고리즘을 통해 조정된 weight와 bias값을 업데이트 해줍니다.

그럼 우리가 훈련 데이터도 만들었고 weight와 bias도 설정하고 loss function과 hypothesis, optimizer까지 모두 선언 하였으니 한번 gradient algorithm을 실행시켜 봅시다.

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20%E1%84%89%E1%85%B5%E1%86%AF%E1%84%89%E1%85%B3%E1%86%B8%20c4d03699f4f54c9494f230ff77170519/Untitled%2010.png)

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20%E1%84%89%E1%85%B5%E1%86%AF%E1%84%89%E1%85%B3%E1%86%B8%20c4d03699f4f54c9494f230ff77170519/Untitled%2011.png)

위의 코드를 통해 gradient algorithm을 실행시킬 수 있고 해당 결과 값을 통해서 약 3750번째 반복 하였을때 loss값이 0이 되면서 적절한 weight와 bias를 찾았다는 것을 확인 할 수 있습니다.

### zero_grad()와 backward()에 대해 자세하게 Araboza

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20%E1%84%89%E1%85%B5%E1%86%AF%E1%84%89%E1%85%B3%E1%86%B8%20c4d03699f4f54c9494f230ff77170519/Untitled%2012.png)

먼저 우리가 앞서 Optimizer부분에서 zero_grad()를 사용했습니다.

왜 zero_grad()를 사용해야 하는지에 대해 자세한 설명이 없어서 추가로 넣었습니다.

.backward()메서드를 사용하여 자동으로 미분을 하게 되면 미분한 값들은 쌓여서 축적이 됩니다.

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20%E1%84%89%E1%85%B5%E1%86%AF%E1%84%89%E1%85%B3%E1%86%B8%20c4d03699f4f54c9494f230ff77170519/Untitled%2013.png)

이와 같이 계속해서 미분 결과 값들이 쌓이게 됩니다.

그렇기 때문에 만약 이를 계속 초기화를 시켜주지 않게 되면 접선의 기울기 값이 쌓이게 되고 결국에는 현재 weight나 bias의 접선의 기울기를 알 수 없기 때문에 적절한 weight와 bias를 찾을 수 없게 됩니다.

설령 loss값이 0이 되더라도 그 loss값은 실제로 loss값이 0이 아니라 축적이 된 접선의 기울기에서의 loss값이기 때문에 틀린 loss값이기 때문입니다.
