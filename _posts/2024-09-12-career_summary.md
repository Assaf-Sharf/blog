---
layout: single
title:  "Professional Experience Statement"
categories: Personal
tag: [dorumugs, kayser, 경력기술서]
toc: true
author_profile: true
---

## Job Title : Data Specialist 

### Personal Information

**Name** : 소재현(蘇在賢), Jaehyun So

**Nickname** : Kayser So, dorumugs

**Email** : dorumugs@mail.com

**Birthday** : January 12, 1984

**Gender** : Male 



### Education

- 2018.09 ~ 2020.12  한양대학교(서울) 대학원 컴퓨터공학 석사졸업 (MNA Lab, 조인휘 교수)
- 2007.07 ~ 2010.02  학점은행제 컴퓨터공학과 편입졸업
- 2004.02 ~ 2007.07  명지전문대학 영어과 편입
- 1999.03 ~ 2002.02  한양공업고등학교 전자기계과 졸업



### Paper

- 2022년 5월 31일  [그림으로 배우는 딥러닝](https://product.kyobobook.co.kr/detail/S000061352299)  에이콘출판(*교보문고 MD 추천*)
- 2020년 12월 31일  졸업논문, 전자상거래에서 성별 예측을 위한 앙상블 모델(*MNA Lab, 조인휘 교수*)
- 2016년 2월 26일  A Proactive Inference Method of Suspicious Domains(*Journal of The Korea Institute of Information Security & Cryptology*)
- 2015년 10월 9일  Detecting trigger-based behaviors in botnet malware(*Research in Applied Computation Symposium*)



### Contest Award

- 2020년 12월 3일 대만민국 바로 알리기 AI 공모전 우수상(*과학기술정보통신부*)



### KeySkill

- Machine Learning, Deep Learning, LLM(LangChain), Recommendation System
- Google Cloud Platform, Hadoop ECO System
- Python, PySpark, Airflow
- RDBMS, BigQuery, Hive, Elasticsearch
- Superset, Looker
- Digital Forensic(Incident Management)





## Professional Experience

### Q10 Technology **ㅣInfo Platform 실장, AI Dev 팀장 겸임**

<u>**회사소개**</u>

- 이커머스, 스타트업 기업 / 직원수 : 600명

- 이직사유 : 새로운 도전



<u>**포지션**</u>

- **기간** : 2017.09 ~ 2018.11
- **소속실** : Info Platform Division
- **소속팀** : AI Dev
- **직위** : 실장, 팀장 겸임



<u>**주요프로젝트 및 업무**</u> 

- **자동 라벨링 프로젝트**
  - **기간**: 2022-01 ~ 현재까지
  - **기술**: LLM, LangChain, streamlit, fastapi, k8s, GCP, ETL, Inference, Python, BigQuery
  - **목표**:
    - 데이터를 분석하여 사용자 구매 히스토리와 아이템 카테고리 기반으로 성별 및 연령대 추론
    - 상품 리뷰 주제 분석을 통해 맞춤형 마케팅 캠페인과 추천 시스템 강화 및 사용자 경험 개선
  - **요 내용**:
    - BigQuery에서 사용자 구매 기록 및 아이템 정보 수집 및 전처리
    - 성별 예측(LOGISTIC_REG) 및 연령대 예측(BOOSTED_TREE_CLASSIFIER) 모델 개발
    - LLM과 LangChain을 활용해 리뷰 자동 라벨링 및 주제 분석
    - Kubernetes(k8s) 도입으로 모델 배포 및 스케일링 자동화, 안정성 확보
  - **성과**:
    - 성별과 연령대 예측을 통해 마케팅 캠페인의 타겟팅 정확도 향상
    - LLM을 통한 리뷰 주제 분석으로 개인화 마케팅 효율성 증가
    - k8s 도입 후 시스템 안정성 및 확장성 대폭 개선

 

- **판매량이 증가할 아이템 추천**

  - **기간**: 2020-03 ~ 2020-06

  - **기술**: ETL, Inference, Python, HIVE

  - **목표**:

    - 창고 적재량 변화, 구매량, 페이지뷰(PV) 데이터를 기반으로 판매량 증가 예상 아이템 예측.
    - 재고 관리 효율화 및 품절 방지

  - **주요 내용**:

    - 데이터 전처리 및 피처 추출
    - Gradient Boosting Regression 모델 개발 및 하이퍼파라미터 튜닝
    - 실시간 모니터링 및 알림 시스템 구축

  - **성과**:

    - 예측 모델 도입 후 재고 부족 사례 감소

    - 재고 관리의 효율성 향상으로 운영 비용 절감

      

- **개인화 추천**
  
  - **기간**: 2018-12 ~ 현재까지
  - **기술**: UltraGCN, ETL, Inference, Python, Hadoop, BigQuery
  - **목표**:
    - 사용자 행동 데이터를 기반으로 성별 및 개인화된 상품 추천 제공
    - UltraGCN을 활용한 아이템 기반 장바구니 추천 기능 고도화
  - **주요 내용**:
    - Hive 및 BigQuery를 통한 대규모 데이터 처리 및 추천 모델 설계.
    - 성별 맞춤형 추천 모델 및 개인화 추천 모델 개
    - UltraGCN을 활용해 장바구니에 추가될 가능성이 높은 아이템 추천
  - **성과**:
    - 추천 시스템의 정확도 증가하고, 사용자 만족도 향상
    - 장바구니 추천 기능으로 사용자 구매 효율성 향상

 

- **전사 데이터 ETL**
  - **기간**: 2018-12 ~ 현재까지
  - **기술**: ETL, Airflow, Python, Sqoop, PySpark, RMQ
  - **목표**:
    - RMQ에서 수신한 데이터를 Hadoop HDFS에 저장하고, 이를 전처리 후 HIVE 테이블 업데이트.
    - 대규모 실시간 데이터를 안정적으로 처리하고 분석 가능한 형태로 변환
  - **주요 내용**:
    - HDFS에 저장된 데이터 전처리 및 중복 제거
    - HIVE 테이블 업데이트 및 성능 최적화
  - **성과**:
    - 실시간 데이터 처리의 안정성 확보로 데이터 처리 속도 향상
    - ETL 파이프라인 개선으로 데이터 처리 오류율 감소

 

- **Hadoop & GCP에서 데이터 관리**
  - **기간**: 2018-12-07 ~ 현재까지
  - **기술**: Hadoop, GCP
  - **목표**:
    - 판매량 예측 서비스 구축을 통한 이커머스와 물류 관리 효율성 극대화.
    - 실시간 데이터 처리 및 판매량 증가 아이템 예측.
  - **주요 내용**:
    - 데이터 파이프라인 구축 및 피처 엔지니어링.
    - Gradient Boosting Regression 모델 성능 개선 및 피처 중요도 평가.
  - **성과**:
    - 인벤토리 추가 공급을 기반한 판매량 지속도 유지 향상

 

- **FDS (Fraud Detection System)**
  - **기간**: 2018-12-07 ~ 2019-05-31
  - **기술**: AutoEncoder, ETL, ML, Python, BigQuery
  - **목표**:
    - 이커머스 구매 데이터를 기반으로 사기 거래 탐지 시스템 구축.
    - 사용자 행동을 정량화하여 사기 거래 패턴 분석.
  - **주요 내용**:
    - AutoEncoder 기반 이상 거래 탐지 모델 개발.
    - 피처 엔지니어링 및 비정형적 행동 패턴 분석.
  - **성과**:
    - 사기 거래 탐지 정확도 비약적 향상.
    - 실시간 사기 탐지 시스템 도입으로 업무 투입률 감소.



### Dorumugs Company

<u>**회사소개**</u>

- 기업 데이터 분석 및 교육, 프리랜서 / 직원수 : 1명

- 퇴직사유 : 업무 역량 강화 



<u>**포지션**</u>

- **기간** : 2017.09 ~ 2018.11
- **소속팀** : 분석팀
- **직위** : 프리랜서



<u>**주요프로젝트 및 업무**</u>

- **머신러닝 논문 데이터 분석 및 인사이트 도출**

  -	**기간**: 2020-03 ~ 2021-03

  -	**기술**: Python, Pandas, Scikit-learn, Matplotlib, Seaborn, NLP

  -	**목표**:
    -	제공받은 데이터를 기반으로 머신러닝 모델을 사용해 인사이트를 도출하고, 논문 작성을 지원
    -	데이터 전처리부터 인사이트 도출까지의 전체 과정을 체계적으로 진행하여 제공

  -	**주요 내용**:
    -	의뢰인의 데이터에 대해 Python과 Pandas를 활용해 데이터 전처리 및 결측치 처리
    -	Scikit-learn을 사용하여 다양한 머신러닝 알고리즘을 적용, 데이터를 분석하고 예측 모델 구축
    -	Matplotlib 및 Seaborn을 통해 시각화 도구를 사용하여 데이터 인사이트 도출
    -	자연어 처리(NLP) 기법을 활용하여 텍스트 데이터에서 핵심 주제 및 패턴을 분석
    -	분석 결과에 기반한 인사이트 제공 및 논문 작성 피드백을 통해 연구 논문의 완성도 향상

  -	**성과**:
    -	제공된 데이터에서 새로운 인사이트 도출로 연구 방향 설정 및 논문 완성도 향상
    -	머신러닝 모델을 활용한 정확한 예측 결과로 연구 논문 통과율 증가
    -	데이터 분석 및 시각화 결과 제공을 통해 의뢰인의 연구 효율성 향상



- **기업 침해사고 포렌식**

  -	**기간**: 2017-09 ~ 2018-12

  -	**기술**: Digital Forensics, Python, ELK Stack, Volatility, Wireshark, Splunk

  -	**목표**:
    -	기업의 침해 사고 발생 시 원인을 분석하고, 피해 범위를 평가하여 복구 작업을 지원
    -	디지털 증거 수집 및 포렌식을 통해 사고의 발생 경로를 추적하고, 재발 방지 대책 마련


-	**주요 내용**:
  -	사고 발생 시 디스크 이미지와 메모리 덤프를 수집하여 포렌식 분석 수행
  -	Volatility를 통해 메모리 분석 및 악성 코드 추적, Wireshark로 네트워크 트래픽 분석
  -	ELK Stack과 Splunk를 활용해 로그 데이터를 수집하고, 이상 패턴 및 침해 경로 식별
  -	Python으로 자동화된 로그 분석 스크립트를 작성하여 빠르게 침입 흔적 탐지
  -	최종 보고서를 통해 사고 경로 및 공격자의 활동 분석 결과를 제공하고, 재발 방지 대책 수립

-	**성과**:
  -	침해 사고 발생 후 48시간 내에 초기 증거 수집 및 분석 완료
  -	사고 원인 규명을 통해 피해 기업의 시스템 복구 시간을 단축
  -	사고 재발 방지 조치로 기업의 보안 수준 향상




- **온라인 포렌식 및 빅데이터 처리 교육**

  -	**기간**: 2017-09 ~ 2018-12

  -	**기술**: Python, Pandas, Spark, Numpy, ELK Stack, Volatility

  -	**목표**:
    -	포렌식 및 빅데이터 처리 기술을 위한 온라인 교육 플랫폼 구축
    -	포렌식 데이터 분석과 빅데이터 처리 역량을 강화하기 위한 체계적인 온라인 교육 과정 제공


  -	**주요 내용**:
    -	포렌식 도구인 Volatility 사용법을 교육하며 메모리 분석과 타임라인 추적 방법 강의
    -	Spark와 Pandas를 이용한 빅데이터 처리 기술 교육, 데이터 전처리와 분산 처리 원리 강의
    -	ELK Stack을 통해 로그 수집 및 분석 교육, 실시간 데이터 분석 방법 제공
    -	실습 중심의 교육 과정으로 실제 사례를 통해 데이터 처리 역량을 키우도록 지원.


  -	**성과**:
    -	포렌식 전문가 및 데이터 분석 전문가 배출




### AhnlabㅣA-FIRST l 선임연구원

<u>**회사소개**</u> 

- 백신, 코스탁 상장 기업 / 직원수 : 1000명
- 퇴직사유 : 프리랜서 전향



<u>**포지션**</u>

- **기간** : 2009.07 ~ 2017.08
- **소속팀** : A-FIRST
- **직위** : 선임연구원



<u>**주요프로젝트 및 업무**</u>

- **Athena** (악성코드의 정형/비정형 데이터 분석)

  -	**기간**: 2014 ~ 2017

  -	**기술**: Hadoop, 악성코드 분석, ML

  -	**목표**:
    -	악성코드 탐지 및 정형/비정형 데이터 분석을 통한 보안 강화
    -	악성 도메인 탐지 모델 개발 및 분석


  -	**주요 내용**:
    -	하둡으로 데이터 이관 및 중복 제거
    -	도메인 생성 시간 및 패턴 분석을 통한 악성 도메인 탐지


  -	**성과**:
    -	악성 도메인 탐지 정확도 향상
    -	보안 시스템 효율성 향상 및 탐지 속도 개선




- **기업 해킹 사고 분석**

  -	**기간**: 2009 ~ 2014년

  -	**기술**: Hadoop, 악성코드 분석, ML, Elasticsearch, 인케이스, 포렌식 도구(로드마스터, 레피드7)

  -	**목표**:
    -	기업 해킹 사고 발생 시 원인 분석과 법적 증거 수집을 통해 신뢰성 있는 분석 제공
    -	피해 시스템 복구 및 보안 개선을 위한 대응 전략 제시



  -	**주요 내용**:

    -	시스템 수집: 해킹 사고가 발생한 시스템을 무결성 보장하는 방식으로 이미징하여 증거 수집
    -	시스템 정보 추출: 파일 시스템, 레지스트리, 로그, 메모리 등에서 아티팩트를 추출하고, 필요 시 맞춤형 도구를 개발하여 데이터 추출
    -	시스템 분석: 대규모 데이터 분석을 위해 Elasticsearch를 사용하여 기록을 인덱싱하고, 시각화 도구로 사고 경로 분석
    -	결과 보고: 사고 분석 결과를 전문가와 비전문가 모두 이해할 수 있는 보고서 형식으로 작성하고, 보안 조치 사항 제안


  -	**성과**:

    -	법적 효력을 갖춘 증거 수집으로 기업의 신뢰성 향상

    -	포렌식 도구 개발 및 적용으로 기업의 보안 체계를 강화하고 사고 대응력 향상

    -	사고 예방을 위한 장기적 보안 전략 수립에 기여, 보안 사고 복구 시간을 단축




