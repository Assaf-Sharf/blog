---
layout: single
title:  "핸즈온 머신러닝 - 10"
categories : scikit-learn
tag : [scikit-learn, machine-learning, python]
toc: true
toc_sticky: true

---

![header](https://capsule-render.vercel.app/api?type=waving&color=a2dcec&height=300&section=header&text=핸즈온 머신러닝 - 10&fontSize=40&animation=fadeIn&fontAlignY=38&fontColor=FFFFFF)

- 참고 : [핸즈온 머신러닝 2판](http://www.kyobobook.co.kr/product/detailViewKor.laf?mallGb=KOR&ejkGb=KOR&barcode=9791162242964)

------------------------------------------------------

&nbsp;

## 케라스를 사용한 인공 신경망 소개

- 뇌에 있는 생물학적 뉴런의 네트워크에서 영감을 받은 머신러닝 모델

&nbsp;

### 생물학적 뉴런에서 인공 뉴런까지

- 신경망을 훈련하기 위한 데이터가 많아짐, 인공 신경망은 규모가 크고 복잡한 문제에서 다른 머신러닝 기법보다 좋은 성능을 발휘
- 컴퓨터 하드웨어 발전으로 대규모 신경망 훈련이 가능, 훈련 알고리즘이 향상

&nbsp;
**뉴런을 사용한 논리 연산**

- 하나 이상의 이진 입력과 이진 출력을 가지는 형태

- 인공 뉴런은 단순히 입력이 일정 개수만큼 활성화되었을 때 출력을 생성

- ![image-20221209172857834](/images/2022-12-09-hands_on_10/image-20221209172857834.png)

  - 왼쪽 첫 번째 네트워크는 항등 함수
    - 뉴런 A 가 활성화 되면 뉴런 C도 활성화, 뉴런 A가 꺼지면 뉴런 C도 꺼짐
  - 두 번째 네트워크는 논리곱 연산을 수행
    - 뉴런 A와 B 모두 활성화될 때만 뉴런 C가 활성화
  - 세 번째 네트워크는 논리합 연산을 수행
    - 뉴런 A와 B중 하나가 활성화되면 뉴런 C가 활성호하

  > 어떤 입력이 뉴런의 활성화를 억제할 수 있다고 가정하면 네 번째 네트워크는 조금 더 복잡한 논리 명제를 계산할 수 있음, 뉴런 A가 활성화되고 뉴런 B가 비활성화될 때 뉴런 C가 활성화
  >
  > 만약 뉴런 A가 항상 활성화되어 있다면 이 네트워크는 논리 부정 연산이 됨 (뉴런 B가 비활성화될 때 뉴런 C가 활성화되고 반대로 B가 활성화 일때 뉴런C가 비활성화 됨)

&nbsp;

**퍼셉트론**

- 퍼셉트론은 TLU 또는 이따금 LTU라고 불리는 조금 다른 형태의 인공 뉴런을 기반으로 함

- 입력과 출력이 어떤 숫자이고, 각각의 입력 연결은 가중치와 연결되어 있음

- TLU는 입력의 가중치 합을 계산한 뒤 계산된 합에 계단 함수를 적용하여 결과물을 출력

- ![image-20221209175439610](/images/2022-12-09-hands_on_10/image-20221209175439610.png)

- 퍼셉트론에서 가장 널리 사용되는 계단 함수는 **헤비사이드 계단 함수**, 부호 함수를 대신 사용하기도 함

- 하나의 TLU는 간단한 선형 이진 분류 문제에 사용할 수 있음, 입력의 선형 조합을 계산해서 그 결과가 임계값을 넘으면 양성 클래스를 출력, 아닌경우에는 음성 클래스를 출력

- 퍼셉트론은 층이 하나뿐인 TLU로 구성, 각 TLU는 모든 입력에 연결되어 있음, 한 층에 있는 모든 뉴런이 이전 층의 모든 뉴런과 연결되어 있을 때 이를 **완전 연결** 층 또는 **밀집 층**이라고 함

- 퍼셉트론의 입력은 **입력뉴런**

  - 어떤 입력이 주입되도 출력으로 통과시키는 역할

- **입력층**은 모든 입력 뉴런으로 구성됨, 여기에 **편향** 특성이 더해짐

  - 전형적으로 이 편향 특성은 항상 1을 출력하는 특별한 종류의 뉴런인 편향 뉴런으로 표현

  - 아래 이미지는 샘플을 세 개의 다른 이진 클래스로 동시로 분류할 수 있으므로 다중 출력 분류기 이미지

    ![image-20221209183402186](/images/2022-12-09-hands_on_10/image-20221209183402186.png)

- 완전 연결 층의 출력 계산식

  ![image-20221209183647021](/images/2022-12-09-hands_on_10/image-20221209183647021.png)

  - X는 입력 특성의 행렬을 의미, 행은 샘플 열은 특성
  - 가중치 행렬 W는 편향 뉴런을 제외한 모든 연결 가중치를 포함. 이 행렬의 행은 입력 뉴런에 해당하고 열은 출력층에 있는 인공 뉴런에 해당
  - 편향 벡터 b는 편향 뉴런과 인공 뉴런 사이의 모든 연결 가중치를 포함, 인공 뉴런마다 하나의 편향 존재
  - Ø를 **활성화 함수**라고 함, 인공 뉴런이 TLU일 경우 이 함수는 계단 함수

- 퍼셉트론

  - 한 번에 한 개의 샘플이 주입되면 각 샘플에 대해 예측이 생성됨

  - 퍼셉트론 학습 규칙

    ![image-20221228183703251](/images/2022-12-09-hands_on_10/image-20221228183703251.png)

    - ![image-20221228183732296](/images/2022-12-09-hands_on_10/image-20221228183732296.png)는 i번째 입력 뉴런과 j번째 출력 뉴런 사이를 연결하는 가중치
    - ![image-20221228183812903](/images/2022-12-09-hands_on_10/image-20221228183812903.png)는 현재 훈련 샘플의 i번째 뉴런의 입력값
    - ![image-20221228183937939](/images/2022-12-09-hands_on_10/image-20221228183937939.png)는 현재 훈련 샘플의 j번째 뉴런의 타깃값
    - ![image-20221228184027687](/images/2022-12-09-hands_on_10/image-20221228184027687.png)는 학습률

  - 각 출력 뉴런의 결정 경계는 선형이므로 복잡한 패턴을 학습하기 어려움, 하지만 훈련 샘플이 선형적으로 구분될 수 있다면 알고리즘이 정답에 수렴 이를 **퍼셉트론 수렴 이론**이라고 함

  - sklearn에서 하나의 TLU 네트워크를 구현한 Perceptron 클래스를 제공

    - 파이썬 클래스도 동일한 방식으로 사용할 수 있음

    - 예제

      ```python
      import numpy as np
      from sklearn.datasets import load_iris
      from sklearn.linear_model import Perceptron
      
      iris = load_iris()
      X = iris.data[:, (2, 3)]  # 꽃잎 길이, 꽃잎 너비
      y = (iris.target == 0).astype(np.int)
      
      per_clf = Perceptron(max_iter=1000, tol=1e-3, random_state=42)
      per_clf.fit(X, y)
      
      y_pred = per_clf.predict([[2, 0.5]])
      y_pred
      >> array([1])
      ```

  - 퍼셉트론은 클래스 확률을 제공하지 않으며 고정된 임곗값을 기준으로 예측을 수행, 이 때문에 퍼셉트론보다 로지스틱 회귀가 더 선호됨

&nbsp;

### 다층 퍼셉트론과 역전파

- 다층 퍼셉트론은 **입력 층** 하나와 **은닉 층** 이라 불리는 하나 이상의 TLU층과 마지막 **출력 층**으로 구성
- 입력층과 가까운 층을 보통 하위 층이라 부르고 출력에 가까운 층을 **상위 층** 이라고 부름
- 출력층을 제외하고 모든 층은 편향 뉴런을 포함하여 다음 층과 완전히 연결되어 있음

![image-20221228185456921](/images/2022-12-09-hands_on_10/image-20221228185456921.png)

- 은닉층을 여러 개 쌓아 올린 인공 신경망을 **심층 신경망**(DNN)이라고 함
- 반복하는 과정 **에포크**, 마지막 출력층의 출력을 계산하는 것을 **정방향 계산**, 마지막으로 알고리즘의 경사 하강법을 수행하여 계산한 오차 그레이디언트를 사용해 네트워크에 있는 모든 연결 가중치 수정을 **역전파**

&nbsp;

### 케라스로 다층 퍼셉트론 구현하기

&nbsp;

**케라스를 사용하여 데이터셋 적재하기**

- 이미지 크기는 28x28

```python
fashion_mnist = keras.datasets.fashion_mnist
(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()
```

- 이미지 정규화

```python
X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]
X_test = X_test / 255.
```

- 클래스 이름의 리스트 생성

``` python
class_names = ["T-shirt/top", "Trouser", "Pullover", "Dress", "Coat",
               "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"]
```

&nbsp;

**시퀀셜 API를 사용하여 모델 만들기**

- 두 개의 은닉층으로 이루어진 분류용 다층 퍼셉트론

```python
model = keras.models.Sequential()
model.add(keras.layers.Flatten(input_shape=[28, 28]))
model.add(keras.layers.Dense(300, activation="relu"))
model.add(keras.layers.Dense(100, activation="relu"))
model.add(keras.layers.Dense(10, activation="softmax"))
```

- model.summary()를 통한 모델 요약

  ![image-20221228191918702](/images/2022-12-09-hands_on_10/image-20221228191918702.png)

&nbsp;

**모델 컴파일**

- 모델 작성후 compile() 메서드를 호출하여 사용할 함수와 옵티마이저를 지정해야 함
- 훈련과 평가 시에 계산할 지표를 추가로 지정할 수 있음
  - class가 배타적인 정수 임으로 loss를 sparse_categorical_crossentropy
  - 샘플마다 클래스별 타깃 확률을 가지고 있다면 categorical_crossentropy
  - 이진 분류시 soft-max 대신 sigmoid 사용하고 binary_crossentropy 사용

```python
model.compile(loss="sparse_categorical_crossentropy",
              optimizer="sgd",
              metrics=["accuracy"])
```

&nbsp;



**모델 훈련과 평가**

- class_weight 매개변수를 지정하면 적게 등장하는 클래스에 높은 가중치, 많이 나오는 클래스에 적은 가중치 부여
- 샘플별로 가중치를 부여하고 싶다면 sample_weight 매개변수를 지정

```python
history = model.fit(X_train, y_train, epochs=30,
                    validation_data=(X_valid, y_valid))
```

- history 객체에 훈련 파라미터, 에포크등 다양한 정보를 가지고 있음

&nbsp;

**모델을 사용해 예측을 만들기**

- 모델의 predict()를 사용해 새로운 예측을 생성할 수 있음

```python
X_new = X_test[:3]
y_proba = model.predict(X_new)
y_proba.round(2)

# y_pred = model.predict_classes(X_new)
y_pred = np.argmax(model.predict(X_new), axis=-1)
y_pred
>> array([9, 2, 1])

np.array(class_names)[y_pred]
>> array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')
```

&nbsp;

### 시퀀셜 API를 사용하여 회귀용 다층 퍼셉트론 만들기

- 사용할 데이터 적제하기
  - 데이터셋 로드후 스케일 조정

```python
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

housing = fetch_california_housing()

X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)
X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_valid = scaler.transform(X_valid)
X_test = scaler.transform(X_test)
```

- 모델 생성및 훈련

```python
model = keras.models.Sequential([
    keras.layers.Dense(30, activation="relu", input_shape=X_train.shape[1:]),
    keras.layers.Dense(1)
])
model.compile(loss="mean_squared_error", optimizer=keras.optimizers.SGD(learning_rate=1e-3))
history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))
mse_test = model.evaluate(X_test, y_test)
X_new = X_test[:3]
y_pred = model.predict(X_new)

y_pred
>>   array([[0.38856643],
           [1.6792021 ],
           [3.1022797 ]], dtype=float32)
```

&nbsp;
### 함수형 API를 사용해 복잡한 모델 만들기

