---
layout: single
title:  "핸즈온 머신러닝 - 4(진행중)"
categories : scikit-learn
tag : [scikit-learn, machine-learning, python]
toc: true
toc_sticky: true
---

![header](https://capsule-render.vercel.app/api?type=waving&color=a2dcec&height=300&section=header&text=핸즈온 머신러닝 - 4&fontSize=40&animation=fadeIn&fontAlignY=38&fontColor=FFFFFF)

- 참고 : [핸즈온 머신러닝 2판](http://www.kyobobook.co.kr/product/detailViewKor.laf?mallGb=KOR&ejkGb=KOR&barcode=9791162242964)

------------------------------------------------------

&nbsp;



## 모델 훈련

- 직접 계산할 수 있는 공식을 사용하여 훈련 세트에 가장 잘 맞는 모델 파라미터를 해석적으로 구함
- 경사 하강법(GD)라고 불리는 반복적인 최적화 방식을 사용하여 모델 파라미터를 조금씩 바꾸면서 비용 함수를 훈련 세트에 대해 최소화 시킴

&nbsp;

## 선형 회귀

- ![image-20220927163018049](/images/2022-09-27-hands_on_4/image-20220927163018049.png)

- 선형이지만 특성이 많아지면 고차원이 됨

- 파라미터 기준은 MSE 최소 하는 파라미터 선정

  ![image-20220927163142878](/images/2022-09-27-hands_on_4/image-20220927163142878.png)

> 입력 특성의 가중치 합 + 편향(절편, x0) 이라는 상수를 더해 예측을 생성

&nbsp;

## 정규방정식

- 공식

​	![image-20220927163532147](/images/2022-09-27-hands_on_4/image-20220927163532147.png)

- 비용함수를 최소하 하는 𝜽 값을 찾기위한 해석적 방법

**정규 방정식 예시**

```python
import numpy as np

X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

plt.plot(X, y, "b.")
plt.xlabel("$x_1$", fontsize=18)
plt.ylabel("$y$", rotation=0, fontsize=18)
plt.axis([0, 2, 0, 15])
plt.show()


X_b = np.c_[np.ones((100, 1)), X]  # 모든 샘플에 x0 = 1을 추가합니다. (X0 = 1)
theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y) # np.linalg.inv로 역행렬 계산
>> array([[4.21509616],
          [2.77011339]])

# 구한 theta_best를 통해서 예측 수행

X_new = np.array([[0], [2]])
X_new_b = np.c_[np.ones((2, 1)), X_new]  # 모든 샘플에 x0 = 1을 추가합니다.
y_predict = X_new_b.dot(theta_best)
y_predict
>> array([[4.21509616],
          [9.75532293]])
```

- 결과 시각화

```python
plt.plot(X_new, y_predict, "r-")
plt.plot(X, y, "b.")
plt.axis([0, 2, 0, 15])
plt.show()
```

![image-20220927164423415](/images/2022-09-27-hands_on_4/image-20220927164423415.png)

&nbsp;



## sklearn 선형 회귀

- `가중치(coef)` , `편향(intercept)` 호출 가능
- `LinearRegression` 클래스는 `scipy.linalg.lstsq()` 함수(최소 제곱)을 사용

```python
from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(X, y)
lin_reg.intercept_, lin_reg.coef_
>> (array([4.21509616]), array([[2.77011339]]))

lin_reg.predict(X_new)
>> array([[4.21509616],
       	  [9.75532293]])


# scipy.linalg.lstsq() or np.linalg.lstsq() 로 표현하면
theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)
theta_best_svd
>> array([[4.21509616],
       	  [2.77011339]])


```

- 이때 `np.linalg.lstsq()` 함수는  유사 역행렬을 계산함

  - `np.linalg.pinv()`을 사용해서 유사 역행렬을 직접 계산 가능
  - `유사 역행렬`은 역행렬을 계산할수 없을때 역행렬과 유사한 행렬을 계산
  - ![image-20220927191720050](/images/2022-09-27-hands_on_4/image-20220927191720050.png)

```python
# 유사 역행렬 구하는 방법
np.linalg.pinv(X_b).dot(y)
>> array([[4.21509616],
          [2.77011339]])
```

- 유사역행렬 자체는 `특이값 분해(SVD)`라 부르는 표준 행렬 분해 기법을 사용해 계산
- ![image-20220927191654670](/images/2022-09-27-hands_on_4/image-20220927191654670.png)
- ![image-20220927191707760](/images/2022-09-27-hands_on_4/image-20220927191707760.png)
- 특성이 중복되어 역행렬이 없을때 정규방정식이 작동하지 않지만 `유사역행렬` 은 항상 존재

&nbsp;

