---
layout: single
title:  "핸즈온 머신러닝 - 4(진행중)"
categories : scikit-learn
tag : [scikit-learn, machine-learning, python]
toc: true
toc_sticky: true
---

![header](https://capsule-render.vercel.app/api?type=waving&color=a2dcec&height=300&section=header&text=핸즈온 머신러닝 - 4&fontSize=40&animation=fadeIn&fontAlignY=38&fontColor=FFFFFF)

- 참고 : [핸즈온 머신러닝 2판](http://www.kyobobook.co.kr/product/detailViewKor.laf?mallGb=KOR&ejkGb=KOR&barcode=9791162242964)

------------------------------------------------------

&nbsp;



## 모델 훈련

- 직접 계산할 수 있는 공식을 사용하여 훈련 세트에 가장 잘 맞는 모델 파라미터를 해석적으로 구함
- 경사 하강법(GD)라고 불리는 반복적인 최적화 방식을 사용하여 모델 파라미터를 조금씩 바꾸면서 비용 함수를 훈련 세트에 대해 최소화 시킴

&nbsp;

## 선형 회귀

- ![image-20220927163018049](/images/2022-09-27-hands_on_4/image-20220927163018049.png)

- 선형이지만 특성이 많아지면 고차원이 됨

- 파라미터 기준은 MSE 최소 하는 파라미터 선정

  ![image-20220927163142878](/images/2022-09-27-hands_on_4/image-20220927163142878.png)

> 입력 특성의 가중치 합 + 편향(절편, x0) 이라는 상수를 더해 예측을 생성

&nbsp;

### 정규 방정식

- 공식

​	![image-20220927163532147](/images/2022-09-27-hands_on_4/image-20220927163532147.png)

- 비용함수를 최소하 하는 𝜽 값을 찾기위한 해석적 방법

**정규 방정식 예시**

```python
import numpy as np

X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

plt.plot(X, y, "b.")
plt.xlabel("$x_1$", fontsize=18)
plt.ylabel("$y$", rotation=0, fontsize=18)
plt.axis([0, 2, 0, 15])
plt.show()


X_b = np.c_[np.ones((100, 1)), X]  # 모든 샘플에 x0 = 1을 추가합니다. (X0 = 1)
theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y) # np.linalg.inv로 역행렬 계산
>> array([[4.21509616],
          [2.77011339]])

# 구한 theta_best를 통해서 예측 수행

X_new = np.array([[0], [2]])
X_new_b = np.c_[np.ones((2, 1)), X_new]  # 모든 샘플에 x0 = 1을 추가합니다.
y_predict = X_new_b.dot(theta_best)
y_predict
>> array([[4.21509616],
          [9.75532293]])
```

- 결과 시각화

```python
plt.plot(X_new, y_predict, "r-")
plt.plot(X, y, "b.")
plt.axis([0, 2, 0, 15])
plt.show()
```

![image-20220927164423415](/images/2022-09-27-hands_on_4/image-20220927164423415.png)

&nbsp;



## sklearn 선형 회귀

- `가중치(coef)` , `편향(intercept)` 호출 가능
- `LinearRegression` 클래스는 `scipy.linalg.lstsq()` 함수(최소 제곱)을 사용

```python
from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(X, y)
lin_reg.intercept_, lin_reg.coef_
>> (array([4.21509616]), array([[2.77011339]]))

lin_reg.predict(X_new)
>> array([[4.21509616],
       	  [9.75532293]])


# scipy.linalg.lstsq() or np.linalg.lstsq() 로 표현하면
theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)
theta_best_svd
>> array([[4.21509616],
       	  [2.77011339]])


```

- 이때 `np.linalg.lstsq()` 함수는  유사 역행렬을 계산함

  - `np.linalg.pinv()`을 사용해서 유사 역행렬을 직접 계산 가능
  - `유사 역행렬`은 역행렬을 계산할수 없을때 역행렬과 유사한 행렬을 계산
  - ![image-20220927191720050](/images/2022-09-27-hands_on_4/image-20220927191720050.png)

```python
# 유사 역행렬 구하는 방법
np.linalg.pinv(X_b).dot(y)
>> array([[4.21509616],
          [2.77011339]])
```

- 유사역행렬 자체는 `특이값 분해(SVD)`라 부르는 표준 행렬 분해 기법을 사용해 계산
- ![image-20220927191654670](/images/2022-09-27-hands_on_4/image-20220927191654670.png)
- ![image-20220927191707760](/images/2022-09-27-hands_on_4/image-20220927191707760.png)
- 특성이 중복되어 역행렬이 없을때 정규방정식이 작동하지 않지만 `유사역행렬` 은 항상 존재

&nbsp;



## 경사 하강법(GD)

- 여러 종류의 문제에서 최적의 해법을 찾을 수 있는 일반적인 최적화 알고리즘
- 비용함수 최적화를 위해 반복수행을 통해 파라미터 조정
- 현 상태의 기울기가 감소하는 방향으로 진행하다가 0인 지점이 최솟값
- 스텝의 크기는 학습률 파라미터로 결정됨
  - 학습률이 너무 작으면 수렴을 위한 반복이 많아지고 시간소요가 커짐
  - 학습률이 너무 크면 큰 값으로 발산할 위험이 존재

![image-20220928110628368](/images/2022-09-27-hands_on_4/image-20220928110628368.png)

> 학습 스텝은 최솟값에 가까워 질수록 줄어듦



![image-20220928110918423](/images/2022-09-27-hands_on_4/image-20220928110918423.png)

> 위 그림과 같이 왼쪽에서 시작하면 지역 최솟값에 빠질 위험이 크고, 오른쪽에서 시작하면 평탄한 부분을 지나는 동안 학습이 오래 걸리며 조기 종료되어 전역 최솟값에 이르지 못할수 있음



**`MSE 비용함수`**

- 선형 회귀를 위한 MSE 비용 함수는 곡선에서 어떤 두 점을 선택해 선을 그어도 곡선을 가로지르지 않는 `볼록함수`

  - 지역 최솟값 X , 전역 최솟값 0

  - 연속된 함수이고 기울기가 변하지 않음

    > 경사 하강법이 전역 최솟값에 가깝게 접근할 수 있다는 것을 보장



**`특성 스케일`**

![image-20220928112312958](/images/2022-09-27-hands_on_4/image-20220928112312958.png)

- 왼쪽이 특성 스케일 적용, 오른쪽은 미적용
- 경사 하강법을 사용할 때는 반드시 모든 특성이 같은 스케일을 갖도록 만들어야함, 그렇지 않으면 같은 결과 도달까지 오랜 시간 소요

&nbsp;

### 배치 경사 하강법

- 경사 하강법 구현시 각 모델 파라미터에 대해 비용 함수의 기울기를 계산

- 모델 파라미터가 조금 변경될 때 비용 함수가 얼마나 변경되는지 계산 <- `편도함수`

- ![image-20220928113007641](/images/2022-09-27-hands_on_4/image-20220928113007641.png)

  - 비용함수의 MSE 그레이디언트 벡터는 비용함수의 편도함수를 모두 가지고있음

    > 매 경사 하강법 스텝에서 전체 훈련 세트 X에 대해 계산 그래서 **배치 경사 하강법**
    >
    > 매우 큰 훈련 세트에서는 아주 느림, 특성수에 민감하지 않아서 수백만 특성에서는 정규방적식, SVD 보다 경사 하강법이 빠름

- 위로 향하는 그레이디언트 벡터가 구해지면 반대 방향인 아래로 이동 **theta - MSE(theta)** 를 의미, 내려가는 스텝의 크기를 결정하기 위해  n : 학습률 사용

  - 공식

    ![image-20220928113641528](/images/2022-09-27-hands_on_4/image-20220928113641528.png)

    ```python
    eta = 0.1  # 학습률
    n_iterations = 1000
    m = 100
    
    theta = np.random.randn(2,1)  # 랜덤 초기화
    
    for iteration in range(n_iterations):
        gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)
        theta = theta - eta * gradients
    
    theta
    >> array([[4.21509616],
              [2.77011339]])
    ```

  - 학습률에 따른 경사 하강법 시각화

    ![image-20220928114155599](/images/2022-09-27-hands_on_4/image-20220928114155599.png)

    - 왼쪽의 경우 학습률이 너무 낮고 최적점 도달까지 시간소요 큼

    - 가운데는 적절한 학습률 빠르게 최적점 도달

    - 오른쪽은 너무 큰 학습률로 최적점에 멀어져 발산

> 적절한 학습률을 찾기 위해서는 그리드 탐색을 사용, 그리드 탐색은 수렴하는데 너무 오래 걸리는 모델을 막기위해 반복 횟수를 제한해야 함
>
> `간단한 방법은 반복횟수 크게 지정하고 그레디언트 벡터가 아주 적어지면 중지`

&nbsp;

### 확률적 경사 하강법

![image-20220928125131057](/images/2022-09-27-hands_on_4/image-20220928125131057.png)

- 배치 경사 하강법은 매 스텝마다 전체 훈련 세트를 사용해 그레디언트를 계산함, 훈련 세트 크면 느림
- 확률적 경사 하강법은 매 스텝에서 한 개의 샘플을 무작위로 선택하고 그 셈플에 대한 그레이디언트를 계산
  - 매 반복마다 계산하는 데이터가 매우 적기 때문에 한 번에 하나의 샘플 처리
- 무작위 적인 특성 때문에 배치 경사 하강법보다 훨씬 불안정
  - 비용함수가 최솟값에 다다를 때까지 요동치면서 감소
  - 요동을 치기때문에 전역을 지나칠수 있음

> 점진적으로 학습률을 감소시켜서 요동치는 문제를 해결
>
> 시작할때는 학습률을 크게하고 점차 작게 줄여서 알고리즘이 전역으로 수렴하도록 함



**학습 스케줄을 사용한 확률적 경사 하강법**

```python
n_epochs = 50
t0, t1 = 5, 50  # 학습 스케줄 하이퍼파라미터

def learning_schedule(t):
    return t0 / (t + t1)

theta = np.random.randn(2,1)  # 랜덤 초기화

for epoch in range(n_epochs):
    for i in range(m):
        if epoch == 0 and i < 20:                    
            y_predict = X_new_b.dot(theta)           
            style = "b-" if i > 0 else "r--"         
            plt.plot(X_new, y_predict, style)        
        random_index = np.random.randint(m)
        xi = X_b[random_index:random_index+1]
        yi = y[random_index:random_index+1]
        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)
        eta = learning_schedule(epoch * m + i)
        theta = theta - eta * gradients
        theta_path_sgd.append(theta)                 

plt.plot(X, y, "b.")                                
plt.xlabel("$x_1$", fontsize=18)                    
plt.ylabel("$y$", rotation=0, fontsize=18)           
plt.axis([0, 2, 0, 15])                              
save_fig("sgd_plot")                                 
plt.show()                                          

theta
>> array([[4.21076011],
          [2.74856079]])
```

![image-20220928125618690](/images/2022-09-27-hands_on_4/image-20220928125618690.png)

- 위에서는 셈플을 무작위로 선택, 셈플마다 뽑힐수도 있고 안뽑힐수도 있음

&nbsp;

**sklearn SGDRegressor**

- 기본값으로 제곱 오차 비용 함수를 최적화 함

```python
from sklearn.linear_model import SGDRegressor

sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1, random_state=42)
sgd_reg.fit(X, y.ravel())

sgd_reg.intercept_, sgd_reg.coef_
>> array([4.24365286]), array([2.8250878])
```

&nbsp;



### 미니배치 경사 하강법

- 각 스텝에서 전체 훈련 세트(배치 경사하강법)이나 하나의 샘플(확률적 경사 하강법)을 기반으로 그레디언트를 계산하는 것이 아니라 **미니배치**라 부르는 임의의 작은 샘플 세트에 대해 그레디언트를 계산.
- gpu 사용가능
- SGD 보다 덜 불규칙하게 움직임, SGD보다 최솟값에 가까울수 있고 반대로 지역 최솟값이 될수 있음

![image-20220928155319137](/images/2022-09-27-hands_on_4/image-20220928155319137.png)

> 배치 경사 하강법이 실제로 최솟점에 도달, 확률적과 미니배치는 주변에 수렴
>
> 확률적 경사하강법과 미니배치 경사 하강법 둘다 적절한 학습 스케줄을 사용하면 최솟값에 도달

&nbsp;

**선형회귀를 사용한 알고리즘 정리**

![image-20220928155542543](/images/2022-09-27-hands_on_4/image-20220928155542543.png)



&nbsp;

## 다항 회귀

-  다항회귀란 데이터들간의 형태가 비선형 일때 데이터에 각 특성의 제곱을 추가해주어서 특성이 추가된 비선형 데이터를 선형 회귀 모델로 훈련시키는 방법



**다항회귀 예시**

```python
import numpy as np
import numpy.random as rnd
np.random.seed(42)

m = 100
X = 6 * np.random.rand(m, 1) - 3
y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)


plt.plot(X, y, "b.")
plt.xlabel("$x_1$", fontsize=18)
plt.ylabel("$y$", rotation=0, fontsize=18)
plt.axis([-3, 3, 0, 10])
save_fig("quadratic_data_plot")
plt.show()
```

![image-20220928160731677](/images/2022-09-27-hands_on_4/image-20220928160731677.png)

- 데이터에 일부 잡음이 포함되어있어서 sklearn의 **PolynomialFeatures**를 사용해 훈련데이터로 변환
  - 훈련 세트에 각 특성을 제곱하여 새로운 특성을 만듬

```python
from sklearn.preprocessing import PolynomialFeatures
poly_features = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly_features.fit_transform(X)

X[0]
>> array([-0.75275929])
X_poly[0]
>> array([-0.75275929,  0.56664654])
```

> x_poly는 원래의 특성 X와 특성의 제곱을 포함

```python
lin_reg = LinearRegression()
lin_reg.fit(X_poly, y)
lin_reg.intercept_, lin_reg.coef_
>> array([1.78134581]), array([[0.93366893, 0.56456263]])


X_new=np.linspace(-3, 3, 100).reshape(100, 1)
X_new_poly = poly_features.transform(X_new)
y_new = lin_reg.predict(X_new_poly)
plt.plot(X, y, "b.")
plt.plot(X_new, y_new, "r-", linewidth=2, label="Predictions")
plt.xlabel("$x_1$", fontsize=18)
plt.ylabel("$y$", rotation=0, fontsize=18)
plt.legend(loc="upper left", fontsize=14)
plt.axis([-3, 3, 0, 10])
plt.show()
```

![image-20220928165417406](/images/2022-09-27-hands_on_4/image-20220928165417406.png)

> 특성이 여러 개일 때 다항 회귀는 이 특성 사이의 관계를 찾음
>
> PolynomialFeatures가 주어진 차수까지 특성 간의 모든 교차항을 추가함



&nbsp;

