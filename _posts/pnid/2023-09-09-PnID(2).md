---
layout: single
title: "심볼 인식 1편"
permalink: /projects/pnid/02
tag: [Vision AI, Drawing]
header:
  teaser: /assets/images/drawing_recognition_teaser.jpg
author_profile: false
sidebar:
  nav: "docs1"
date: 2023-09-18
use_math: true
---

 이번 포스트에서는 설계도면(P&ID)에서 핵심적인 요소 중 하나인 **'심볼'**에 주목하여 그 인식과 관련된 기술과 개발 과정을 소개 하겠습니다. <br><br>
 설계 도면에서의 심볼은 밸브, 계장 기기, 펌프 등을 도식화한 도형으로, 프로세스 설계와 운영에 있어서 매우 중요한 역할을 합니다. 따라서, 이를 정확하게 인식하고 해석하는 기술은 프로젝트의 핵심 요소 중 하나입니다. <br><br>
 이번 포스트에서는 인식 대상인 '심볼'에 대한 이해를 높이고, 이를 정확하게 인식하는 기술과 개발 과정 및 한계점을 탐구하겠습니다.
 <br><br>

<div align="center">
<img src="../../assets/images/2023-09-09-PnID(2)/gate_valve-1695033311953-2.png" alt="gate_valve" style="zoom:50%;" />
</div>
<center>[ (좌)실제 게이트 밸브, (우)게이트 밸브를 도식화한 P&ID 심볼 ]</center>  <br>

## 📋 Table of Contents

1. [심볼 정의 및 종류](#1-심볼-정의-및-종류)
2. [기반 기술](#2-기반-기술)
3. [심볼 인식을 위한 딥러닝 모델 타임라인](#3-심볼-인식을-위한-딥러닝-모델-타임라인)
4. [심볼 인식 모델 구축 과정](#4-심볼-인식-모델-구축-과정)
5. [한계점](#5-한계점)

## 1. 심볼 정의 및 종류
### 심볼 정의
P&ID에서 심볼은 특정한 기기, 장치, 또는 프로세스를 나타내는 기호 또는 도형으로 정의됩니다. 각 심볼은 그림과 함께 설명서 또는 레전드에 포함되어 해당 기기 또는 장치에 대한 상세 정보를 담고 있습니다.

### 심볼 종류
 일반적으로 사용되는 심볼에는 밸브, 펌프, 계측 제어 기기 등 15개 이상의 카테고리가 있으며, 600여 개 이상의 하위 클래스로 분류됩니다. 
 <br><br> 아래는 밸브류 카테고리들의 심볼 예시입니다.

![밸브류 심볼 레전드 도면](../../assets/images/2023-09-09-PnID(2)/valve_legend.png)
<center>[ 밸브류 심볼 레전드 도면 ]</center>
*출처: [블로그 링크](https://blog.naver.com/jhmillennium/223110345122){:target="_blank"}*

## 2. 기반 기술

객체를 인식하는 기반 기술은 크게 이미지 세그멘테이션(Image Segmentation)과 객체 탐지(Object Detection) 기술을 중심으로 합니다. 이 두가지 기술은 다음과 같은 개념 및 차이점을 가지고 있습니다.

### 이미지 세그멘테이션 (Image Segmentation)
 - 이미지 세그멘테이션은 이미지를 픽셀 수준에서 세분화하는 작업을 의미합니다. 즉, 이미지를 작은 부분으로 나누어 각 부분이 어떤 객체인지 또는 배경인지 판별합니다.
 - 주요 목표는 이미지 내의 각 픽셀을 하나의 클래스 또는 세그먼트에 할당하는 것입니다. 이것은 객체의 경계를 감지하지 않고 객체의 내부를 분할하는 작업을 포함합니다.
 - 시맨틱 세그멘테이션(Semantic Segmentation)과 인스턴스 세그멘테이션(Instance Segmentation)으로 구분됩니다.
 - 인스턴스 세그멘테이션의 대표적인 모델로는 Mask R-CNN이 있으며, RoI header와 Classification Header로 구성된 2-Stage Detector 모델이다.
 - RoI header는 객체의 위치를 정확하게 예측하는 역할을 하며, Classification Header는 객체의 클래스를 분류합니다.

### 객체 탐지 (Object Detection)
 - 객체 탐지는 이미지 상에서 객체의 존재와 그 위치를 탐지하는 작업을 의미합니다.
 - 객체 탐지는 이미지 내에서 객체의 바운딩 박스 (경계 상자)로 탐지하며, 각 객체의 클래스를 식별하는 것을 목표로 합니다.
 - 객체 탐지 모델로는 일반적으로 YOLO (You Only Look Once) 계열의 모델을 많이 활용합니다.
 - YOLO 모델은 1-Stage Detector 로서 격자 형식으로 이미지를 나누고, 격자 셀마다 객체 탐지 박스와 해당 객체의 클래스 확률을 동시에 예측합니다.

<div align="center">
<img src="../../assets/images/2023-09-09-PnID(2)/object_detection.png" alt="object_detection" style="zoom:40%;" />
</div>
    (a)image classification, (b)object detection, (c)semantic segmentation, and (d)instance segmentation. 

*출처: [Computer vision algorithms and hardware implementations](https://www.researchgate.net/publication/335007177_Computer_vision_algorithms_and_hardware_implementations_A_survey){:target="_blank"}*

### Vision Transformer (ViT)

- Vision Transformer, 줄여서 ViT는 이미지 처리를 위한 딥러닝 모델 중 하나입니다. 이 모델은 기존의 CNN(Convolutional Neural Network) 아키텍처와는 다른 방식으로 이미지의 정보를 추출합니다.
- 주요 특징으로 어텐션 메커니즘을 활용하고, 패치(patch) 기반 처리, 스케일러블한 구조를 가지고 있습니다.
- P&ID 내 심볼을 인식하기 위해서 ViT 모델에서 발전한 Swin Transformer 모델을 활용하였고, CNN 계열의 모델에 비해 더 정확한 성능을 보였습니다.

## 3. 심볼 인식을 위한 딥러닝 모델 타임라인
<div align="center">
<img src="../../assets/images/2023-09-09-PnID(2)/timeline(1).jpg" alt="(As-Is)timeline" style="zoom:50%;" />
</div>
<center>[ 심볼 인식 모델 타임라인 ]</center>
### 2020년 상반기
본 프로젝에서 심볼 인식을 수행하기 위해 선행 연구 조사를 시작했습니다. 선행 연구 중에서 *[Rahul et al.(2019)](https://arxiv.org/abs/1901.11383){:target="_blank"}*의 논문을 key paper로 선정했습니다. Rahul et al.(2019)의 연구는 설계 도면 내 객체를 인식하기 위해 세그멘테이션 기반 Fully Convolutional Network(FCN) 모델을 활용하였습니다..<br>
프로젝트 초기 단계에서, 이러한 key paper를 벤치마킹하여 *[FCN](https://arxiv.org/abs/1605.06211v1){:target="_blank"}*, *[U-Net](https://arxiv.org/abs/1505.04597v1){:target="_blank"}* 등과 같은 **시맨틱 세그멘테이션** 기반 딥러닝 모델을 활용했습니다.<br>
But, 설계 도면의 물량을 정확하게 계산하는 것이 이 프로젝트의 핵심 목표 중 하나였고, 시맨틱 세그멘테이션은 객체를 클래스별로 인식하는 데는 우수한 성과를 보였지만, 동일 클래스 내의 객체를 구분하여 물량을 파악하기에는 어려운 한계가 있었죠.<br>

<div align="center">
  <img src="../../assets/images/2023-09-09-PnID(2)/01.semantic_segmentation_architecture.jpg" alt="Semantic_Segmentation_Architecture" width="55%" />
  <img src="../../assets/images/2023-09-09-PnID(2)/01.semantic_segmentation_result.jpg" alt="Semantic_Segmentation_Results" width="40%" />
</div>
<center>[ Semantic Segmentation 기반 (좌)시스템 아키텍처와 (우)인식 결과 ]</center>
*출처: [A Study of Symbol Detection on P&ID Based on Semantic Segmentation](https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE10512414){:target="_blank"}*

### 2020년 하반기
시맨틱 세그멘테이션의 한계점을 해결하기 위해 동일 클래스 내에서 객체를 구분할 수 있는 **인스턴스 세그멘테이션** 기반 *[Mask R-CNN](https://arxiv.org/pdf/1703.06870v3.pdf){:target="_blank"}* 모델을 도입했습니다. 설계 도면의 특성 때문에 YOLO 와 같은 객체 탐지 모델을 활용하지 않았습니다. 설계 도면은 일반적으로 높은 밀집도를 가지며, 객체를 바운딩 박스 형태로 탐지하면 주변 밀접한 객체와 겹치게 됩니다. 이는 추후 과정에서 프로세스 흐름을 파악할 때 오류를 발생시킬 수 있는 원인이 됩니다. 인스턴스 세그멘테이션은 픽셀 단위로 객체를 탐지하므로 YOLO와 같은 문제점을 해결할 수 있습니다.<br><br>
학습 데이터셋 확보를 위해 rotation, scaling, elastic deformation 등 다양한 데이터 증강(data augmentation)과 합성 데이터(Synthetic Data)인 Fake Drawing을 생성하여 학습을 수행했습니다.

<div align="center">
  <img src="../../assets/images/2023-09-09-PnID(2)/02.instance_segmentation_architecture.jpg" alt="Instance_Segmentation_Architecture" width="48%" />
  <img src="../../assets/images/2023-09-09-PnID(2)/02.instance_segmentation_result01.jpg" alt="Instance_Segmentation_Results" width="40%" />
</div>
<center>[ Instance Segmentation 기반 (좌)시스템 아키텍처와 (우)인식 결과 ]</center>
*출처: [A Study of Symbol Detection on P&ID Based on Instance Segmentation](http://cde.or.kr/data/conf/2020_c/%EC%A0%9C26%ED%9A%8C%EB%8F%99%EA%B3%84%ED%94%84%EB%A1%9C%EC%8B%9C%EB%94%A9_%EB%AA%A9%EC%B0%A8%20%EB%B0%9C%EC%B7%8C.pdf){:target="_blank"}*

### 2021년 상반기
인공지능 분야에서의 석유와 같이 중요한 자원은 역시 **데이터**입니다. 하지만 석유도 우리 생활에 활용하려면 다양한 공정을 거쳐야 합니다. 마찬가지로, 데이터도 딥러닝 모델에 학습하기 위해서는 정확한 정답을 태깅하는 레이블링 작업을 거쳐야 합니다.<br><br>
본 프로젝트를 수행할 때 회사에서 보유하고 있는 도면 데이터는 풍부했지만, 레이블링된 학습 데이터셋이 부족했었습니다. ~~(프로젝트 초창기에는 충분한 예산 확보가 되지 않아 레이블링 외주를 맡기기 어려운 상태였습니다😥.)~~ 따라서 레이블링 작업을 직접 수행하게 되었고, 이는 다시 하고 싶지 않은 경험이었습니다.~~(덕분에 비문증이 생겼..)~~ 그래도 레이블링 데이터가 쌓일수록 심볼 인식 모델의 성능은 지속적으로 향상되었습니다. 특히 단순히 양이 많은 것보다 다양한 케이스의 데이터가 많을수록 모델 성능 향상의 폭이 컸습니다. (단순한 다다익선보다는 다양한 케이스의 다다익선이 더 옳은 표현인 것 같습니다.) 하지만 모델의 성능이 향상될수록 눈과 손목은 나빠졌죠☠️.<br><br>
레이블링 수작업 문제를 해결하기 위해, 앞서 설명드린 것과 같이 다양한 데이터 증강 기법과 합성 데이터인 Fake Drawing을 생성했습니다. 하지만, 모델 성능에는 인공 데이터보다는 real 데이터가 더 효과적이었습니다.<br> 따라서, real 데이터에 레이블링을 손쉽게 하기 위해 Template Matching을 활용한 **Semi-Auto Labeling** 도구를 자체적으로 개발하였습니다. Template Matching은 탐지하려는 객체 하나를 기준으로 정하고, 이와 유사한 형태의 심볼들을 탐지하는 computer vision 기술입니다. 이 방법은 유사도에 따라 탐지하는 객체의 수와 탐지 정밀도가 반비례합니다. 유사도가 높으면 탐지하는 객체의 수가 줄지만 굉장히 높은 정밀도를 보이며, 반대로 유사도를 낮추면 탐지 정밀도는 떨어지지만 최대한 많은 유사 객체를 찾습니다.<br>
Semi-Auto Labeling은 레이블링 작업자가 유사도를 직접 조정하여 레이블링하므로 완전 자동은 아니지만 반자동 형태로 레이블링 작업량을 절감할 수 있었습니다.

<div align="center">
  <img src="../../assets/images/2023-09-09-PnID(2)/03.template_matching.jpg" alt="Template_Matching" width="80%" />
</div>
    (Step 1) 레이블링 대상 객체를 선택, (step 2) 기준 객체 영역 추출, (step3) 임계치에 따라 유사한 객체 레이블링.

### 2021년 하반기 ~ 2022년 상반기
2021년 하반기, ICLR 2021에서 computer vision 분야에서도 Transformer를 활용한 *[Vision Transformer](https://arxiv.org/abs/2010.11929){:target="_blank"}(ViT)* 논문이 드디어 발표되었습니다. 이전에 구축했던 심볼 인식 모델은 CNN 계열의 모델인 ResNet을 백본 모델로 활용했었습니다. Vision AI 분야에 Transformer가 SOTA를 갱신하여 백본 모델을 ViT로 교체하고 인식 성능을 향상시켰습니다.<br>
2022년 상반기, ViT의 등장으로 인해 transformer 기반의 vision 모델들이 다수 발표되었습니다. 심볼 인식 모델 또한 ViT에서 더 발전된 *[Swin Transformer ](https://arxiv.org/abs/2103.14030v2){:target="_blank"}*의 모델로 백본을 교체하여 성능 향상을 이루어냈습니다.<br>
아래 그림과 같이 동일한 이미지에 대해 백본 모델만 ResNet에서 Swint Transformer로 변경하여 심볼 인식을 수행해보았습니다. 빨간 박스 영역처럼 노이즈가 심한 경우에서 Transformer 기반 백본 모델의 recall 성능이 더 높았고, 전반적인 심볼 인식 IoU 값도 더 향상됨을 확인할 수 있었습니다.

<div align="center">
  <img src="../../assets/images/2023-09-09-PnID(2)/02.instance_segmentation_result02.jpg" alt="Instance_Segmentation_Architecture" width="45%" />
  <img src="../../assets/images/2023-09-09-PnID(2)/04.swinT_result.jpg" alt="Instance_Segmentation_Results" width="45%" />
</div>
<center>[ Mask R-CNN의 백본별 (좌)CNN 계열 ResNet 모델, (우)Transformer 계열 Swin-T 모델 인식 결과 ]</center><br>
이와 같이 심볼 인식 모델을 개발하기 위해서 2020년 시맨틱 세그멘테이션 기반 모델에서 시작하여 2022년 상반기 Swin Transformer 모델 도입까지 3년 동안 심볼 인식 모델을 지속적으로 발전시켰습니다.

## 4. 심볼 인식 모델 구축 과정
### 데이터 수집 및 준비
딥러닝 모델을 학습시키기 위해 실제 건설 프로젝트의 설계 도면을 수집하고 준비하는 과정을 거쳤습니다. 이 과정에서 수집된 설계 도면의 양은 건설 사업의 규모에 따라 다양했습니다. 대체로 200장에서 600장 사이였고, 본 연구를 위해 특정 건설 사업을 선정하여 약 600장의 설계 도면을 사용했습니다.<br>
수집한 설계 도면은 주로 스캔본이 PDF 형식으로 저장되었으며, 딥러닝 모델 학습을 위해 이미지 파일 형태로 변환되어야 했습니다. 먼저, 도면 데이터의 해상도를 500dpi(Dots per inch)로 변환하고 이미지 크기를 4678px * 3307px으로 리사이징하였습니다. Dpi는 해상도를 나타내며, 적절한 dpi 설정은 도면 이미지를 식별하기 위한 중요한 요소였습니다. 낮은 dpi 설정은 이미지 내 객체를 인식하기 어렵게 만들고, 반면에 너무 높은 dpi 설정은 이미지의 사이즈를 너무 커지게 만들었습니다. 따라서 적절한 dpi 설정을 통해 이미지의 해상도와 크기를 최적화하였습니다.<br>
딥러닝 모델을 학습시키기 위해서는 인식하려는 심볼의 위치 정보와 해당 심볼의 클래스명이 레이블링된 데이터가 필요합니다. 이를 위해 우리는 Semi-Auto Labeling 도구를 사용하여 레이블링 작업을 수행하였습니다. 레이블링 작업은 데이터셋의 품질을 향상시키는 데 핵심적인 역할을 하기 때문에 여러 차례 크로스체크를 거쳐 작업을 완료하였으며, 총 30장의 도면에 대한 레이블링 작업을 마무리했습니다.

### 데이터 전처리
레이블링된 데이터를 딥러닝 모델에 학습시키기 위해 몇 가지 추가 작업이 필요합니다. 첫째로, 학습에 사용할 데이터셋의 양이 부족하고, 다양한 케이스를 포함해야 했습니다. 둘째로, 모델의 입력 레이어로 사용하기 위해 이미지를 조정해야 했습니다.

***데이터 양 부족 문제 해결을 위한 Data Augmentation:*** 초기 데이터셋의 양이 한정적이었기 때문에 Data Augmentation을 수행하였습니다. 도면 데이터를 아래의 코드와 같이 회전(90도, 180도, 270도) 및 뒤집기(상하, 좌우) 등을 적용하여 데이터를 증강하였습니다.

<details>
<summary>코드 접기/펼치기</summary>
<div markdown="1">

```python
class ImageAugmentation:
    def __init__(self, settings):
        self.settings = settings
        self.input_folderpath = self.settings['inputs']['input_folderpath']
        self.output_folderpath = self.settings['inputs']['output_folderpath']
# ```(생략)```
    # 데이터 형식 변경 메서드
    def dataformation(self, aug_img, aug_path, data, shape_type, new_coordinates, deformation_type):
# ```(생략)```
        # JSON 내의 모양 좌표 업데이트
        for i in range(len(new_coordinates)):
            data["shapes"][i]["shape_type"] = shape_type[i]
            data["shapes"][i]["points"] = new_coordinates[i]

        # JSON 내의 이미지 경로, 높이 및 너비 업데이트
        data["imagePath"] = new_path
        data["imageHeight"] = aug_img.shape[0]
        data["imageWidth"] = aug_img.shape[1]

        # 업데이트된 JSON을 출력 파일에 작성
        with open(json_path, 'w') as outfile:
            json.dump(data, outfile, indent=2)
# ```(생략)```
    # 전체 파이프라인 메서드
    def pipeline(self):
# ```(생략)```
        ts = self.settings['transforms']

        # 회전 각도 확인
        if ts['rotation']['rotation_state'] == True:
            rotangle = ts['rotation']['rotangle']
            if rotangle >= 360:
                rotangle = 359
        else:
            rotangle = 0

        # 주석이 달린 이미지 반복
        for img in inp_imgs:
# ```(생략)```
            # transforms 적용
            new_aug_cor = list()
            for i in tqdm(range(length)):
                # Rotate
                aug_coordinates = coordinates[i]
                if ts['rotation']['rotation_state'] == True:
                    if rotangle != 0:
                        aug_coordinates = transforms().rotation(anno_img, aug_coordinates, rotangle)

                    deformation_type = 'rot' + str(rotangle)

                # Flip
                if ts['flipping']['flipping_state'] == True:
                    if ts['flipping']['vertical_flip'] == True:
                        aug_coordinates = transforms().flipvertical(anno_img, aug_coordinates)
                        deformation_type = 'vflip'
                    if ts['flipping']['horizontal_flip'] == True:
                        aug_coordinates = transforms().fliphorizontal(anno_img, aug_coordinates)
                        deformation_type = 'hflip'

                if len(aug_coordinates) == 0:
                    continue

                if ts['flipping']['flipping_state'] and ts['rotation']['rotation_state']:
                    deformation_type = 'rot' + str(rotangle) + '+' + deformation_type

                new_aug_cor.append(aug_coordinates)

            # 이미지에 회전 및 뒤집기 변환 적용
            if rotangle != 0:
                aug_img = ndimage.rotate(anno_img, rotangle, reshape=True)
            else:
                aug_img = anno_img
            if ts['flipping']['vertical_flip'] == True:
                aug_img = anno_img[::, ::-1]
            if ts['flipping']['horizontal_flip'] == True:
                aug_img = anno_img[::-1, ::]

            # 새로운 주석 데이터 생성 및 저장
            obj.dataformation(aug_img, aug_path, data, shape_type, new_aug_cor, deformation_type)

if __name__ == '__main__':
# ```(생략)```
    # 적용할 augmentation 방법 설정
    settings = {
# ```(생략)```
        'transforms': {
            'rotation': {
                'rotation_state': True,
                'rotangle': 0
            },
            'flipping': {
                'flipping_state': False,
                'horizontal_flip': False,
                'vertical_flip': False
            }
        }
    }
    # 회전 augmentation 방법 적용
    for rotangle in [0, 90, 180, 270]:
        print('[+] rotangle : ', rotangle)
        settings['transforms']['rotation']['rotangle'] = rotangle
        obj = ImageAugmentation(settings)
        obj.pipeline()
    # 좌우 반전 augmentation 방법 적용
    settings['transforms']['rotation']['rotation_state'] = False
    settings['transforms']['flipping']['flipping_state'] = True
    settings['transforms']['flipping']['horizontal_flip'] = True
    print('[+] flip : ', 'horizontal_flip')
    obj = ImageAugmentation(settings)
    obj.pipeline()

    # 상하 반전 augmentation 방법 적용
    settings['transforms']['flipping']['horizontal_flip'] = False
    settings['transforms']['flipping']['vertical_flip'] = True
    print('[+] flip : ', 'vertical_flip')
    obj = ImageAugmentation(settings)
    obj.pipeline()
# ```(생략)```
```
</div>
</details>
<br>

***이미지 조정을 위한 이미지 패치 생성:*** 모델의 입력 이미지로 사용하기 위해 원본 도면 이미지를 패치 단위로 잘라내었습니다. 원본 도면 이미지의 크기는 4678px * 3307px 이었는데, 모델에 입력될 때 이미지 크기가 축소되면서 심볼이 너무 작아져 인식이 어려워지는 문제가 있었습니다. 이를 해결하기 위해 Mask R-CNN 논문에서 권장하는 1024px * 1024px 크기의 패치 이미지로 자르기로 결정했습니다. <br>
이미지 패치를 생성할 때, 단순히 격자로 이미지를 잘라내면 이미지의 일부가 잘리는 문제가 발생할 수 있으므로, 겹치는 부분(overlap)을 70%로 설정하여 sliding window 방식을 사용하여 도면 한 장당 약 116장의 패치 이미지를 추출했습니다.

***레이블 데이터 포맷 변환과 데이터셋 분할:*** 마지막으로, 레이블 데이터의 형식을 coco 데이터셋 레이블 포맷으로 변환하고, 학습용과 검증용 데이터셋을 8:2의 비율로 구분하였습니다.

이와 같은 데이터 전처리 과정을 통해 모델이 효과적으로 학습될 수 있도록 데이터셋을 준비하였습니다.

### 모델 학습
***하이퍼파라미터 수정:*** 모델 학습을 위해서 준비한 데이터에 맞게 몇몇 하이퍼파라미터를 조정해야합니다.
1. Swin_Transformer_Object_Detection/configs/swin/cascade_mask_rcnn_swin_base_patch4_window7_mstrain_480-800_giou_4conv1f_adamw_3x_pid.py
- base = 참조하는 파일명으로 변경
- num_classes 변경 (int)
- max_epochs 변경 (int, defualt=200)

  <details>
  <summary>코드 접기/펼치기</summary>
  <div markdown="1">
  ```python
  _base_ = [
      '../_base_/models/cascade_mask_rcnn_swin_fpn.py',
      '../_base_/datasets/pid_instance.py',
      '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'
  ]
  # ```(생략)```
  roi_head=dict(
    bbox_head=[
      dict(
        type='ConvFCBBoxHead',
        num_classes=119,
      )
  # ```(생략)```
      dict(
        type='ConvFCBBoxHead',
        num_classes=119,
      )
  # ```(생략)```
      dict(
        type='ConvFCBBoxHead',
        num_classes=119,
      )
    ]
  )
  # ```(생략)```
  runner = dict(type='EpochBasedRunnerAmp', max_epochs=200)
  ```
  </div>
  </details>
  <br>

2. Swin_Transformer_Object_Detection/configs/base/models/cascade_mask_rcnn_swin_fpn.py
- num_classes 변경 (int)
위와 동일하게 'num_classes' 변수에 인식하고자하는 클래스 개수로 수정합니다.

3. Swin_Transformer_Object_Detection/configs/base/datasets/pid_instance.py
- dataset_type 변경 : 'PIDDataset'
- dataset_root 변경 : '~/data/train_dataset/training/pid_dataset/'
- 데이터셋 경로 변경
  <details>
  <summary>코드 접기/펼치기</summary>
  <div markdown="1">
  ```python
  dataset_type = 'PIDDataset'
  data_root = '~/data/train_dataset/training/pid_dataset/'
  # ```(생략)```
  data = dict(
  samples_per_gpu=1,
  workers_per_gpu=2,
  train=dict(
    type=dataset_type,
    ann_file=data_root + 'train.json',
    img_prefix=data_root + 'train_images/',
    pipeline=train_pipeline),
  val=dict(
    type=dataset_type,
    ann_file=data_root + 'val.json',
    img_prefix=data_root + 'val_images/',
    pipeline=test_pipeline),
  test=dict(
    type=dataset_type,
    ann_file=data_root + 'test.json',
    img_prefix=data_root + 'test_images/',
    pipeline=test_pipeline))
  # ```(생략)```
  ```
  </div>
  </details>
  <br>

4. Swin_Transformer_Object_Detection/configs/base/default_runtime.py
- checkpoint_config = dict(interval=5) #저장할 epoch의 interval 설정(int)

5. Swin_Transformer_Object_Detection/mmdet/datasets/pid.py
- class명 PIDDataset으로 변경 
- CLASSES 변경 (str)
  <details>
  <summary>코드 접기/펼치기</summary>
  <div markdown="1">
  ```python
  @DATASETS.register_module()
  class PIDDataset으로(CustomDataset):
    CLASSES = ('1101', '137', '142', '103', '104', '107', '102', '136', '158', '105', '108', '1102', '131', '145', '110', '161', '122', '155', '133', '159', '162', '160', '1103', '1104', '1105', '204', '209', '206', '208', '207', '313', '349', '163', '113', '106', '305', '1106', '308', '307', '316', '309', '344', '314', '310', '326', '329', '343', '311', '301', '302', '325', '317', '319', '126', '303', '318', '707', '1601', '304', '901', '703', '704', '1107', '709', '710', '1702', '1701', '1501', '508', '523', '614', '507', '509', '519', '516', '617', '1617', '1618', '1619', '1620', '1502', '521', '528', '504', '1401', '520', '525', '527', '518', '1503', '1402', '1403', '426', '506', '405', '424', '425', '515', '1405', '413', '511', '510', '1603', '1602', '1604', '1605', '1607', '1609', '1608', '1606', '1613', '1611', '1612', '1614', '1610', '1615', '1616', '619', '407')
  # ```(생략)```
  ```
  </div>
  </details>
  <br>

6. Swin_Transformer_Object_Detection/mmdet/datasets/__init__.py
- from .pid import PIDDataset 추가
- PIDDataset 추가
  <details>
  <summary>코드 접기/펼치기</summary>
  <div markdown="1">
  ```python
  from .pid import PIDDataset
  # ```(생략)```
  __all__ = [
    'PIDDataset',  
  # ```(생략)```
  ]
  ```
  </div>
  </details>
  <br>

***Model Training:*** 모델 학습 하이퍼파라미터 수정이 완료되면 아래와 같이 모델을 학습합니다. 학습은 Dgx-station V100 2장으로 ??시간 소요되었습니다.
```bash
python train.py --config [CONFIG_PTH] --work-dir [CHECKPOINTS_PTH]

python Swin_Transformer_Object_Detection/train.py --config Swin_Transformer_Object_Detection/configs/swin/cascade_mask_rcnn_swin_base_patch4_window7_mstrain_480-800_giou_4conv1f_adamw_3x_pid.py --work-dir ~/model/symbol_model
```
* CONFIG_PTH: configuration 파일
* CHECKPOINTS_PTH: 학습된 모델이 저장될 경로

### 모델 테스트 및 성능 평가
모델 평가를 위해서 아래와 같이 추론합니다.
```bash
python Swin_Transformer_Object_Detection/symbol_detector.py --prj [PROJECT_NAME]

python Swin_Transformer_Object_Detection/symbol_detector.py --prj project
```
* PROJECT_NAME: 건설 프로젝트 이름

### 실험 결과

## 5. 한계점

본 실험의 한계점과 향후 방향 설명

> **"심볼 인식 2편"**에서는 현재의 한계점을 해결하기 위한 준지도학습 방식 심볼 인식 내용을 소개합니다.