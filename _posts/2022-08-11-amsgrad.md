---
layout: single
title:  "On The Convergence of Adam and Beyond 논문 리뷰"
categories : paper
tag : [논문리뷰, 논문 읽기 모임, optimizer, amsgrad]
toc: true
toc_sticky: true
---

![header](https://capsule-render.vercel.app/api?type=waving&color=a2dcec&height=300&section=header&text=On The Convergence of Adam and Beyond 논문 리뷰&fontSize=30&animation=fadeIn&fontAlignY=38&fontColor=FFFFFF)

[On The Convergence of Adam and Beyond 논문](https://arxiv.org/pdf/1904.09237.pdf)

&nbsp;

## 1. 사전에 알고 가야하는 Optimizer

&nbsp;

### 1-1. SGD

- **장점**
  - 전체 data를 가지고 한 번에 loss function을 계산하는 것이 아닌 전체 데이터의 일부만으로 loss function을 계산함
  - loss function을 여러번 빠르게 계산이 가능함



- **단점**
  - loss function의 최솟값을 찾기 위해 자주 step의 방향을 바꿔야 해서 학습이 불안정함
  - 순간순간의 기울기에 따라 방향이 결정되어서 상황에 따라서 비효율적인 Local minimum 수렴



![image-20220811153845105](/images/2022-08-11-amsgrad/image-20220811153845105.png)



&nbsp;

### 1-2. Momentum

- **장점**
  - 현재 Gradient를 통해 이동하는 방향과 별개로 과거에 이동했던 방식을 기억하면서 그 방향으로 일정량을 주기적으로 이동하는 방식
  - local minimum에 빠지지 않게 해줌
  - 경사 하강법 보다 빠른 속도로 진행
  - 기울기와 현재의 속도를 고려해서 다음의 방향을 지정함
  - SGD의 local minimum 취약점 보안



- 단점
  - 현재와 과거의 방향을 모두 기억해야 해서 메모리 소비가 큼



![image-20220811154101868](/images/2022-08-11-amsgrad/image-20220811154101868.png)



&nbsp;

### 1-3. NAG

- **장점**
  - Momentum  최적화의 변형으로 기본 Momentum 최적화 보다 더 빠른 성능을 보여줌
  - 현재의 속도로 먼저 이동한 후에 Gradient를 고려해서 다음 이동방향을 지정
  - Momentum의 속도를 이용해서 빠른 이동이라는 장점과 안정성을 가짐



![image-20220811154245756](/images/2022-08-11-amsgrad/image-20220811154245756.png)



&nbsp;

### 1-4. AdaGrad

> 학습률이 너무 작으면 학습 시간이 너무 길고, 너무 크면 발산해서 학습이 제대로 이루어지지 않는 문제를 
>
> 해결하기 위해서 나온 Optimizer



- **장점**
  - Gradient 변화가 큰 Parameter는 최적점에 가까워졌다고 판단해서 점차적으로 가중치 낮춤
  - Gradient 변화가 작은 Parameter는 최적점에 멀리 있다고 판단해서 점차적으로 가중치 높임
  - 과거의 Gradient 변화량을 참고해서 각각의 매개변수에 맞춤형 learning rate를 제공함

- **단점**
  - 반복이 계속되면서 G가 증가해 점차적으로 h가 너무 커져서 가중치가 너무 적어서 학습률 소실이 발생함
  - Global minimum에 도달하기 전에  h값이 너무 커져서 W 가 0에 수렴될 가능성이 있음



![image-20220811154722243](/images/2022-08-11-amsgrad/image-20220811154722243.png)



&nbsp;



### 1-5. RMSProp

> AdaGrad가 너무 빨리 느려져서 global minimum에 수렴하지 못하는 문제로 나온 Optimizer



- **장점**
  - AdaGrad와 반대로 모든 기울기를 고려하는 것이 아닌 가장 최근의 반복에서 계산된 Gradient만 누적해서 수렴 문제를 해소함
  - AdaGrad 보다 훨씬 더 좋은 성능을 보임
  - 지수 이동평균을 사용해서 최신 기울기들을 더 크게 반영해서 AdaGrad의 학습률이 0에 수렴되는 것을 막음



![image-20220811154910588](/images/2022-08-11-amsgrad/image-20220811154910588.png)



- p가 작을수록 가장 최신의 기울기 반영

- 누적 가중치에 큰 변동이 존재해도 새로운 batch의 기울기가 완만하면 학습률을 높임

- h가 p에 의해서 무한정으로 커지지 못함 



&nbsp;



### 1-6. AdaDelta

> RMSProp 와 유사하게 AdaGrad 단점을 보안, global learning rate를 자동으로 적절하게 설정하기 위해서 나옴



- **장점**
  - G를 계산할 때 합을 사용하지 않고 지수 이동평균을 사용함
  - 이전 Parameter와 이번의 Parameter의 차이인 갱신 값에 중점을 둠
  - 분모, 분자에 갱신 값인 delta가 존재하고 분자의 갱신 값은 0에 가깝게 시작해 점점 증가해서 학습률 높이고 부모의 갱신 값도 0에 가깝게 시작해서 점차 증가해서 학습률 제한함, 모두 이전 학습량과 gradient에 영향 받으면서 스스로 학습 step을 변화시킴



![image-20220811155602501](/images/2022-08-11-amsgrad/image-20220811155602501.png)





&nbsp;

### 1-7. ADAM 

> RMSProp 와 Momentum 을 합친 방식, Momentum 방식과 유사하게 기울기의 지수 평균을 저장하고 RMSProp와 유사하게 기울기의 제곱 값의 지수 평균을 저장



- **장점**
  - 두 개의  momentum m, v가 존재 m은 Gradient 값을 좀 더 빠르게 계산하도록 도와주는 역할을 수행
  - v는 분포가 희박한 곳에서 영향력을 극대화해서 빠르게 분포가 희박한 곳을 벗어나게 하는 역할 수행
  - Gradient가 커져도 step size는 고정되어서 안정적으로 최적화를 수행
  - 간단한 구현이 가능하고 효율적인 연산 수행이 가능하다.
  - AdaGrad, RMSProp, SGD에 비해 성능이 좋음



![image-20220811155735607](/images/2022-08-11-amsgrad/image-20220811155735607.png)



&nbsp;



### 1-8. NADAM

> ADAM + NAG, 기존의 ADAM이 현재 위치에서 Gradient를 계산하였는데 NADAM은 Momentum 방향으로 이동 후 새 위치에서 Gradient 계산



- **장점**
  - ADAM보다는 조금 더 빠르고 정확하게 최솟값을 찾을 수 있는 장점 존재
  - ADAM에 NAG를 합쳐서 momentum을 보안함 



![image-20220811155839971](/images/2022-08-11-amsgrad/image-20220811155839971.png)



&nbsp;



## 2. 초안에서 제시한 2가지의 중점

&nbsp;

**2-1. 지수 이동 평균을 사용한 알고리즘의 문제점**

- 최근에 심층 네트워크 훈련에 사용되는 RMSPROP, ADAM, ADADELTA, NADAM은 지수 이동 평균을 사용하는 알고리즘

- 위와 같은 알고리즘이 공간에 따라서 Global minimum에 수렴하지 못하는 것을 확인
- 수렴의 문제가 지수 이동 평균에 있고 이를 ADAM을 통해서 설명

&nbsp;

**2-2. 새롭게 제안하는 알고리즘 AMSGrad**

- ADAM의 문제점을 발견하였고 이를 해결하기 위해서 새롭게 알고리즘을 제안
- 지수 이동 평균 함수를 변형시켜서 문제를 해결함



&nbsp;

### **2-1.** **지수 이동 평균을 사용한 알고리즘의 문제점**



  ![(null)](/images/2022-08-11-amsgrad/cif00001.png)  



  ![(null)](/images/2022-08-11-amsgrad/cif00001-16602013291043.png)  

- 위에서 Algorithm 1 에서  SGD의 평균 함수가 사용

  ![(null)](/images/2022-08-11-amsgrad/cif00001-16602013434725.png)  

- 맨 처음에서 설명한 것처럼 SGD를 더 좋게 만든 ADAGRAD는 SGD와 다른 평균 함수 사용하여 더 좋은 성능을 보여줍니다.

&nbsp;



  ![(null)](/images/2022-08-11-amsgrad/DRWF354.png)  

- 논문에서 언급된 내용과 같이 좋은 성능은 평균 함수를 적절하게 사용하는 것이 중요하다는 것을 알려주고 있습니다.

- 이처럼 SGD 평균 함수를 사용한 Algorithms 1 을 ADAGRAD 평균함수로. 사용했을 때 더 좋은 성능을 보여주는 것을 통해서 평균 함수의 중요성을 알려주고 있습니다.



&nbsp;

- 지수 이동평균을 기반으로 하는 알고리즘

  ![(null)](/images/2022-08-11-amsgrad/cif00001-16602014189008.png)  

  ![(null)](/images/2022-08-11-amsgrad/cif00001-166020142456910.png)  

  - 논문에서 설명하는 주요한 차이는 ADAGRAD에서 사용하는 단순 평균 함수를 ADAM에서는 지수 이동 평균을 사용한다는 점입니다.



- ADAM을 예시로 사용하는 이유

  - 지수 이동 평균을 사용하는 optimizer는 RMSPROP,ADAM,NADAM,ADADELTA 등 다양한 것이 존재
  - 이 중에서 RMSPROP는 ADAM의 HyperParameter 중에 B1 = 0으로 했을 때 같은 알고리즘
  - NADAM 은 ADAM + NAG
  - ADADELTA 는 ADAM의 변형

  > 즉, RMSPROP, NADAM, ADADELTA 는 ADAM에서 파생되었기 때문에 ADAM으로 설명을 해도 각각의 알고리즘으로도 적용이 가능하기 때문에 본 논문에서 ADAM을 예시로 지수 이동 평균의 문제점을 설명합니다.



&nbsp;

  ![(null)](/images/2022-08-11-amsgrad/cif00001-166020150193812.png)  

- 위의 예시에서 C>2 일때 x=-1이 global minimum 임
- B1 = 0, B2=1/(1+C^2) 가정했을때 , ADAM 은 x=-1이 아닌 x = +1로 수렴이 됩니다.
- 위에서 3단계마다 한번씩  최적의 수렴지점인 x = -1로 이동하기위한 C(Gradient)를 얻지만 나머지 2단계에서 C(Gradient)는 주어진 B2 값에 따라서 지수이동 평균이 C의 계수만큼 축소 시켜서 결국 C(Gradient)가 상쇄되어서 x = -1로 수렴하지못하고 x= +1로 수렴이 됩니다.

> **이러한 수렴의 예제에서 지수 이동 평균이 Global minimum에 도달하지 못하는 것을 증명**

&nbsp;



### 2-2. 새롭게 제안하는 알고리즘 AMSGrad

- 저자가 소개하는 알고리즘은 ADAM과 RMSPROP의 장점을 유지하면서 수렴도 문제없이 보장하는 알고리즘

  ![(null)](/images/2022-08-11-amsgrad/cif00001-166020161429414.png)  

- 소개된 AMSGrad와 ADAM의 차이점 

  - ADAM 알고리즘에서 존재하는 2개의 Momentum(m,v) 중에서 B2 안에 존재하는 v momentum의 최댓값을 지수 이동 평균 대신에 평균 함수로써 사용하여 Gradient를 정규화 합니다.

  - ADAM은 특정 시간 t 에서 ![image-20220811160853186](/images/2022-08-11-amsgrad/image-20220811160853186.png) 일 때 ADAM 은 학습률을 크게 증가시키고 이 결과는 전체의 성능에 안 좋은 영향을 주고 이와 같이 앞서 설명했던 ADAGrad 역시 누적된 Gradient로 인해서 학습률을 크게 저하시켜서 전체 성능을 저하시킴

  - 이러한 문제점과 반대로 AMSGrad는 학습률을 감소시키거나 증가시키지 않아서 안정적인 학습 가능



&nbsp;

- ADAM의 비수렴 문제를 위한 두번째 예시

  ![image-20220811161018931](/images/2022-08-11-amsgrad/image-20220811161018931.png)

  - HyperParameter를 b1 = 0.9 , b2 = 0.99 로 일반적으로 ADAM에서 사용하는 설정으로 ADAM,AMSGrad 모두 설정하고 수행함.

  - 결과적으로 ADAM은 x = +1로 수렴하고 AMSGrad는 Global Minimum인 x=-1로 수렴

    

![image-20220811161059000](/images/2022-08-11-amsgrad/image-20220811161059000.png)

- 앞에서 소개된 1차원 ADAM 수렴 문제를 ADAM과 AMSGrad로 수행했을 때 수렴의 차이를 보여주는 그래프입니다.

![image-20220811161128510](/images/2022-08-11-amsgrad/image-20220811161128510.png)

- 위 그래프는 로지스틱 회귀를 수행했을 때 ADAM과 AMSGrad의 결과입니다.



&nbsp;



![image-20220811161201652](/images/2022-08-11-amsgrad/image-20220811161201652.png)

- feedforward neural network로 수행한 ADAM, AMSGrad 성능

  ![(null)](/images/2022-08-11-amsgrad/cif00001-166020193667518.png)  

- CIFARNET으로 수행했을 때 ADAM과 AMSGrad의 loss



> 위의 검증을 통해서 AMSGrad 가 조금더 좋은 성능을 기록합니다.



## 3. AMSGrad 사용 및 현재

&nbsp;

### AMSGrad 사용법

  ![(null)](/images/2022-08-11-amsgrad/DRW9AD6.png)  

- amsgrad = True 로 사용이 가능



### **AMSGrad** 현재

- 논문에서 ADAM과 비교한 데이터 셋인 CIFAR-10 데이터 셋에서는 AMSGrad가 ADAM 보다 뛰어난 성능을 보여주었지만 다른 데이터 셋에서는 ADAM과 비슷한 성능을 보여주거나 훨씬 더 안 좋은 성능을 보여주어서 결국에는 ADAM을 대체 하지는 못했습니다.