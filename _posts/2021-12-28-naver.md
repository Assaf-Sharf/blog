---
layout: single
title:  "네이버 블로그 크롤링"
---

# 크롤링 연습 (아나콘다 + 주피터 환경)

### Step 1. 필요 모듈 및 라이브러리 세팅  

### import
    1. 내장 모듈 로딩
    - sys 모듈 : 파이썬 인터프리터가 제공하는 변수와 함수를 직접 제어할 수 있게 해주는 모듈  
        > sys.path : 파이썬 모듈이 저장 위치(여기 있는 모듈들은 경로 상관없이 로딩 가능)  
        > sys.exit() : 강제로 스크립트 종료
    - os 모듈 : 환경 변수나 디렉토리, 파일 등을 제어할 수 있게 해주는 모듈  
        > os.chdir("C:\WINDOWS") : 현재 자신의 디렉토리 위치를 WINDOWS로 바꿔준다  
        > os.getcwd() : 현재 자신의 디렉토리 위치를 호출한다.
    - time 모듈 : 시간 관련 모듈
        > time.time() : 현재 시간을 실수 형태로 돌려준다   
        > time.localtime() : 실수값을 연, 월, 일, 분, 초.. 의 형태로 돌려준다  
        > time.strtime('출력할 포맷', time.localtime(time.time()))  
        > time.sleep() : 일정한 시간 간격을 두고 루프를 실행  
    - warinis 모듈 : 경고 메시지를 출력하거나 걸러내는 기능

    2. 아나콘다 기본 라이브러리 로딩 (아나콘다에는 자주 쓰이는 라이브러리가 탑재되어 있다)
        > pandas : 데이터 생성, 로딩, 구조화 등 데이터 분석에 가장 많이 활용
        > numpy : 환경 변수나 디렉토리, 파일 등을 제어 
        > tqdm : for문을 돌릴 때 진행률(%)을 표시해준다. 

### pip install
    > bs4 : html을 처리해서 가져올 수 있는 데이터 형태로 변환해주는 라이브러리 
    > selenium : 브라우저 자동화 라이브러리(페이지 이동 등). 데이터 수집 자동화에도 많이 활용 
                 (주로 크롬드라이버와 함께 쓰인다 


```python
import sys 
import os 
import time   
import warnings
warnings.filterwarnings('ignore') 

import pandas as pd 
import numpy as np  

from bs4 import BeautifulSoup   
from selenium import webdriver 
from tqdm import tqdm_notebook  
```


```python
# python 버전 확인
!python --version
```


```python
# 판다스 버전 확인
pd.__version__
```


```python
# 검색어, 검색 기간 정의
query_txt = input('1.크롤링할 키워드는 무엇입니까?: ')
```

# 1. 크롤링할 블로그 url 수집하기


```python
pwd # 현재 경로 불러오기
```

현재 경로에 chromedriver.exe 옮겨놓기!


```python
#Step 1. 크롬 웹브라우저 실행
path = os.path.join(os.getcwd(),"chromedriver.exe")  
    # join, getcwd로  현재 경로에 있는 chromedriver.exe를 실행

driver = webdriver.Chrome(path)
driver.get('http://www.naver.com')
time.sleep(2)  # 2초간 정지
```


```python
# Step 2. 네이버 검색창에 "검색어" 검색
element = driver.find_element_by_id("query")
element.send_keys(query_txt)  # query_txt는 위에서 입력한 키워드
element.submit()
time.sleep(1)
```


```python
# 'VIEW' 클릭
driver.find_element_by_link_text("VIEW").click( )
time.sleep(1)
```


```python
# '블로그' 클릭
driver.find_element_by_link_text("블로그").click( )
time.sleep(1)

# '옵션' 클릭
driver.find_element_by_link_text("옵션").click( )
time.sleep(1)
```


```python
# 검색옵션 확인
item_li = driver.find_elements_by_css_selector('.option .txt')

for i in range(0, len(item_li)):
    print(item_li[i].text)
```


```python
# 검색기간 '3개월' 클릭
print(item_li[10].text)

item_li[10].click()
```


```python
# 스크롤 다운
# driver.execute_script("window.scrollTo(0, 500)")
# time.sleep(2)

# 스크롤을 밑으로 내려주는 함수
def scroll_down(driver):
    driver.execute_script("window.scrollTo(0, 19431049)")
    time.sleep(1)

# n: 스크롤할 횟수 설정
n = 5
i = 0
while i < n: # 이 조건이 만족되는 동안 반복 실행
    scroll_down(driver) # 스크롤 다운
    i = i+1
```


```python
# 블로그 글 url들 수집
url_list = []
title_list = []

# URL_raw 크롤링 시작
articles = ".api_txt_lines.total_tit"
article_raw = driver.find_elements_by_css_selector(articles)
article_raw[0]
```


```python
article_raw[0].get_attribute('href')
```


```python
# 크롤링한 url 정제 시작
for article in article_raw:
    url = article.get_attribute('href')
    url_list.append(url)
time.sleep(1)
    
# 제목 크롤링 시작    
for article in article_raw:
    title = article.text
    title_list.append(title)

    print(title)

print("")
print('url갯수: ', len(url_list))
print('title갯수: ', len(title_list))
```


```python
url_list
```


```python
title_list
```


```python
df = pd.DataFrame({'url':url_list, 'title':title_list})
df
```


```python
pwd
```


```python
# 저장하기
df.to_excel("blog_url.xlsx")
```

# 2. 블로그 내용 크롤링하기


```python
import sys
import os
import pandas as pd
import numpy as np
```


```python
# "url_list.csv" 불러오기
url_load = pd.read_excel("blog_url.xlsx") # 기본 모델

num_list = len(url_load)

print(num_list)
url_load
```


```python
dict = {}    # 전체 크롤링 데이터를 담을 그릇

# 수집할 글 갯수 정하기
number = 10  

# 수집한 url 돌면서 데이터 수집
for i in tqdm_notebook(range(0, number)):
    # 글 띄우기
    url = url_load['url'][i]
    driver = webdriver.Chrome(path)  # 윈도우는 "chromedriver.exe"
    driver.get(url)   # 글 띄우기
    
    # 크롤링
    
    try : 
        # iframe 접근
        driver.switch_to.frame('mainFrame')

        target_info = {}  # 개별 블로그 내용을 담을 딕셔너리 생성

        # 제목 크롤링 시작
        overlays = ".se-module.se-module-text.se-title-text"                                 
        tit = driver.find_element_by_css_selector(overlays)          # title
        title = tit.text

        # 글쓴이 크롤링 시작
        overlays = ".nick"                                 
        nick = driver.find_element_by_css_selector(overlays)         # nickname
        nickname = nick.text

        # 날짜 크롤링
        overlays = ".se_publishDate.pcol2"                                 
        date = driver.find_element_by_css_selector(overlays)         # datetime
        datetime = date.text

        # 내용 크롤링
        overlays = ".se-component.se-text.se-l-default"                                 
        contents = driver.find_elements_by_css_selector(overlays)    # contents

        content_list = []
        for content in contents:
            content_list.append(content.text)
 
        content_str = ' '.join(content_list)                         # content_str

        # 글 하나는 target_info라는 딕셔너리에 담기게 되고,
        target_info['title'] = title
        target_info['nickname'] = nickname
        target_info['datetime'] = datetime
        target_info['content'] = content_str

        # 각각의 글은 dict라는 딕셔너리에 담기게 됩니다.
        dict[i] = target_info
        time.sleep(1)
        
        # 크롤링이 성공하면 글 제목을 출력하게 되고,
        print(i, title)

        # 글 하나 크롤링 후 크롬 창을 닫습니다.
        driver.close()       
    
    # 에러나면 현재 크롬창을 닫고 다음 글(i+1)로 이동합니다.
    except:
        driver.close()
        time.sleep(1)
        continue
    
    # 중간,중간에 파일로 저장하기
    if i == 30 or 50 or 80:
        # 판다스로 만들기
        import pandas as pd
        result_df = pd.DataFrame.from_dict(dict, 'index')

        # 저장하기
        result_df.to_excel("blog_content.xlsx", encoding='utf-8-sig')
        time.sleep(3)

print('수집한 글 갯수: ', len(dict))
print(dict)
```


```python
dict
```


```python
# 판다스로 만들기
import pandas as pd
result_df = pd.DataFrame.from_dict(dict, 'index')
result_df
```


```python
# 엑셀로 저장하기
result_df.to_excel("blog_content.xlsx", encoding='utf-8-sig')
```


```python
pwd
```
