---
layout: single
title: "딥러닝과 NLP(1) - 개념 and 토큰화"
categories: 머신러닝,딥러닝
tag: [python, 문제, blog, github, 파이썬, 알고리즘, 머신러닝, 딥러닝,  모두의, sw 텐서플로, 케라스, 개념, tensorflow, keras]
toc: true
sidebar:
  nav: "docs"
---
## 1. 딥러닝이 자연어 처리(NLP)에 가장 효과적인 이유

[참고영상 Appier 유튜브 영상](https://www.youtube.com/watch?v=gUMvBRI-WGo)


```python
### 기존 NLP vs 딥러닝 NLP
```


```python
기존 NLP는 모든 단어를 독립적인 토큰으로 인지함
그에 반해, 딥러닝 NLP는 각 단어들을 하나의 벡터공간으로 매핑   
- 기존 NLP가 단어를 하나씩 사전을 찾아가며 공부하는 거라면, 
  딥러닝 NLP 는 많은 양의 텍스트를 한꺼번에 읽으면서 습득하는 것과 비슷 (학습하는 방식이 더 자연스러워짐)
```

### 키워드 기반 마케팅에 적용?


```python
예전에는 마케팅 담당자가 타겟 키워드를 일일이 나열해 방대한 목록을 만들었지만, 
이젠 AI에 몇 가지 씨앗(seeds)을 입력하면, 각 씨앗이 벡터 공간에서
유사한 키워드를 찾아내고 자동으로 활용하게 됨
특히, 단어벡터를 활용해 긍정과 부정을 분류해둔다면 정서(감정) 분석 측면에 매우 효과적
```

### NLP의 미래?


```python
아직 텍스트 제어가 어렵다. 특히 문장이 길어지면 품질이 현저하게 떨어짐
이 문제 해결을 수많은 과학자들이 노력중이며, 관련 툴들이 발전하여   
생성된 콘텐츠의 테마, 스타일, 길이 등에서 제어가 용이해지며 NLP를 활용해
문서를 분석하고 작성하는 것이 훨씬 쉽고 빨라질 것으로 전망
```

## 2. 토큰화, 코드로 이해하기

[코드출처 : 모두의 딥러닝](https://book.interpark.com/product/BookDisplay.do?_method=detail&sc.prdNo=327029899&gclid=CjwKCAiAsNKQBhAPEiwAB-I5zWB_M4jMKGFqiFNOYSAWhJOcJgWe5wkpqEx14EZLGYeWz09_rx4L5RoCX7EQAvD_BwE)


```python
import numpy as np
from numpy import array
from tensorflow.keras.preprocessing.text import Tokenizer # 토큰화
from tensorflow.keras.preprocessing.sequence import pad_sequences # 케라스 전처리 도구 for 패딩
from tensorflow.keras.models import Sequential # 레이어를 연결
from tensorflow.keras.layers import Dense,Flatten,Embedding # 레이어 설정, 1차원화, word2vect
```

### 문장 -> 단어 (토큰화)


```python
# 텍스트 전처리 함수 import
from tensorflow.keras.preprocessing.text import text_to_word_sequence
text = '해보지 않으면 해낼 수 없다' # 전처리할 텍스트
# 토큰화 
result = text_to_word_sequence(text)
print('원문: ', text)
print('토큰화: ', result)
```

    원문:  해보지 않으면 해낼 수 없다
    토큰화:  ['해보지', '않으면', '해낼', '수', '없다']



```python
# 각 단어의 빈도수 세기 

# 토큰화 함수 import
from tensorflow.keras.preprocessing.text import Tokenizer
# 세 개의 문장 정의 
docs = ['먼저 텍스트의 각 단어를 나누어 토큰화 합니다.',
       '텍스트의 단어로 토큰화 해야 딥러닝에서 토큰화 인식됩니다.',
       '토큰화 한 결과는 딥러닝에서 사용 할 수 있습니다.',]

token = Tokenizer() # 토큰화 함수 지정
token.fit_on_texts(docs) # 문장에 적용

# word_counts 는 순서를 기억하는 OrderedDict 클래스를 사용 
print('단어카운트: \n', token.word_counts)
```

    단어카운트: 
     OrderedDict([('먼저', 1), ('텍스트의', 2), ('각', 1), ('단어를', 1), ('나누어', 1), ('토큰화', 4), ('합니다', 1), ('단어로', 1), ('해야', 1), ('딥러닝에서', 2), ('인식됩니다', 1), ('한', 1), ('결과는', 1), ('사용', 1), ('할', 1), ('수', 1), ('있습니다', 1)])



```python
# 문장카운트 : .document_count
print("문장 카운트: ", token.document_count)
```

    문장 카운트:  3



```python
# 문장에 각 단어가 몇개씩 있지? .word_docs
print('문장 안에 각 단어가 몇개씩 있지? \n', token.word_docs)
```

    문장 안에 각 단어가 몇개씩 있지?
    defaultdict(<class 'int'>, {'먼저': 1, '각': 1, '단어를': 1, '토큰화': 3, '텍스트의': 2, '나누어': 1, '합니다': 1, '해야': 1, '딥러닝에서': 2, '인식됩니다': 1, '단어로': 1, '한': 1, '결과는': 1, '사용': 1, '수': 1, '할': 1, '있습니다': 1})



```python
# 각 단어에 부여된 인덱스 값은? .word_index
print('각 단어에 부여된 인덱스 값: \n', token.word_index) 
```

    각 단어에 부여된 인덱스 값: 
     {'토큰화': 1, '텍스트의': 2, '딥러닝에서': 3, '먼저': 4, '각': 5, '단어를': 6, '나누어': 7, '합니다': 8, '단어로': 9, '해야': 10, '인식됩니다': 11, '한': 12, '결과는': 13, '사용': 14, '할': 15, '수': 16, '있습니다': 17}
