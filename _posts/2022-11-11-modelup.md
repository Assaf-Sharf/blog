---
layout: single
title:  "[DL/고급] 하이퍼파라미터와 앙상블 - 모델의 성능 끌어올리기 🚀"
toc: true
toc_sticky: true
categories:
  - advanced
---
## 1. 모델의 최대 성능 끌어내기 (KerasTuner, 모델 앙상블)

### 1.0 들어가며 
단지 작동만 하는 모델을 구축하려면 그냥 많은 시도를 해보며 모델을 만들어봐도 충분하지만, 더 뛰어난 성능의 모델을 만들려면 많은 기법이 필요하다. 이번 글에서는 모델이 그냥 작동하는 수준을 넘어서 머신러닝 경연 대회에서 우승할 수 있는 정도의 성능의 모델을 구축하는 기법들에 대해서 다룰 것이다. 

### 1.1 하이퍼파라미터 최적화

#### 1.1.1 하이퍼파라미터 최적화란?
딥러닝 모델을 만들 때는 상당히 많은 결정을 해야한다. 예를 들어 '얼마나 많은 층을 쌓을까?' , '층마다 얼마나 많은 유닛이나 필터를 두어야 할까?' , 'relu 함수를 사용해야 하나? 아니면 다를 함수를 사용해야 하나?' 등의 질문들에 대한 결정을 해야한다. 이런 구조에 관련된 파라미터를 역전파로 훈련되는 모델 파라미터와 구분하여 **하이퍼파라미터**라고 부른다. 하이퍼파라미터 튜닝에 대한 공식적인 규칙은 없다. 경험이 많은 딥러닝 엔지니어와 연구자들은 하이퍼파라미터에 대한 직관을 가지고 있지만 이들조차도 하이퍼파라미터 튜닝에 대부분의 시간을 투자한다. 하지만 하루 종일 하이퍼파라미터를 수정하는 것은 사람이 할 일이 아니다. 이는 기계에 위임하는 것이 더 낫다.

가능한 결정 공간을 자동적, 조직적, 규칙적인 방법으로 탐색해야 한다. 가능성 있는 구조를 탐색해서 실제 가장 높은 성능을 내는 구조를 찾아야 한다. 하이퍼파라미터 자동 최적화가 이에 관련된 분야이다. 이는 하나의 중요한 연구 분야이다.

전형적인 하이퍼파라미터 최적화 과정은 다음과 같다.

1. 일련의 하이퍼파라미터를 (자동으로) 선택한다.
2. 선택된 하이퍼파라미터로 모델을 구축한다.
3. 훈련 데이터에 학습하고 검증 데이터로 성능을 측정한다.
4. 다음으로 시도할 하이퍼파라미터를 (자동으로) 선택한다.
5. 위의 과정을 반복한다.
6. 마지막으로 테스트 데이터로 성능을 측정한다.

검증 성능과 다양한 하이퍼파라미터 사이의 관계를 분석해서 다음 번에 시도할 하이퍼파라미터를 선택하는 알고리즘이 이 과정의 핵심이다. 이 과정에서 **베이즈 최적화**, **유전 알고리즘**, **간단한 랜덤 탐색** 등의 여러가지 기법을 사용할 수 있다. 모델의 가중치(파라미터)를 훈련하는 것과 달리 하이퍼파라미터를 업데이트 하는 것은 다음의 이유들 때문에 쉽지 않은 과정이다.

* 하이퍼파라미터 공간은 연속적이지 않고, 미분가능하지 않기 때문에 경사 하강법을 사용할 수 없고, 대신 훨씬 비효율적인 그레디언트-프리 최적화 기법을 사용해야 한다.
* 최적화 과정의 피드백 신호를 계산하는 것은 새로운 모델을 만들고 처음부터 다시 훈련해야하기 때문에 매우 비용이 많이 든다.
* 피드백 신호는 잡음이 많을 수 있다. 어떤 훈련이 0.2%의 성능을 높였다면 더 좋은 모델 설정 때문일까? 아니면 초기 가중치 값이 우연히 좋았던 것일까?

다행히 하이퍼파라미터 튜닝을 쉽게 수행할 수 있는 도구인 KerasTuner가 있다.

#### 1.1.2 KerasTuner 사용하기
먼저 KerasTuner를 설치한다.


```python
!pip install keras-tuner -q
```

    [K     |████████████████████████████████| 135 kB 14.0 MB/s 
    [K     |████████████████████████████████| 1.6 MB 55.9 MB/s 
    [?25h

KerasTuner를 사용하면 units=32와 같은 하드코딩된 하이퍼파라미터 값을 Int(name='units=32', min_value=16, max_value=64, step=16)과 같이 가능한 선택 범위로 바꿀 수 있다. 어떤 모델에 대한 이런 선택의 집합을 하이퍼파라미터 튜닝 과정의 **탐색 공간(search space)**이라고 부른다.

탐색 공간을 지정하기 위해 모델 구축 함수를 정의한다. 이 함수는 하이퍼파라미터 범위를 샘플링할 수 있는 hp 매개변수를 받고 컴파일된 케라스 모델을 반환한다.


```python
from tensorflow import keras
from tensorflow.keras import layers

def build_model(hp):
  # hp 객체에서 하이퍼파라미터 값을 샘플링, 샘플링한 이 값(units)은 일반적인 파이썬 상수
  units = hp.Int(name='units', min_value=16, max_value=64, step=16)

  model = keras.Sequential([
      layers.Dense(units, activation='relu'),
      layers.Dense(10, activation='softmax')
  ])
  # Int,Float,Boolean,Choice 등 여러 종류의 하이퍼파라미터를 제공
  optimizer = hp.Choice(name='optimizer', values=['rmsprop','adam'])

  model.compile(
      optimizer = optimizer,
      loss = 'sparse_categorical_crossentropy',
      metrics = ['accuracy'])
  return model # 컴파일된 모델 반환
```

HyerModel 클래스를 상속하고 build 매서드를 정의하면 모델 구축을 조금 더 모듈화하고 설정하기 쉽게 만들 수 있다.


```python
from keras_tuner.engine import hypermodel
import keras_tuner as kt

class SimpleMLP(kt.HyperModel):
  def __init__(self, num_classes):
    self.num_classes = num_classes

  def build(self,hp): # 위의 build_model() 함수와 동일
    units = hp.Int(name='units',min_value=16,max_value=64,step=16)
    model = keras.Sequential([
        layers.Dense(units,activation='relu'),
        layers.Dense(self.num_classes,activation='softmax')
    ])

    optimizer = hp.Choice(name='optimizer',values =['rmsprop','adam'])
    model.compile(
        optimizer = optimizer,
        loss = 'sparse_categorical_crossentropy',
        metrics = ['accuracy'])
    return model

hypermodel = SimpleMLP(num_classes=10)
```

다음 단계는 **'튜너(tuner)'**를 정의하는 것이다. 튜너를 다음 과정을 반복하는 for 루프로 생각하면 된다.

* 일련의 하이퍼파라미터 값을 선택한다.
* 이런 값으로 모델 구축 함수를 호출하여 모델을 만든다.
* 모델을 훈련하고 평가 결과를 기록한다.

KerasTuner는 RandomSearch, BayesianOptimization, Hyperband의 내장 튜너를 제공한다. 이전 선택의 결과를 바탕으로 최상의 하이퍼파라미터 값을 예측하는 BayesianOptimization 튜너를 사용해보겠다.


```python
tuner = kt.BayesianOptimization(
    build_model, # 모델 구축 함수 (or HyperModel 클래스 객체) 지정
    objective = 'val_accuracy', # 튜너가 최적화할 지표 지정, 항상 검증 지표 지정
    max_trials = 100, # 탐색을 끝내기 전까지 시도할 최대 횟수
    executions_per_trial = 2, # 각 모델 설정을 몇번씩 훈련해서 평균할지 설정
    directory = 'mnist_kt_test', # 탐색 로그 저장 위치
    overwrite = True # 새로운 탐색을 시작하기 위해 디렉터리의 데이터 덮어쓸지 여부
    # 모델 구축함수를 수정했다면 True, 동일한 모델 구축함수로 탐색 진행하면 False

 )
```


```python
#search_space_summary() 매서드로 탐색 공간의 요약 정보 출력
tuner.search_space_summary()
```

    Search space summary
    Default search space size: 2
    units (Int)
    {'default': None, 'conditions': [], 'min_value': 16, 'max_value': 64, 'step': 16, 'sampling': None}
    optimizer (Choice)
    {'default': 'rmsprop', 'conditions': [], 'values': ['rmsprop', 'adam'], 'ordered': False}


이제 탐색을 시작해보자. 검증 데이터를 전달하는 것을 잊지 말아야 하고, 테스트 세트를 검증에 사용해서는 안된다. 그렇지 않으면 테스트 데이터에 과대적합되기 시작해서 더 이상 테스트 결과를 신뢰할 수 없게 된다.


```python
(x_train, y_train),(x_test, y_test) = keras.datasets.mnist.load_data()
x_train = x_train.reshape((-1,28*28)).astype('float32') / 255
x_test = x_test.reshape((-1,28*28)).astype('float32') / 255
x_train_full = x_train[:]
y_train_full = y_train[:]

# 검증 세트 따로 떼어 놓기
num_val_samples = 10000
x_train, x_val = x_train[:-num_val_samples],x_train[-num_val_samples:]
y_train, y_val = y_train[:-num_val_samples],y_train[-num_val_samples:]

# 모델마다 에포크가 얼마나 필요한지 모르기 때문에 에포크를 크게 지정하고 EarlyStopping 콜백 사용
callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss',patience=5)]

tuner.search(
    x_train, y_train,
    batch_size=128,
    epochs=100,
    validation_data=(x_val, y_val),
    callbacks=callbacks,
    verbose=2,
)
```


```python
''' *결과 도출에 1시간 30분가량 걸리기 때문에 이후 코드들에서는 미리 도출된 결과값 사용*
결과 출처 https://github.com/rickiepark/deep-learning-with-python-2nd/blob/main/chapter13_best-practices-for-the-real-world.ipynb'''
Trial 100 Complete [00h 00m 42s]
val_accuracy: 0.9745000004768372

Best val_accuracy So Far: 0.9764499962329865
Total elapsed time: 01h 21m 04s
INFO:tensorflow:Oracle triggered exit
```

위의 하이퍼파라미터 튜닝은 선택 가능한 옵션도 적고 가벼운 MNIST 데이터로 훈련하기 때문에 적게 걸리는 편이다. 일반적인 탐색 공간과 데이터셋에서는 밤새 또는 며칠에 걸쳐 하이퍼파라미터 튜닝이 수행된다. 탐색 과정에 문제가 생기면 언제나 다시 시작할 수 있다. 튜너에 overwrite=False를 지정하면 디스크에 저장된 탐색 로그에서 이어서 탐색을 수행한다.

탐색이 끝나면 최상의 하이퍼파라미터 설정을 확인하고 이를 사용해서 최상의 하이퍼파라미터로 구성된 모델을 다시 훈련한다.


```python
top_n = 4
# 모델 구축 함수에 전달할 수 있는 HyerParameters 객체 리스트 반환
best_hps = tuner.get_best_hyperparameters(top_n) 
```

최상의 모델을 다시 훈련할 때는 검증 데이터로 성능 평가할 필요가 없기 때문에 훈련 데이터에 검증 데이터를 포함해서 훈련하는 것이 좋을 수 있다. 검증 세트를 따로 보관하지 않고 원본 MNIST 훈련 데이터를 모두 사용해서 최종 모델을 훈련해본다.

하지만 전체 훈련 데이터에서 훈련하기 전에 마지막으로 정할 파라미터가 하나 있다. 바로 훈련 에포크 횟수이다. 탐색할 때 EarlyStopping 콜백의 patience 값을 낮추면 비용을 절약할 수 있지만 최적의 모델이 아닐수도 있다. 최상의 에포크 횟수를 찾기 위해 검증 세트를 사용해보자.


```python
def get_best_epoch(hp):
  model = build_model(hp)
  callbacks = [
      keras.callbacks.EarlyStopping(
          monitor='val_loss',mode='min',patience=10) #patience 값 높게
  ]
  history = model.fit(
      x_train , y_train,
      validation_data = (x_val,y_val),
      epochs = 100
      batch_size = 128,
      callbacks = callbacks)
  val_loss_per_epoch = history.histoty['val_loss']
  best_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch)) + 1
  print(f'최상의 에포크: {best_epoch}')
  return best_epoch
```

마지막으로 더 많은 데이터에서 훈련하므로 전체 데이터셋에서 이 에포크 횟수보다 조금 더 오래 훈련한다.


```python
def get_best_trained_model(hp):
  best_epoch = get_best_epoch(hp)
  model.fit(
      x_train_full,y_train_full,
      batch_size=128, epochs = int(best_epoch*1.2)) # 20% 더 많은 데이터로 훈련
  return model

best_models = []
for hp in best_hps:
  model = get_best_trained_model(hp)
  model.evaluate(x_test,y_test)
  best_models.append(model)
```

#### 1.1.3 올바른 검색 공간을 만드는 기술
전체적으로 봤을 때 하이퍼파라미터 최적화는 어느 작업에서 최고의 모델을 얻거나 머신러닝 경연 대회에서 우승하기 위한 강력할 도구다. 하지만 하이퍼파라미터 튜닝으로 모델 아키텍처의 [모범 사례](https://hamin-chang.github.io/convarch/)를 대체할 수 없다. 모든 것을 하이퍼파라미터로 설정해서 튜너가 찾도록 하면 너무 많은 비용이 든다. 그렇기 때문에 올바른 탐색 공간을 설계할 필요가 있다. **하이퍼파라미터 튜닝은 자동화이지 마법이 아니다.** 

하이퍼파라미터 튜닝은 모델 설정에 대한 결정을 **미시적 결정**(층의 유닛 개수를 얼마로 해야할까?)에서 높은 수준의 **아키텍처 결정**(이 모델에 잔차 연결을 사용해야할까?)으로 바꿀 수 있다. 미시적 결정은 특정 모델이나 특정 데이터셋에 따라 다르지만 고수준 결정은 여러 작업과 데이터셋에 걸쳐 일반화가 더 잘된다. 예를 들어 거의 모든 이미지 분류 문제는 같은 종류의 탐색 공간 템플릿으로 풀 수 있다.

이런 논리를 따라 KerasTuner는 이미지 분류와 같이 넓은 범위를 가진 문제에 관해 **사전에 정의된 탐색 공간**을 제공한다. 데이터를 추가하고 탐색을 실행하면 꽤 높은 성능의 모델을 얻을 수 있다. 예를 들어 kt.applications.HyerXception과 kt.applications.HyperResNet등의 하이퍼 모델이 있다.

### 1.2 모델 앙상블
**모델 앙상블**은 가장 좋은 결과를 얻을 수 있는 또 다른 강력한 기법이다. 앙상블은 여러 개의 다른 모델의 예측을 합쳐서 더 좋은 예측을 만드는 기법인데, 캐글 같은 머신러닝 대회에서 우승자들이 많이 사용하는 기법이다.

앙상블은 독립적으로 훈련된 다른 종류의 잘 동작하는 모델이 각기 다른 장점을 가지고 있다는 가정을 바탕으로 한다. 각 모델은 각자의 가정(고유한 모델 구조와 랜덤 가중치 초기화)를 이용하고 각자의 관점으로 훈련 데이터의 매니폴드를 이해한다. 이들의 관점을 모으면 데이터를 훨씬 정확하게 묘사할 수 있다. 분류로 예를 들어본다. 분류기 예측을 앙상블하기 위해 합치는 가장 쉬운 방법은 추론할 때 나오는 예측의 평균을 내는 것이다.


```python
preds_a = model_a.predict(x_val)
preds_b = model_b.predict(x_val)
preds_c = model_c.predict(x_val)
preds_d = model_d.predict(x_val)
final_preds = 0.25 * (preds_a + preds_b + preds_c + preds_d) 
```

하지만 이 방식은 분류기들이 어느정도 비슷한 성능일 때 잘 작동한다. 분류기 중 하나가 다른 모델보다 월등히 나쁘면 최종 예측은 앙상블에 있는 가장 좋은 분류기만큼 좋지 않을 수 있다.

분류기를 앙상블하는 더 좋은 방법은 검증 데이터에서 학습된 가중치를 사용해서 가중 평균하는 것이다. 전형적으로 분류기가 좋을수록 높은 가중치를 가지고 나쁠수록 낮은 가중치를 가진다. 좋은 앙상블 가중치를 찾기 위해 **랜덤 서치**나 **넬더-미드** 알고리즘 같은 최적화 알고리즘을 사용할 수 있다. 이는 다른 글에서 다뤄보도록 하겠다.

다음과 같이 가중 평균을 내면 된다.


```python
preds_a = model_a.predict(x_val)
preds_b = model_b.predict(x_val)
preds_c = model_c.predict(x_val)
preds_d = model_d.predict(x_val)
final_preds = 0.5 * preds_a + 0.25 * preds_b + 0.1 * preds_c + 0.15 * preds_d
# 가중치 (0.5,0.25,0.1,0.15)는 알고리즘으로 학습된 가중치.
```

앙상블의 핵심은 분류기의 다양성이다. 모든 모델이 같은 방향을 편향되어 있다면 앙상블의 결과도 동일한 편향을 유지할 것이다. 모델들이 서로 다른 방향으로 편향되어 있다면 편향은 서로 상쇄돼고 앙상블이 더 견고하고 정확해진다. 

기회가 된다면 앙상블 모델에 대해서는 다른 글에서 다뤄보겠다.

[<케라스 창시자에게 배우는 딥러닝 개정 2판>(길벗, 2022)을 학습하고 개인 학습용으로 정리한 내용입니다.] 출처: 프랑소와 숄레 지음, ⌜케라스 창시자에게 배우는 딥러닝 개정2판⌟, 박해선 옮김, 길벗, 2022 도서보기: https://www.gilbut.co.kr/book/view?bookcode=BN003496
