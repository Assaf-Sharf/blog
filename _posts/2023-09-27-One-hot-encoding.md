---
layout: single
title: One hot encoding
category: deeplearning
tag: [deeplearning , Python, Pytorch , one hot encoding]
---

이번에서는 One - hot encoding이라는 것을 다루어 볼 겁니다.

one-hot encoding의 단어만 보고 해당 단어를 해석하면

단 1개의 핫한 것만 인코딩 한다는 의미가 됩니다.

사실 정확한 의미도 비슷합니다(?) 

선택해야 하는 선택지의 개수 만큼의 차원을 가짐과 동시에, 각 선택지의 인덱스에 해당하는 원소는 1 나머지는 0을 부여하는 방법입니다

**???:도대체 무슨 소리임?**

사실 말로만 하면 잘 이해가 안될 수 있습니다.

저도 처음 말로만 들었을 때는 여러분들과 똑같은 생각을 했습니다.

![Untitled](https://github.com/jusunglee-ai/jusunglee-ai.github.io/assets/125032849/8d6fb07e-1bb2-4e34-ad5f-36ceef56bd84)

그래서 예시 자료를 가져왔습니다.

위의 자료를 바탕으로 한번 예시를 가져볼게요.

만약 우리에게 어떤 사진이 있어요 근데 이 사진은 a, b ,c 이 3가지 중 하나를 나타내는 사진 입니다. 그리고 이 사진을 보고 이 사진이 a, b ,c 중에서 어떤 것을 나타내고 의미하는지 맞춰 보라는 문제가 주어졌습니다.

그럼 우리에게 주어지는 선택지는 3가지가 될 겁니다.

그리고 해당하는 알파벳은 1로 표시 해당되지 않은 알파벳은 0으로 표시하라는 문제가 주어졌습니다.

그럼 만약 해당 사진이 알파벳 a를 나타내는 사진이라면 a에 1을 주고 나머지는 0을 주게 됩니다. 그럼 [1,0,0]으로 정답지를 제출 할 수 있겠죠.

바로 이 원리가 one-hot encoding입니다.

정답인 부분만 1 나머지는 0으로 처리하며 선택지의 개수만큼 차원을 가진다 라는 부분을 나타내는 것이 [1,0,0]입니다. 선택지가 3개이기 때문에 3차원의 벡터가 되었으며 인덱스에 해당하는 원소 즉 0번 인덱스(a)에만 1을 부여하고 0을 부여했습니다.

한마디로 정답 선택지의 개수만큼 차원을 가지며 정답인 원소에만 1 나머지는 0을 부여한다는 의미입니다.

---

### 그럼 one-hot encoding이라는걸 왜 하는 거임?

---

one-hot encoding은 주로 다중 클래스 분류 문제에서 많이 사용이 됩니다.

물론 다중 클래스 분류 문제를 one-hot encoding을 무조건 써야만 해결하는 것은 아닙니다.

하지만 각 클래스 간의 관계가 균등하다는 이유가 있기 때문에 one-hot encoding을 쓰는 것이 다중 클래스 분류 문제에서 사용하기 좋습니다.

그럼 그 관계가 균등하다는 것이 도대체 무슨 의미를 이야기하는 건지 알아 보도록 합시다.

우리가 처음 예시로 들었던 a, b ,c 3개의 문제를 다시 예시로 들어보도록 합시다.

a, b ,c에 정수 인코딩을 통해 인덱스를 부여 해봅시다.

그럼 a=1, b=2, c=3로 부여해서 [a,b,c]=[1,2,3]으로 표현이 가능합니다.

만약 이 정수 인코딩을 통한 인덱스를 통해 loss를 구하게 된다면 어떤 현상이 발생하는지 알아보도록 합시다.

 

loss함수는 기존의 mse함수를 쓰도록 하겠습니다.

대신 여기서 평균을 구하지는 않고 제곱 오차(실제값-예측값)^2 부분만 따로 쓰도록 하겠습니다.

![Untitled 1](https://github.com/jusunglee-ai/jusunglee-ai.github.io/assets/125032849/154d21c0-7a07-443d-ba54-cf4d4b0cd4c3)

그럼 만약 정답이 3번 인덱스 c였는데 모델이 1번 인덱스인 a를 골랐다고 해봅시다.

그럼 (3-1)^2=4가 될 것이고

만약 정답이 3번 인덱스인 c였는데 모델이 2번 인덱스인 b를 고른 경우에는 (3-2)^2=1이 될 겁니다.

그럼 이 결과를 보게 된다면 여러분들은 감이 어느 정도 오실 수 있습니다.

결국에는 위의 2가지의 경우 둘 다 완전히 다른 예측을 해서 틀렸는데 1번 경우보다 2번 경우의 loss가 더 작네? 이렇게 된다면 모델에게 b가 c에 더 적합하고 가깝다는 정보를 제공하는 것과 마찬가지며 이를 바탕으로 loss를 개선하게 되면 전혀 다른 예측을 한다는 의미가 아닌가?

이번에는 3가지의 경우가 아니라 5가지의 경우를 들어봅시다.

그리고 알파벳으로 하면 감이 잘 오지 않으니 여러분들이 좋아하시는 동물로 예시를 들어보도록 합시다.

[강아지,고양이,토끼,새,곰] 이렇게 5개의 선택지가 있다고 가정합시다.

그리고 이를 [1,2,3,4,5]로 정수 인코딩을 통해 인코딩을 했습니다.

그럼 만약 실제 그림이 곰(5번)인데 모델이 강아지(1번)을 예측 했다고 가정합시다.

그렇게 된다면 평균 오차는 (5-1)^2=16이 될 것이며, 실제 그림이 곰(5번)인데 모델이 이번에는 새(4번)을 예측했다고 가정해봅시다.

이 경우의 제곱 오차는 (5-4)^2=1이 됩니다.

제곱 오차 16과 1의 차이는 엄청난 차이이며 이는 모델이 새와 곰은 비슷한 동물(데이터)구나 라고 판단을 할 수 있다는 것 입니다.

또한 곰은 강아지보다는 새에 가까운 동물이다. 라는 의미를 담게 되고 이러한 정보를 가지고 데이터를 예측하고 판단하게 됩니다.

하지만 이런 다중 클래스 분류에서 데이터 순서는 아무 의미가 없으며 데이터간의 오차는 모두 균등해야 옳습니다.

그렇기 때문에 우리는 이러한 다중 클래스 분류 문제에서 one-hot encoding을 통해서 데이터 순서에 상관 없이 각각의 모든 클래스 간의 차이를 균등하게 두기 위해서 one-hot encoding을 사용합니다.