---
layout: single
title:  "USAD 논문 리뷰"
categories : paper
tag : [USAD, 논문리뷰, 딥러닝]
toc: true
toc_sticky: true
---

![header](https://capsule-render.vercel.app/api?type=waving&color=a2dcec&height=300&section=header&text=USAD 논문 리뷰&fontSize=40&animation=fadeIn&fontAlignY=38&desc=최재혁%20GitHub.io&descAlignY=51&descAlign=62&fontColor=FFFFFF)



USAD 논문 : [**USAD**](https://dl.acm.org/doi/pdf/10.1145/3394486.3403392)



## Introduction

- AE-based & GAN-based anomaly detection 모델에 한계점이 존재
    - AE-based AD : AE는 정상 데이터를 잘 복원하도록 학습되기 때문에 이상치가 정상 데이터와 유사하면  reconstruction error가 작아 이상치로 탐지되지 않음
        - **LSTM-AE**
            - Training : 정상 데이터의 reconstruction error를 기반으로 LSTM-AE를 학습하여 정상 데이터의 분포를 학습함
            - Anomaly detection : 학습이 완료된 LSTM-AE를 기반으로 도출한 새로운 input의 reconstruction error가 threshold를 초과하면 이상치로 탐지함
            
              ![Untitled](/paper/images/2022-06-22-USAD/Untitled.png)
            
            - LSTAM-AE는 정상 데이터만으로 학습을 시켜서 정상에 대한 분포를 모델이 학습하도록 함
            - inference에서는 새로운 input을 주었을 때 정상 데이터가 아닌 데이터는 처음 보기 때문에 제대로 복원하지 못함 → 이를 이용해서 anomaly detection 수행 (이때 reconstruction error 높아짐)
            - input W가 LSTM encoder에서 점차 축소되고 E(w)일 때 압축된 z가 됨, 이후 LSTM Decoder를 통해서 복원 수행
            - Anomaly Score = ![image-20220622095012956](/paper/images/2022-06-22-USAD/image-20220622095012956.png)
        
    - GAN-based AD : GAN은 학습이 불안정하여 mode collapse와 non-convergence같은 문제가 발생하기 쉬움
        - mode collapse :  Generator가 다양한 이미지를 만들어내지 못하고 비슷한 이미지만 생성하는 현상
        - non-convergence : 모델이 학습하면서 parameter들이 수렴하지 않고 진동할 때 비수렴 발생
        - **MAD-GAN**
          
            - Training : 정상 데이터만으로 LSTM 구조의 generator와 discriminator를 학습하여 정상 데이터의 분포를 학습시킴
            
              ![Untitled 1](/paper/images/2022-06-22-USAD/Untitled 1.png)
            
            - MAD-GAN은 전체적으로 GAN 구조
            - noise Z가 LSTM generator에 input으로 들어가서 fake data 생성
            - 생성된 fake data와 실제 data를 LSTM Discriminator에 input
            - LSTM Discriminator에서 abnormal 인지 normal 인지 판단한 출력값이 나옴
                - 전체적으로 fake data를 abnormal, real data를 normal로 판단하는 것이 discriminator가 하는 일
                - generator는 discriminator를 속이는 방향
                - GAN의 Adversarial Net과 동일하게 훈련됨
            - noise가 조금 있는 정상 데이터도 잘 판단함
            - inference 과정(Anomaly detection)
              
                ![Untitled 1](/paper/images/2022-06-22-USAD/Untitled 2.png)
                
                - ![image-20220622095507458](/paper/images/2022-06-22-USAD/image-20220622095507458.png) 의 optimal latent space를 찾는 것이 목표, 이를 찾기위해 k번 반복 수행하면서 ![image-20220622095533827](/paper/images/2022-06-22-USAD/image-20220622095533827.png)를 생성 , ![image-20220622095533827](/paper/images/2022-06-22-USAD/image-20220622095533827.png)는 real input인 ![image-20220622095614497](/paper/images/2022-06-22-USAD/image-20220622095614497.png)와 유사하게 맞춰짐
                - 이때 G,D 각각 , Reconstruction loss, Discrimination loss를 추출
                - 최종 검출 score은 ![image-20220622095640449](/paper/images/2022-06-22-USAD/image-20220622095640449.png)가 일정  threshold 초과하면 검출
            
    
    
    
    > 오토인코더와 적대적 학습의 한계점을 보정하면서 두 방법의 장점들을 합친 적대적 학습을 하는 오토인코더 구조를 제안





## Training

- **Autoencoder Training**
    - encoder E가 input ![Untitled 1](/paper/images/2022-06-22-USAD/image-20220622095614497.png)를 latent space Z로 압축한 후, Decoder D가 Z를 ![Untitled 1](/paper/images/2022-06-22-USAD/image-20220622095614497.png)와 동일하게 복원하는 Autoencoder를 학습
    
    - 기존과 다르게 DSAU에서는 adversarial trining을 위해 encoder E, decoder d1 , decoder d2로 구성, 하나의 encoder를 공유하는 두 개의 AE 형태
    
      ![Untitled 1](/paper/images/2022-06-22-USAD/Untitled 3.png)
    
    - 각각의 Decoder는 각 decoder의 reconstruction error를 기반으로 학습이 수행됨, (AE1, AE2 생성)
    
    - 두 AE 모두 정상 데이터의 분포를 학습하게 됨
    
      
    
- **Adversarial Training**
    - Autoencoder Training 단계에서 AE1에서 복원된 ![image-20220622100136622](/paper/images/2022-06-22-USAD/image-20220622100136622.png)를 기반으로 AE의 adversarial training 진행
      
        ![Untitled 1](/paper/images/2022-06-22-USAD/Untitled 4.png)
        
    - AE1이 GAN의  generator 역할을 수행함, AE1이 생성한 data와 real data가 AE2의 input으로 주어짐, AE2는 gan의 discriminator 역할 수행
    - AE1은 AE2를 잘 속이는 data를 만들고, AE2는 real과 fake를 잘 구분하도록 학습
    
    > 이 과정을 통해서 AE2는 정상 데이터와 유사한 이상 데이터도 잘 탐지하게 되면서 기존의 autoencoder의 한계를 보안할 수 있게 학습됨
    
    
    
- **Two-phase Training**
    - USAD 구성하는 두 AE
        - AE1 : input 잘 복원하면서 AE2를 속이는 모델
        - AE2 : input 잘 복원하면서 AE1이 복원한 데이터와 input을 잘 구별하는 모델
        
    - AE2, AE1의 adversarial training을 통해 정상 데이터와 유사한 이상치를 잘 탐지하게 하고, 전체적으로 AE 구조이기 때문에 안정적인 학습이 가능하게 됨
        - n = epoch에 해당하는 값
        
          ![image-20220622100234737](/paper/images/2022-06-22-USAD/image-20220622100234737.png)
        
          ![image-20220622100311747](/paper/images/2022-06-22-USAD/image-20220622100311747.png)
        
        - 둘다 ![image-20220622100343256](/paper/images/2022-06-22-USAD/image-20220622100343256.png)에 해당하는 reconstruction error가 동일하게 들어가고 이 부분에서 input가 동일하게 복원하는 부분
        - ![image-20220622100410708](/paper/images/2022-06-22-USAD/image-20220622100410708.png) 에서는 AE2를 잘속일수 있도록 input가 최대한 유사하게 생성
        - ![image-20220622100455122](/paper/images/2022-06-22-USAD/image-20220622100455122.png) 에서는 AE1이 생성한 데이터와 real data를 잘구분 하도록 학습
        
        > 안정적인 학습을 위해 초반에 reconstruction error에 가중치 주고 후반에는 adversarial training에 가중치를 부여함(초반에 복원 부분의 1/n으로 되어있기 때문에 epoch가 늘어날수록 영향력이 줄어들고 후반으로 갈수록 1-1/n으로 영향력이 늘어남)
        > 
        
        > 하나의 encoder에 두 개의 decoder를 사용해서 기존의 GAN base AE의 불안정한 학습을 AE 기반으로 만든 모델을 통해서 안정적 학습 가능하게 함
        > 
        





## Inference

![Untitled 1](/paper/images/2022-06-22-USAD/Untitled 5.png)

- 학습이 완료된 AE1, AE2를 기반으로 anomaly score을 산출함
- real_data를 encoder를 통해 압축된 Z를 생성, Z를 decoder를 통해 복원
- 복원된 데이터를 encoder → Z → decoder를 통해 산출
- AE1이 GAN의 generator, AE2는 GAN의 discriminator
- anomaly score = ![image-20220622100546892](/paper/images/2022-06-22-USAD/image-20220622100546892.png)

- α, β에 대한 내용
    - anomaly score의 계수의 합은 1, 계수의 비중에 따라 아래와 같이 false positive와 true positive간의 trade-off가 발생함(α+β = 1)
        - α > β : true & false positive 감소 → 낮은 이상치 탐지력을 가짐
        - α < β : true & false positive 증가 → 높은 이상치 탐지력을 가짐

- 끝으로 다른 모델과 현장에서 사용되는 데이터 셋으로 검증했을 때 높은 이상치 탐지 성능을 기록





## USAD 후기

- GAN의 모델 구조를 autoencoder를 통해서 적대적 학습을 통해 이상치 탐지 성능을 높임
- 다른 이상탐지와 비슷하게 autoencoder를 사용하지만 원본과 아주 유사하게 비슷한 이상데이터도 검출 가능




