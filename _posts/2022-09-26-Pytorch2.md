---
layout: single
title:  "Pytorch-2 basics"
categories : Pytorch
toc : true

---



pytorch : numpy + auto grad

# tensor
numpy에서 ndarray가 pytorch에서는 tensor라는 객체로 사용 됨.

```python
import torch
t_array = torch.FloatTensor(n_array)

torch.from_numpy 

```

pytorch의 tensor는 gpu에 올려서 사용가능
```
if torch.cuda.is_available():
	x_data_cuda = x_data.to(‘cuda’)
x_data_cuda.device

# device(type = ‘cuda’, index = 0)
```



## tensor handling

- view: reshape 대신에 쓴다고 생각하면 됨

*view와 reshape의 차이점
view는 주소를 받아와서 쓰기 때문에 기존 값이 바뀌면 같이 바뀜
replace는 값을 받아와서 쓰기 때문에 기존값이 바뀌어도 안바뀜

tip) view를 써라

- squeeze,unsqueeze : 1차원 삭제,추가
```python

tensor_ex.unsqueeze(0)

tensor_ex.unsqueeze(1)

tensor_ex.unsqueeze(2)
```


### tensor operation

대부분의 것들이 numpy와 비슷하게 이루어짐

행렬곱셈 연산은 dot이 아니라 mm (matmul) 사용
둘다 벡터라 내적구할때는 dot 써도 되는데
행렬곱셈 연산은 mm 사용해야함

tip) mm을 쓰자

*mm과 matmul은 broadcasting 지원 차이
matmul은 broadcasting 지원되서 웬만하면 돌아감

### tensor operations for ml/dl

```python

import torch
import torch.nn.functional as F

tensor = torch.FloatTensor([0.5,0.7,0.1])
h_tensor = F.softmax(tensor, dim =0)
```
쓸 때마다 찾아서 쓰도록

### autograd
파이토치 핵심은 자동 미분 지원 -> backward 함수 사용
```python
w = torch.tensor(2.0, requires_grad = True)
y = w**2
z = 10*y +25
z.backward()
w.grad
```

편미분

```python
a = torch.tensor([2., 3.], requires_grad=True)
b = torch.tensor([6., 4.], requires_grad=True)
Q = 3*a**3 - b**2
external_grad = torch.tensor([1., 1.])

#external_grad : 편미분 하는거 인듯
Q.backward(gradient=external_grad)

a.grad 
```

