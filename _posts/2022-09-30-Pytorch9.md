---
layout: single
title:  "Pytorch-9. Hyperparameter Tuning"
categories : Pytorch
tag : troubleshooting, OOM
toc : true
---


# Hyperparameter Tuning

+ 모델 스스로 학습하지 않는 값은 사람이 지정해야함
(learning rate, NAS, 모델의 크기, optimizer 등)
+ hyperparameter에 의해서 값이 크게 좌우 될 수도 있음
(사실 데이터가 훨씬 더 중요)
+ 마지막 0.01을 쥐어 짜야 할때 해볼만 


## 방법
+ 기본적인 방법 
	+ grid : 순차적으로 값을 올리거나 내림
	+ random : 임의적으로 값을 지정
+ 최근에는 베이지안 기반 기법들이 주도 (BOHB 논문)

## Ray
multi-node multi. processing 지원 모듈
ml / dl 의 병렬 처리를 위해 개발된 모듈
기본적으로 현재의 분산병렬 ML/DL 모듈의 표준
Hyperparameter Search를 위한 다양한 모듈 제공


### 실습코드)
config 에 search space 지정 (학습 공간들을 지정)

하면서 가망이 없는 것들은 버리면서 진행함


reporter : 결과 출력 양식 지정

tune.run : 병렬 처리 양식으로 학습 시행

*이건진짜 모르겠음