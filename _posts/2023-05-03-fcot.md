---
published: true
title:  "Faithful Chain-of-Thought Reasoning"
excerpt: "A novel prompting method that provides faithful explanations and improves performance on complex reasoning tasks"
header:
  overlay_color: "#000"
  overlay_filter: "0.5"
  overlay_image: assets/images/fcot/layover.png
  teaser: assets/images/fcot/pipeline.png
  actions:
    - label: "Paper"
      url: "https://arxiv.org/abs/2301.13379"
    - label: "Code"
      url: "https://github.com/veronica320/Faithful-COT"
authors: 
  - Qing Lyu|equal
  - Shreya Havaldar|equal
  - Adam Stein|equal
  - Li Zhang
  - Delip Rao
  - Eric Wong
  - Marianna Apidianaki
  - Chris Callison-Burch

pipeline:
  - url: "/assets/images/fcot/pipeline.png"
    image_path: "/assets/images/fcot/pipeline.png"
    alt: "Faithful Chain-of-Thought Pipeline"
    title: "Faithful Chain-of-Thought Pipeline"

standard_vs_cot:
  - url: "/assets/images/fcot/standard_vs_cot.png"
    image_path: "/assets/images/fcot/standard_vs_cot.png"
    alt: "Standard prompting vs. Chain-of-thought prompting"
    title: "Standard prompting vs. Chain-of-thought prompting"

cot_unfaithful:
  - url: "/assets/images/fcot/COT_unfaithful_1example.jpg"
    image_path: "/assets/images/fcot/COT_unfaithful_1example.jpg"
    alt: "COT unfaithful example"
    title: "COT unfaithful example"

GSM8K_example:
  - url: "/assets/images/fcot/GSM8K_example.png"
    image_path: "/assets/images/fcot/GSM8K_example.png"
    alt: "GSM8K example"
    title: "GSM8K example"

StrategyQA_example:
  - url: "/assets/images/fcot/StrategyQA_example.png"
    image_path: "/assets/images/fcot/StrategyQA_example.png"
    alt: "StrategyQA example"
    title: "StrategyQA example"

saycan_example:
  - url: "/assets/images/fcot/saycan_example.png"
    image_path: "/assets/images/fcot/saycan_example.png"
    alt: "Saycan example"
    title: "Saycan example"

clutrr_example:
  - url: "/assets/images/fcot/clutrr_example.png"
    image_path: "/assets/images/fcot/clutrr_example.png"
    alt: "Clutrr example"
    title: "Clutrr example"        

results:
  - url: "/assets/images/fcot/results.png"
    image_path: "/assets/images/fcot/results.png"
    alt: "Results"
    title: "Results"

prompt_variation:
  - url: "/assets/images/fcot/prompt_variation.png"
    image_path: "/assets/images/fcot/prompt_variation.png"
    alt: "Robustness to prompt phrasing"
    title: "Robustness to prompt phrasing"

exemplar_choice:
  - url: "/assets/images/fcot/exemplar_choice.png"
    image_path: "/assets/images/fcot/exemplar_choice.png"
    alt: "Robustness to exemplar choice"
    title: "Robustness to exemplar choice"        

ablation:
  - url: "/assets/images/fcot/ablation.png"
    image_path: "/assets/images/fcot/ablation.png"
    alt: "Ablation"
    title: "Ablation"

---
<!-- <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script> -->


<head>
  <style>
  .styled-table {
      border-collapse: collapse;
      margin: 25px 0;
      font-size: 0.9em;
      font-family: sans-serif;
      min-width: 400px;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.15);
  }
  .styled-table thead tr {
    background-color: #009879;
    color: #ffffff;
    text-align: middle;
  }
  .styled-table th {
    vertical-align: middle;
  },
  .styled-table td {
      padding: 12px 15px;
      text-align: middle;
  }
  .styled-table tbody td {
      text-align: middle;
  }
  .styled-table tbody tr {
    border-bottom: 1px solid #dddddd;
  }
  .styled-table tbody tr:nth-of-type(even) {
      background-color: #f3f3f3;
  }

  .styled-table tbody tr:last-of-type {
      border-bottom: 2px solid #009879;
  }
  </style>
</head>


Complex reasoning tasks, such as commonsense reasoning and math reasoning, have long been the Achilles heel of Language Models (LMs), until a recent line of work on *Chain-of-Thought (CoT)* prompting [[6](https://arxiv.org/abs/2203.11171), [7](https://proceedings.neurips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html), [8](https://arxiv.org/abs/2205.10625), i.a.] brought striking performance gains.

In this post, we introduce a new reasoning franework, **Faithful CoT**, to address a key shortcoming of existing CoT-style methods -- the lack of *faithfulness*. By guaranteeing faithfulness, our method provides a reliable *explanation* of how the answer is derived. Meanwhile, it outperforms vanilla CoT on 9 out of the 10 datasets from 4 diverse domains, showing a strong synergy between interpretability and accuracy.

## Chain-of-Thought (CoT) Prompting

Chain-of-Thought (CoT) prompting [[7](https://proceedings.neurips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html)] is a type of few-shot learning technique, where an LM is prompted to generate a reasoning chain along with the answer, given only a few in-context exemplars (right). This has remarkably boosted LMs' performance on a suite of complex reasoning tasks, compared to *standard prompting* [[1](https://proceedings.neurips.cc/paper_files/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html?utm_medium=email&utm_source=transaction)], where the model is prompted to generate only the answer but not the reasoning chain (left).

{% include gallery id="standard_vs_cot" layout="center" caption="An illustration of standard prompting vs. CoT prompting on a math problem, generated by Codex (code-davinci-002)." custom_style="width: 600px; height: 300px; margin-left: 20px;margin-right: 20px" %}


## Lack of Faithfulness in CoT

In addition to accuracy improvement, CoT is claimed to "provide an interpretable window into the behavior of the model". But **are these CoT reasoning chains actually good "explanations"**? 

Not necessarily, because they lack one fundamental property of interpretability, *faithfulness*:

>**Faithfulness**: An explanation (e.g., the generated reasoning chain) should **accurately represent** the reasoning process behind the model's prediction (i.e., how the model arrives at the final answer)"[3].

In most existing CoT-style methods, the final answer **does not necessarily follow from** the previously generated reasoning chain, so there is no guarantee on faithfulness:

{% include gallery id="cot_unfaithful" layout="center" caption="An example of *unfaithful* output from CoT prompting a math problem. The answer (green) does not follow from the reasoning chain (blue)." custom_style="width: 600px; height: 217px; margin-left: 20px;margin-right: 20px" %}

In the above example of CoT output, the answer "0" is not even mentioned in the reasoning chain. In other words, the LM doesn't really get to the answer in the way that it states to. This, along with more examples in our paper and other recent studies (e.g., [[5](https://arxiv.org/abs/2305.04388)]), illustrates that such CoT methods are not truely *self-interpretable*.

The lack of faithfulness in CoT can be dangerous in high-stake applications because it can give a false impression of "inherent interpretiblity", whereas there is indeed no causal relationship between the reasoning chain and the answer. Even worse, when an unfaithful explanation looks *plausible* (i.e., convincing to humans), this makes it easier for people (e.g., legal practitioners)  to over-trust the model (e.g., a recidivism predictor) even if it has implicit biases (e.g., against racial minorities) [[4](https://dl.acm.org/doi/abs/10.1145/3375627.3375830)].

## Our method: Faithful CoT

We propose Faithful CoT, a faithful-by-construction prompting framework where the answer is derived by deterministically executing the reasoning chain. Specifically, we break down a complex reasoning task into two stages: **Translation** and **Problem Solving**. 

During **Translation**, an LM translates a Natural Language query into a reasoning chain, which interleaves Natural Language and Symbolic Language. The Natural Language component is a decomposition of the original query into multiple simpler, interdependent subproblems. Then, each subproblem is solved in a task-dependent Symbolic Language, such as Python, <a href="https://en.wikipedia.org/wiki/Datalog">Datalog</a>, or <a href="https://en.wikipedia.org/wiki/Planning_Domain_Definition_Language">Planning Domain Definition Language (PDDL)</a>. Next, in the **Problem Solving** stage, the reasoning chain is executed by a deterministic solver, e.g., a Python/Datalog interpreter, or a PDDL planner, to derive the answer. 

{% include gallery id="pipeline" layout="center" caption="An overview of our Faithful Chain-of-Thought Reasoning pipeline, consisting of **Translation** and **Problem Solving**." %}

Our method is applicable to various reasoning tasks, thanks to its flexible integration with any choice of SL and external solver. We show how it works on four diverse tasks: Math, Multi-hop Question Answering (QA), Planning, and Relational Reasoning. Click the following tabs to explore each task. 


<ul class="tab" data-tab="44bf2f41-34a3-4bd7-b605-29d394ac9b0f" data-name="tasks">
      <li class="active">
          <a href="#">Math </a>
      </li>
  
      <li class="">
          <a href="#">Multi-hop QA </a>
      </li>
  
      <li class="">
          <a href="#">Planning </a>
      </li>
  
      <li class="">
          <a href="#">Relational Reasoning </a>
      </li>
</ul>
<ul class="tab-content" id="44bf2f41-34a3-4bd7-b605-29d394ac9b0f" data-name="tasks">
  
      <li class="active">
<p class="notice"><strong>Math Reasoning</strong>: Given a math question, we want to obtain the answer as a real-valued number. Here, we use Python as the symbolic language and the Python Interpreter as the determinstic solver. Below is an example from <a href="https://github.com/openai/grade-school-math">GSM8K</a>, a dataset of grade-school math questions.</p>

<figure class="center ">
  
    
      <a href="/assets/images/fcot/GSM8K_example.png" title="GSM8K example" class="image-popup">
          <img src="/assets/images/fcot/GSM8K_example.png" alt="GSM8K example" style="">
      </a>
  
</figure>

</li>
  
      <li class="">
<p class="notice"><strong>Multi-hop Question Answering (QA)</strong>: The input is a question involving multiple steps of reasoning, and the answer can be <code class="language-plaintext highlighter-rouge">True</code>, <code class="language-plaintext highlighter-rouge">False</code>, or a string. Depending on the dataset, we use either <a href="https://en.wikipedia.org/wiki/Datalog">Datalog</a> or Python as the symbolic language, and their respective interpreter as the solver. Here’s an example from the <a href="https://allenai.org/data/strategyqa">StrategyQA</a> dataset, which contains open-domain science questions.</p>

<figure class="center ">
  
    
      <a href="/assets/images/fcot/StrategyQA_example.png" title="StrategyQA example" class="image-popup">
          <img src="/assets/images/fcot/StrategyQA_example.png" alt="StrategyQA example" style="">
      </a>
  
</figure>

</li>
  
      <li class="">
<p class="notice"><strong>Planning</strong>: In a user-robot interaction scenario, the user gives a household task query, and the goal is come up with a plan of actions that the robot should take in order to accomplish the task. The symbolic language we use for this scenario is <a href="https://en.wikipedia.org/wiki/Planning_Domain_Definition_Language">Planning Domain Definition Language (PDDL)</a>, a standard encoding language for classical planning tasks. Then, we use a <a href="">PDDL planner</a> as the solver. See an example from the <a href="https://say-can.github.io/">Saycan</a> dataset, consisting of user queries in a kitchen scenario.</p>

<figure class="center ">
  
    
      <a href="/assets/images/fcot/saycan_example.png" title="Saycan example" class="image-popup">
          <img src="/assets/images/fcot/saycan_example.png" alt="Saycan example" style="">
      </a>
  
</figure>

</li>
  
      <li class="">
<p class="notice"><strong>Relational Reasoning</strong>: Given a relational reasoning problem, we want to obtain the answer as a string variable. For example, the <a href="https://github.com/facebookresearch/clutrr">CLUTRR</a> dataset involves inferring the family relationship between two people from a short story. Here, we use logical expressions as the symbolic language and a simple rule-based inference engine as the solver. See the following example.</p>

<figure class="center ">
  
    
      <a href="/assets/images/fcot/clutrr_example.png" title="Clutrr example" class="image-popup">
          <img src="/assets/images/fcot/clutrr_example.png" alt="Clutrr example" style="">
      </a>
    
</figure>

</li>
  
</ul>


## Findings

### Faithful CoT brings performance gains

Though our key motivation is to enhance interpretability, we do find that faithfulness empirically improves LMs' performance on various reasoning tasks. We show this on 10 datasets from the four domains above: Math Reasoning (GSM8K, SVAMP, MultiArith, ASDiv, AQUA), Multi-hop QA (StrategyQA, Date Understanding, Sports Understanding), Planning (Saycan), and Logical Inference (CLUTRR). 

{% include gallery id="results" layout="center" %}

In comparison with existing prompting methods (standard [[1](https://proceedings.neurips.cc/paper_files/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html?utm_medium=email&utm_source=transaction)], CoT [[7](https://proceedings.neurips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html)], Least-to-Most [[8](https://arxiv.org/abs/2205.10625)]), Faithful CoT performs the best on 8 out of the 10 datasets, with the same underlying LM (`code-davinci-002`) and greedy decoding strategy. In particular, Faithful CoT outperforms CoT with an average accuracy gain of 4.5 on MWP, 1.9 on Planning, 4.2 on Multi-hop QA, and 18.1 on Logical Inference. This performance gain generalizes across multiple code-generation LMs (`code-davinci-001`, `code-davinci-002`, `text-davinci-001`, `text-davinci-002`, `text-davinci-003`, `gpt-4`; see our [repo](https://github.com/veronica320/Faithful-COT) for latest results).

As for the other two datasets, Faithful CoT and Least-to-Most prompting both perform almost perfectly (99+ accuracy) on Sports Understanding, which may already be saturated. On StrategyQA, there is still a large accuracy gap between Faithful CoT and other methods. The primary cause is likely the sparsity of Datalog in the pretraining data for Codex, which we exmaine with an in-depth analysis in our paper. Still, with further pretraining on Datalog, we believe that there is room for improvement with our method.

After the recent release of ChatGPT (`gpt-3.5-turbo`) and GPT-4 (`gpt-4`) were released, we also experiment with them as the underlying LM Translator, instead of Codex:

<table class="styled-table">
    <thead>
      <tr>
        <th></th>
        <th colspan="5" style="text-align: center; vertical-align: middle;">Math Reasoning</th>
        <th colspan="1" style="text-align: center; vertical-align: middle;">Planning</th>
        <th colspan="3" style="text-align: center; vertical-align: middle;">Multi-hop QA</th>
        <th colspan="1" style="text-align: center; vertical-align: middle;">Relational Reasoning</th>
      </tr>
    </thead>
    <tbody>
      <tr>
          <th></th>
          <td>GSM8K</td>
          <td>SVAMP</td>
          <td>MultiArith</td>
          <td>ASDiv</td>
          <td>AQUA</td>
          <td>saycan</td>
          <td>StrategyQA</td>
          <td>date</td>
          <td>sports</td>
          <td>CLUTRR</td>
      </tr>
      <tr>
          <th>Codex</th>
          <td>72.2</td>
          <td>83.5</td>
          <td><strong>98.8</strong></td>
          <td>80.2</td>
          <td>47.2</td>
          <td>89.3</td>
          <td><strong>63.0</strong></td>
          <td>81.6</td>
          <td>99.1</td>
          <td>58.9</td>
      </tr>
      <tr>
          <th>ChatGPT</th>
          <td>75.8</td>
          <td>83.0</td>
          <td>95.3</td>
          <td>81.7</td>
          <td>53.5</td>
          <td>80.6</td>
          <td>51.5</td>
          <td>73.5</td>
          <td>52.3</td>
          <td>12.1</td>
      </tr>
      <tr>
          <th>GPT-4</th>
          <td><strong>95.0</strong></td>
          <td><strong>95.3</strong></td>
          <td>98.5</td>
          <td><strong>95.6</strong></td>
          <td><strong>73.6</strong></td>
          <td><strong>92.2</strong></td>
          <td>54.0</td>
          <td><strong>95.8</strong></td>
          <td><strong>99.3</strong></td>
          <td><strong>62.7</strong></td>
      </tr>
    </tbody>
</table>

Notably, equipped with GPT-4, Faithful CoT sets the new State-of-the-Art performance on many of the above datasets, achieving **95.0+** few-shot accuracy (❗) on almost all Math Reasoning datasets, Date Understanding, and Sports Understanding. However, the gap on StrategyQA becomes even larger. 

### Faithful CoT is robust to prompt design choices

How sensitive is Faithful CoT to various design choices in the prompt, such as the choice of exemplars and the phrasing of the prompt? To answer this, we vary each factor and repeat the experiment for multiple times (see our [paper](https://arxiv.org/abs/2301.13379) for details).

{% include gallery id="prompt_variation" layout="center" caption="Robustness to prompt phrasing. Variation 1/2/3 are three different prompt phrasings that are slightly different from the original." custom_style="width: 600px; height: 250px; margin-left: 20px;margin-right: 20px" %}

{% include gallery id="exemplar_choice" layout="center" caption="Robustness to the choice of exemplars. Set 1/2/3/4/5 are five different sets of exemplars randomly sampled from a total of 20 annotated examples." custom_style="width: 600px; height: 330px; margin-left: 20px;margin-right: 20px" %}

The above results show that the performance gains of Faithful CoT are minimally influenced by these factors, suggesting the robustness of our method.


### The solver is essential

How much does each component of Faithful CoT contribute to the performance? We perform an ablation study where we remove different parts from the framework and see how the performance changes. In addition to the original prompt (`Full`), we experiment with four variations:

- **No rationale**: we remove the rationales in the prompt, i.e., everything in the brackets from the NL comments, e.g., `independent, support: ["There are 15 trees"]`.

- **No NL but nudge**: we remove all NL comments in the prompt except the "nudge" line: e.g., `# To answer this question, we write a Python program to answer the following subquestions`.

- **No NL**: we remove all NL comments in the prompt.

- **No solver**: Instead of calling the external solver, we add `Answer: {answer}` to the end of every exemplar and let the LM predict the answer itself.

{% include gallery id="ablation" layout="center" caption="Ablation study results: accuracy when we remove different parts of the framework." custom_style="width: 600px; height: 250px; margin-left: 20px;margin-right: 20px"  %}

The external solver turns out to be essential to the performance, as it relieves the burden of problem solving from the LM. Without it, the accuracy suffers a huge decline on GSM8K, Date Understanding, and CLUTRR (-50.8, -22.9, and -19.4 respectively), while on SayCan it improves by 2.9 nonetheless, potentially because of its data homogeneity (see further analysis in our paper).

## Conclusion

We've introduced Faithful CoT, a novel framework that addresses the lack of faithfulness in existing CoT-style prompting methods. By splitting the reasoning task into two stages, Translation and Problem Solving, our framework provides a **faithful explanation** of how the answer is derived, and additionally improves the **performance** across various reasoning tasks and LMs. 

For more details, check out our [paper](https://arxiv.org/abs/2301.13379) and [Github repository](https://github.com/veronica320/Faithful-COT). 


Concurrent with our work, Chen et al. (2022) [[2](https://arxiv.org/abs/2211.12588)] and Gao et al. (2022) [[3](https://proceedings.mlr.press/v202/gao23f.html)] also explore the similar idea of generating programs as reasoning chains. We recommend that you check out their cool work as well! 
{: .notice--info}

## References

<small>
[1] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. (2020). Language models are few-shot learners. *Advances in neural information processing systems*, 33, 1877-1901.
</small>

<small>
[2] Chen, W., Ma, X., Wang, X., & Cohen, W. W. (2022). Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. *arXiv preprint arXiv:2211.12588*.
</small>

<small>
[3] Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y., ... & Neubig, G. (2023, July). Pal: Program-aided language models. In *International Conference on Machine Learning* (pp. 10764-10799). PMLR.
</small>

<small>
[4] Slack, D., Hilgard, S., Jia, E., Singh, S., & Lakkaraju, H. (2020, February). Fooling lime and shap: Adversarial attacks on post hoc explanation methods. In *Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society* (pp. 180-186).
</small>

<small>
[5] Turpin, M., Michael, J., Perez, E., & Bowman, S. R. (2023). Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting. *arXiv preprint arXiv:2305.04388*.
</small>

<small>
[6] Wang, X., Wei, J., Schuurmans, D., Le, Q. V., Chi, E. H., Narang, S., ... & Zhou, D. (2022, September). Self-Consistency Improves Chain of Thought Reasoning in Language Models. In *The Eleventh International Conference on Learning Representations*.
</small>

<small>
[7] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., ... & Zhou, D. (2022). Chain-of-thought prompting elicits reasoning in large language models. *Advances in Neural Information Processing Systems*, 35, 24824-24837.
</small>

<small>
[8] Zhou, D., Schärli, N., Hou, L., Wei, J., Scales, N., Wang, X., ... & Chi, E. H. (2022, September). Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. In *The Eleventh International Conference on Learning Representations*.
</small>

## Citation

```
@article{lyu2023faithful,
    title={Faithful Chain-of-Thought Reasoning},
    author={Lyu, Qing and Havaldar, Shreya and Stein, Adam and Zhang, Li and Rao, Delip and Wong, Eric and Apidianaki, Marianna and Callison-Burch, Chris},
    journal={arXiv preprint arXiv:2301.13379},
    year={2023}
}
```




