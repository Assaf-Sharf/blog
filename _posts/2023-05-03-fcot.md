---
title:  "Faithful Chain-of-Thought Reasoning"
excerpt: "A novel prompting method that provides faithful explanations and improves performance on complex reasoning tasks"
header:
  overlay_color: "#000"
  overlay_filter: "0.5"
  overlay_image: assets/images/fcot/layover.png
  teaser: assets/images/fcot/pipeline.png
  actions:
    - label: "Paper"
      url: "https://arxiv.org/abs/2301.13379"
    - label: "Code"
      url: "https://github.com/veronica320/Faithful-COT"
authors: 
  - Qing Lyu|equal
  - Shreya Havaldar|equal
  - Adam Stein|equal
  - Li Zhang
  - Delip Rao
  - Eric Wong
  - Marianna Apidianaki
  - Chris Callison-Burch

pipeline:
  - url: "/assets/images/fcot/pipeline.png"
    image_path: "/assets/images/fcot/pipeline.png"
    alt: "Faithful Chain-of-Thought Pipeline"
    title: "Faithful Chain-of-Thought Pipeline"

cot_unfaithful:
  - url: "/assets/images/fcot/COT_unfaithful_1example.jpg"
    image_path: "/assets/images/fcot/COT_unfaithful_1example.jpg"
    alt: "COT unfaithful example"
    title: "COT unfaithful example"

GSM8K_example:
  - url: "/assets/images/fcot/GSM8K_example.png"
    image_path: "/assets/images/fcot/GSM8K_example.png"
    alt: "GSM8K example"
    title: "GSM8K example"

StrategyQA_example:
  - url: "/assets/images/fcot/StrategyQA_example.png"
    image_path: "/assets/images/fcot/StrategyQA_example.png"
    alt: "StrategyQA example"
    title: "StrategyQA example"

saycan_example:
  - url: "/assets/images/fcot/saycan_example.png"
    image_path: "/assets/images/fcot/saycan_example.png"
    alt: "Saycan example"
    title: "Saycan example"

clutrr_example:
  - url: "/assets/images/fcot/clutrr_example.png"
    image_path: "/assets/images/fcot/clutrr_example.png"
    alt: "Clutrr example"
    title: "Clutrr example"        

results:
  - url: "/assets/images/fcot/results.svg"
    image_path: "/assets/images/fcot/results.svg"
    alt: "Results"
    title: "Results"

---
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<!-- --- -->

Complex reasoning tasks, such as [commonsense reasoning]() and [math reasoning](), have long been the Achilles heel of Language Models (LMs), until a recent line of work on *Chain-of-Thought (CoT)* prompting [1, 2, 3, i.a.] brought striking performance gains.

In this post, we introduce a new prompting method, **Faithful CoT**, to address a key shortcoming of existing CoT-style methods -- the lack of *faithfulness*. By guaranteeing faithfulness, our method provides a reliable *explanation* of how the answer is derived. Meanwhile, it outperforms vanilla CoT on 9 out of the 10 datasets from 4 diverse domains, showing a strong synergy between interpretability and accuracy.

## Lack of Faithfulness in CoT

In *CoT prompting*, a model is asked to generate a Natural Language reasoning chain along with the answer, given a few in-context exemplars. Compared to *standard prompting* (where the reasoning chain is not included), CoT considerably boosts LMs' performance on a suite of complex reasoning tasks.

In addition to accuracy improvement, CoT is claimed to "provide an interpretable window into the behavior of the model". However, it lacks one fundamental property of interpretability, *faithfulness*:

>**Faithfulness**: An explanation (e.g., the generated reasoning chain) should **accurately represent** the reasoning process behind the model's prediction (i.e., how the model arrives at the final answer)"[3].

In most existing CoT-style methods, the final answer **does not necessarily follow from** the previously generated reasoning chain, so there is no guarantee on faithfulness:

{% include gallery id="cot_unfaithful" layout="center" caption="An example of *unfaithful* output from CoT prompting [1] on GSM8K, a Math Word Problem dataset. The answer (green) does not follow from the reasoning chain (blue)." custom_style="width: 600px; height: 217px; margin-left: 20px;margin-right: 20px" %}

In the above example of CoT output, the answer "0" is not even mentioned in the reasoning chain. This, along with more examples in our paper, illustrates that such CoT methods are not truely *self-interpretable*.

The lack of faithfulness in CoT can be dangerous in high-stake applications because it may mislead people into believing that the model is inherently interpretable, while there is indeed no causal relationship between the reasoning chain and the answer. Even worse, when an unfaithful explanation looks *plausible* (i.e., convincing to humans), this makes it easier for people (e.g., legal practitioners)  to over-trust the model (e.g., a recidivism predictor) even if it has implicit biases (e.g., against racial minorities) [4].

## Our method: Faithful CoT

We propose Faithful CoT, a faithful-by-construction prompting framework where the answer is derived by deterministically executing the reasoning chain. Specifically, we break down a complex reasoning task into two stages: **Translation** and **Problem Solving**. 

During **Translation**, an LM translates a Natural Language query into a reasoning chain, which interleaves Natural Language and Symbolic Language. The Natural Language component is a decomposition of the original query into multiple simpler, interdependent subproblems. Then, each subproblem is solved in a task-dependent Symbolic Language, such as Python, Datalog, or Planning Domain Definition Language (PDDL). Next, in the **Problem Solving** stage, the reasoning chain is executed by a deterministic solver, e.g., a Python/Datalog interpreter, or a PDDL planner, to derive the answer. 

{% include gallery id="pipeline" layout="center" caption="An overview of our Faithful Chain-of-Thought Reasoning pipeline, consisting of **Translation** and **Problem Solving**." %}

Our method is applicable to various reasoning tasks, thanks to its flexible integration with any choice of SL and external solver. We show how it works on four diverse tasks: math, question and answering, planning, and logic.  Click the following tabs to explore each task. 


<ul class="tab" data-tab="44bf2f41-34a3-4bd7-b605-29d394ac9b0f" data-name="tasks">
      <li class="active">
          <a href="#">Math </a>
      </li>
  
      <li class="">
          <a href="#">Q&amp;A </a>
      </li>
  
      <li class="">
          <a href="#">Planning </a>
      </li>
  
      <li class="">
          <a href="#">Logic </a>
      </li>
</ul>
<ul class="tab-content" id="44bf2f41-34a3-4bd7-b605-29d394ac9b0f" data-name="tasks">
  
      <li class="active">
<p class="notice"><strong>Math Reasoning</strong>: Given a math question, we want to obtain the answer as a real-valued number. Here, we use Python as the symbolic language and the Python Interpreter as the determinstic solver. Below is an example from the <a href="">GSM8K</a> dataset.</p>

<figure class="center ">
  
    
      <a href="/assets/images/fcot/GSM8K_example.png" title="GSM8K example" class="image-popup">
          <img src="/assets/images/fcot/GSM8K_example.png" alt="GSM8K example" style="">
      </a>
    
  
  
</figure>

</li>
  
      <li class="">
<p class="notice"><strong>Multi-hop Question Answering (QA)</strong>: The input is a question involving multiple steps of reasoning, and the answer can be <code class="language-plaintext highlighter-rouge">True</code>, <code class="language-plaintext highlighter-rouge">False</code>, or a string. Depending on the dataset, we use either Datalog or Python as the symbolic language, and their respective interpreter as the solver. Here’s an example from the <a href="">StrategyQA</a> dataset.</p>

<figure class="center ">
  
    
      <a href="/assets/images/fcot/StrategyQA_example.png" title="StrategyQA example" class="image-popup">
          <img src="/assets/images/fcot/StrategyQA_example.png" alt="StrategyQA example" style="">
      </a>
    
  
  
</figure>

</li>
  
      <li class="">
<p class="notice"><strong>Planning</strong>: In a user-robot interaction scenario, the user gives a household task query, and the goal is come up with a plan of actions that the robot should take in order to accomplish the task. The symbolic language we use for this scenario is <a href="">Planning Domain Definition Language (PDDL)</a>, a standard encoding language for classical planning tasks. Then, we use a <a href="">PDDL planner</a> as the solver. See an example from the <a href="">Saycan</a> dataset.</p>

<figure class="center ">
  
    
      <a href="/assets/images/fcot/saycan_example.png" title="Saycan example" class="image-popup">
          <img src="/assets/images/fcot/saycan_example.png" alt="Saycan example" style="">
      </a>
    
  
  
</figure>

</li>
  
      <li class="">
<p class="notice"><strong>Logical Reasoning</strong>: Given a logical inference problem, we want to obtain the answer as a string variable. For example, the <a href="">CLUTRR</a> dataset involves inferring the family relationship between two people from a short story. Here, we use logical expressions as the symbolic language and a simple rule-based inference engine as the solver. See the following example.</p>

<figure class="center ">
  
    
      <a href="/assets/images/fcot/clutrr_example.png" title="Clutrr example" class="image-popup">
          <img src="/assets/images/fcot/clutrr_example.png" alt="Clutrr example" style="">
      </a>
    
  
  
</figure>

</li>
  
</ul>


## Results

In addition to providing a reliable explanation, we also find that faithfulness improves empirical accuracy on reasoning tasks. We show this on 10 datasets from the above 4 domains: Math Reasoning (GSM8K, SVAMP, MultiArith, ASDiv, AQUA), Multi-hop QA (StrategyQA, Date Understanding, Sports Understanding), Planning (Saycan), and Logical Inference (CLUTRR). 

{% include gallery id="results" layout="center" %}

In comparison with existing prompting methods ([standard](), [CoT](), [Least-to-Most]()), Faithful CoT performs the best on 8 out of the 10 datasets, with the same underlying LM (OpenAI Codex, `code-davinci-002`) and greedy decoding strategy. In particular, Faithful CoT outperforms CoT with an average accuracy gain of 4.5 on MWP, 1.9 on Planning, 4.2 on Multi-hop QA, and 18.1 on Logical Inference. 

As for the other two datasets, Faithful CoT and Least-to-Most prompting both perform almost perfectly (99+ accuracy) on Sports Understanding, which may already be saturated. On StrategyQA, there is still a large accuracy gap between Faithful CoT and other methods. The primary cause is likely the sparsity of Datalog in the pretraining data for Codex, which we exmaine with an in-depth analysis in our paper. Still, with further pretraining on Datalog, we believe that there is room for improvement with our method.

After the recent release of ChatGPT (`gpt-3.5-turbo`) and GPT-4 (`gpt-4`) were released, we also experiment with them as the underlying LM Translator, instead of Codex:


|         | **GSM8K** | **SVAMP** | **MultiArith** | **ASDiv** | **AQUA** | **saycan** | **StrategyQA** | **date** | **sports** | **CLUTRR** |
|-----------------------------|:---------:|:---------:|:--------------:|:---------:|:--------:|:----------:|:--------------:|:--------:|:----------:|:----------:|
| **Codex** |   72.2    |   83.5    |    **98.8**    |   80.2    |   47.2   | 89.3       |    **63.0**    | 81.6     | 99.1       | 58.9       |
| **ChatGPT**  |   75.8    |   83.0    |      95.3      |   81.7    |   53.5   | 80.6       |      51.5      | 73.5     | 52.3       | 12.1       |
| **GPT-4**     | **95.0**  | **95.3**  |      98.5      | **95.6**  |  **73.6**  | **92.2**       |      54.0      | **95.8**     | **99.3**       | **62.7**       |


Notably, equipped with GPT-4, Faithful CoT sets the new State-of-the-Art performance on many of the above datasets, achieving❗**95.0+** few-shot accuracy❗on almost all Math Reasoning datasets, Date Understanding, and Sports Understanding. However, the gap on StrategyQA becomes even larger. 


## Conclusion

We propose Faithful CoT, a novel framework that addresses the lack of faithfulness in existing CoT-style prompting methods. By splitting the reasoning task into two stages, Translation and Problem Solving, our method guarantees the reasoning chain is a faithful explanation of how the model arrives at the answer.

Based on experiments on 10 datasets across 4 diverse domains, we find that Faithful CoT also boosts accuracy over existing prompting baselines on a vast majority of datasets. These results give empirical evidence that improving model *interpretability* does not come at the expense of overall *performance*; in fact, we see a strong synergy in between.

In our [paper](https://arxiv.org/abs/2301.13379), through a comprehensive analysis on the strengths and weaknesses of our method, we show its robustness to the choice of exemplars and prompt phrasing, the pivotal role of the solver, as well as frequent error patterns where it still struggles.

For more details, check out our [paper](https://arxiv.org/abs/2301.13379) and [Github repository](https://github.com/veronica320/Faithful-COT). 


### Citation
```
@article{lyu2023faithful,
    title={Faithful chain-of-thought reasoning},
    author={Lyu, Qing and Havaldar, Shreya and Stein, Adam and Zhang, Li and Rao, Delip and Wong, Eric and Apidianaki, Marianna and Callison-Burch, Chris},
    journal={arXiv preprint arXiv:2301.13379},
    year={2023}
}
```




