# 타이타닉 생존자 예측(클론 코딩)


```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
```


```python
#타이타닉 데이터 불러들이기
titanic_df = pd.read_csv('./titanic_train.csv')
titanic_df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>male</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>A/5 21171</td>
      <td>7.2500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>female</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>PC 17599</td>
      <td>71.2833</td>
      <td>C85</td>
      <td>C</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>Heikkinen, Miss. Laina</td>
      <td>female</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>STON/O2. 3101282</td>
      <td>7.9250</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
      <td>female</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>113803</td>
      <td>53.1000</td>
      <td>C123</td>
      <td>S</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0</td>
      <td>3</td>
      <td>Allen, Mr. William Henry</td>
      <td>male</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>373450</td>
      <td>8.0500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
  </tbody>
</table>
</div>




```python
#불러온 데이터 column type 확인
titanic_df.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 891 entries, 0 to 890
    Data columns (total 12 columns):
     #   Column       Non-Null Count  Dtype  
    ---  ------       --------------  -----  
     0   PassengerId  891 non-null    int64  
     1   Survived     891 non-null    int64  
     2   Pclass       891 non-null    int64  
     3   Name         891 non-null    object 
     4   Sex          891 non-null    object 
     5   Age          714 non-null    float64
     6   SibSp        891 non-null    int64  
     7   Parch        891 non-null    int64  
     8   Ticket       891 non-null    object 
     9   Fare         891 non-null    float64
     10  Cabin        204 non-null    object 
     11  Embarked     889 non-null    object 
    dtypes: float64(2), int64(5), object(5)
    memory usage: 83.7+ KB
    

- pandas에서 object 형식은 string으로 봐도 무방(스트링 데이터가 길면 오브젝트로 구분한다고 함)
- .info()로 알 수 있는 내용은 전체 행의 갯수가 891개이고 12개의 열로 구성된 것과 각 열의 데이터 타입을 알 수 있었음
- 행의 총 갯수보다 부족한 열에 퍼져있는 Null 값을 처리해야함을 파악


```python
# 책에서는 Null값을 평균 또는 고정값으로 변경한다고 함
# 이 부분은 분석가들따라 다른 방식으로 전처리를 할 것 같음

titanic_df['Age'].fillna(titanic_df['Age'].mean(), inplace=True)
titanic_df['Cabin'].fillna('N', inplace=True)
titanic_df['Embarked'].fillna('N', inplace=True)

print('처리 후 데이터 Null 값의 개수?',titanic_df.isnull().sum().sum())
```

    처리 후 데이터 Null 값의 개수? 0
    


```python
#위에서 실행한 코드 중 이해가 안가는 부분

# sum()하나만 썼을 때 출력?
titanic_df.isnull().sum()
```




    PassengerId    0
    Survived       0
    Pclass         0
    Name           0
    Sex            0
    Age            0
    SibSp          0
    Parch          0
    Ticket         0
    Fare           0
    Cabin          0
    Embarked       0
    dtype: int64



- 상위 셀을 통해 알 수 있는 내용은 sum()을 한 번 수행하면 각 열에서 isnull()에 해당하는 행의 갯수의 합을 출력하고
- sum().sum()을 하면 각 열에서 isnull()에 해당한 행의 갯수의 합들의 합을 구하는 것!


```python
# Null값을 처리했고 이제는 문자열로 포진한 column을 파악해서 인코딩을 해줄 것 같음

print('Sex 값 분포:\n',titanic_df['Sex'].value_counts())
print('\nCabin 값 분포:\n',titanic_df['Cabin'].value_counts())
print('\nEmbarked 값 분포:\n',titanic_df['Embarked'].value_counts())
```

    Sex 값 분포:
     male      577
    female    314
    Name: Sex, dtype: int64
    
    Cabin 값 분포:
     N              687
    C23 C25 C27      4
    G6               4
    B96 B98          4
    D                3
                  ... 
    C54              1
    B101             1
    D28              1
    E38              1
    C87              1
    Name: Cabin, Length: 148, dtype: int64
    
    Embarked 값 분포:
     S    644
    C    168
    Q     77
    N      2
    Name: Embarked, dtype: int64
    

- value_count()함수는 열에 적혀있는 값들의 종류별 갯수를 파악하는 함수
- '\n'으로 띄어쓰기를 해서 결과값 확인할 때 가독성 올리기

- Cabin(선실)의 값들을 이 상태로 인코딩 할 상태가 아님
- 선실이 D면 D고 A면 A지 C23 C25 C27은 호그와트 9와4분의3승강장도 아니고..


```python
#이를 해결하기 위해 Cabin의 값 중에서 제일 첫 글자만 따와서 정리

titanic_df['Cabin'] = titanic_df['Cabin'].str[:1]
print(titanic_df['Cabin'].value_counts())
```

    N    687
    C     59
    B     47
    D     33
    E     32
    A     15
    F     13
    G      4
    T      1
    Name: Cabin, dtype: int64
    

## 데이터 탐색

Q.어떤 유형의 승객이 생존 확률이 높았을까?

A.여성과 아이들, 노약자가 우선순위로 구조될 것이고 그 다음은 유명인과 부자였을 것. 설국열차 꼬리칸 생각하면 편함


```python
#성별에 따른 생존확률 조사
titanic_df.groupby(['Sex', 'Survived'])['Survived'].count()
```




    Sex     Survived
    female  0            81
            1           233
    male    0           468
            1           109
    Name: Survived, dtype: int64



### groupby() 이해하고 넘어가기

- titanic_df.groupby(['Sex', 'Survived'])의 의미는 'Sex'열에서 같은 값끼리 구분 하고 (ex)Female과 Male로 나뉘어져 있을 것
- 그 다음 'Survived'로 다시 구분하는 것. 즉, Female의 생존/사망, Male의 생존/사망으로 나누어 진다


```python
# 시각화를 통해 다시 확인하기

sns.barplot(x='Sex', y='Survived', data=titanic_df)
```




    <AxesSubplot:xlabel='Sex', ylabel='Survived'>




    
![output_15_1](././output_15_1.png)
   



```python
# 객실 등급에 따른 생존률을 시각화로 확인
sns.barplot(x='Pclass', y='Survived', hue='Sex', data=titanic_df)
```




    <AxesSubplot:xlabel='Pclass', ylabel='Survived'>




    
![png](./output_16_1.png)
    


- 여기서 hue 인자는 'Pclass안에서 성별로 따로 구분지어서 보기'위해서 넣는 파라미터

### 성별과 객실에 따른 생존확률 결과

1. 전체적으로 여성의 생존률이 높음
2. 좋은 객실일수록 살아남을 확률이 높음


```python
# 나이구간별 생존 확률 조사

def get_category(age):
    cat = ''
    if age <= 1: cat = 'Unknown'
    elif age <= 5: cat = 'Baby'
    elif age <= 12: cat = 'Child'
    elif age <= 18: cat = 'Teenager'
    elif age <= 25: cat = 'Student'
    elif age <= 35: cat = 'Young Adult'
    elif age <= 60: cat = 'Adult'
    else: cat = 'Elderly'
        
        
    return cat
```


```python
# 막대 그래프 크기 설정(분석진행에 문제 없음)
plt.figure(figsize=(10,6))
```




    <Figure size 720x432 with 0 Axes>




    <Figure size 720x432 with 0 Axes>



```python
# X축 값을 순차적으로 표시하기 위한 리스트 작성
group_names = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Elderly']
```


```python
# lambda 식에 위에서 생성한 get_category() 함수를 반환값으로 지정
# get_category(X)는 입력값으로 'Age' 칼럼 값을 받아서 해당하는 cat 반환

titanic_df['Age_cat'] = titanic_df['Age'].apply(lambda x: get_category(x))

sns.barplot(x='Age_cat', y='Survived', hue='Sex', data=titanic_df, order=group_names)
```




    <AxesSubplot:xlabel='Age_cat', ylabel='Survived'>




    
![png](./output_22_1.png)
    



```python
titanic_df.drop('Age_cat', axis=1, inplace=True)
```

- titanic_df['Age'].apply(lambda x: get_category(x)) 식은'Age'열을 get_category()함수를 적용해서 적용시킨다는 건데
- 우리는 적용시킨 것을 또 다른 변수로 만든 것임
- 마지막에 drop 시킨 것을 통해 알 수 있음(있었으니까 드롭도 가능한 것)


```python
# 남아있는 문자열 카테고리 데이터를 인코딩

from sklearn import preprocessing

def encode_features(dataDF):
    features = ['Cabin', 'Sex', 'Embarked']
    for feature in features:
        le = preprocessing.LabelEncoder()
        le = le.fit(dataDF[feature])
        dataDF[feature] = le.transform(dataDF[feature])
        
    return dataDF
```


```python
titanic_df = encode_features(titanic_df)
titanic_df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>1</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>A/5 21171</td>
      <td>7.2500</td>
      <td>7</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>0</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>PC 17599</td>
      <td>71.2833</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>Heikkinen, Miss. Laina</td>
      <td>0</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>STON/O2. 3101282</td>
      <td>7.9250</td>
      <td>7</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
      <td>0</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>113803</td>
      <td>53.1000</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0</td>
      <td>3</td>
      <td>Allen, Mr. William Henry</td>
      <td>1</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>373450</td>
      <td>8.0500</td>
      <td>7</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>



- Cabin Sex, Emberked 값들이 모두 숫자형 데이터로 바뀜

### 피처데이터 가공한 내역을 정리하고 함수로 만들어서 재사용에 용이하도록 만들기


```python
# 1.Null 처리 함수
def fillna(df):
    df['Age'].fillna(df['Age'].mean(), inplace=True)
    df['Cabin'].fillna('N', inplace=True)
    df['Embarked'].fillna('N', inplace=True)
    df['Fare'].fillna(0, inplace=True)
    
    return df


# 2. 머신러닝 알고리즘에 불필요한 속성 제거
def drop_features(df):
    df.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)
    return df
```


```python
# 3. 레이블 인코딩 수행
from sklearn.preprocessing import LabelEncoder

def format_features(df):
    df['Cabin'] = df['Cabin'].str[:1]
    features = ['Cabin', 'Sex', 'Embarked']
    for feature in features:
        le = LabelEncoder()
        le = le.fit(df[feature])
        df[feature] = le.transform(df[feature])
        
    return df
```


```python
# 4. 앞에서 설정한 데이터 전처리 함수 호출
def transform_features(df):
    df = fillna(df)
    df = drop_features(df)
    df = format_features(df)
    return df
```

## 지금까지 했던 과정을 방금 만든 함수를 통해 진행해보기


```python
titanic_df = pd.read_csv('./titanic_train.csv')

# 머신러닝에 훈련하기 위해 train과 test 분류

y_titanic_df = titanic_df['Survived']
X_titanic_df = titanic_df.drop('Survived', axis = 1)
```


```python
# 만든 함수로 가공

X_titanic_df = transform_features(X_titanic_df)
```


```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_titanic_df, y_titanic_df, test_size=0.2, random_state=11)
```

### 활용할 알고리즘

1. 결정트리
2. 랜덤 포레스트
3. 로지스틱 회귀(이름은 회귀지만 강력한 분류 알고리즘이라고 함)


```python
# 머신러닝 알고리즘 세팅

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
```


```python
# 각 모델의 클래스 생성하기

dt_clf = DecisionTreeClassifier(random_state=11)
rf_clf = RandomForestClassifier(random_state=11)
lr_clf = LogisticRegression()
```


```python
# 결정트리분류기 학습/예측/평가

dt_clf.fit(X_train, y_train)
dt_pred = dt_clf.predict(X_test)
print('결정트리정확도: {0:.4f}'.format(accuracy_score(y_test, dt_pred)))
```

    결정트리정확도: 0.7877
    


```python
# 랜덤포레스트 학습/예측/평가

rf_clf.fit(X_train, y_train)
rf_pred = rf_clf.predict(X_test)
print('랜덤포레스트정확도: {0:.4f}'.format(accuracy_score(y_test, rf_pred)))
```

    랜덤포레스트정확도: 0.8547
    


```python
# 로지스틱회귀 학습/예측/평가

lr_clf.fit(X_train, y_train)
lr_pred = lr_clf.predict(X_test)
print('로지스틱회귀정확도: {0:.4f}'.format(accuracy_score(y_test, lr_pred)))
```

    로지스틱회귀정확도: 0.8492
    

    C:\Users\Amitis\Anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):
    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
    
    Increase the number of iterations (max_iter) or scale the data as shown in:
        https://scikit-learn.org/stable/modules/preprocessing.html
    Please also refer to the documentation for alternative solver options:
        https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
      n_iter_i = _check_optimize_result(
    

### 교차검증을 통해 결정 트리 모델을 더 평가하기


```python
from sklearn.model_selection import KFold

def exec_kfold(clf, folds=5):
    kfold = KFold(n_splits=folds)
    scores = []
    
    for iter_count, (train_index, test_index) in enumerate(kfold.split(X_titanic_df)):
        # X_titanic_df 데이터에서 교차 검증별로 학습과 검증 데이터를 가리키는 index 생성
        X_train, X_test = X_titanic_df.values[train_index], X_titanic_df.values[test_index]
        y_train, y_test = y_titanic_df.values[train_index], y_titanic_df.values[test_index]
        
        # Classifier 학습, 예측, 정확도 계산
        clf.fit(X_train, y_train)
        predictions = clf.predict(X_test)
        accuracy = accuracy_score(y_test, predictions)
        scores.append(accuracy)
        print('교차 검증 {0} 정확도: {1:.4f}'.format(iter_count, accuracy))
        
    mean_score = np.mean(scores)
    print('평균 정확도: {0:.4f}'.format(mean_score))
```


```python
exec_kfold(dt_clf, folds=5)
```

    교차 검증 0 정확도: 0.7542
    교차 검증 1 정확도: 0.7809
    교차 검증 2 정확도: 0.7865
    교차 검증 3 정확도: 0.7697
    교차 검증 4 정확도: 0.8202
    평균 정확도: 0.7823
    


```python
# cross_val_score()를 활용한 교차검증

from sklearn.model_selection import cross_val_score

scores = cross_val_score(dt_clf, X_titanic_df, y_titanic_df, cv=5)
for iter_count, accuracy in enumerate(scores):
    print('교차검증 {0} 정확도: {1:.4f}'.format(iter_count, accuracy))
    
print('평균정확도: {0:.4f}'.format(np.mean(scores)))
```

    교차검증 0 정확도: 0.7430
    교차검증 1 정확도: 0.7753
    교차검증 2 정확도: 0.7921
    교차검증 3 정확도: 0.7865
    교차검증 4 정확도: 0.8427
    평균정확도: 0.7879
    


```python
# gridsearch를 이용해 최적 하이퍼 파라미터를 찾고 예측하기

from sklearn.model_selection import GridSearchCV

parameters = {'max_depth':[2,3,5,10],
             'min_samples_split':[2,3,5],
             'min_samples_leaf':[1,5,8]}

grid_dclf = GridSearchCV(dt_clf, param_grid=parameters, scoring='accuracy', cv=5)
grid_dclf.fit(X_train, y_train)

print('Grid 최적 하이퍼 파라미터:',grid_dclf.best_params_)
print('Grid 최고 정확도:',grid_dclf.best_score_)

# Grid 최적 하이퍼 파라미터로 학습된 estimator로 예측/퍙가 수행
best_dclf = grid_dclf.best_estimator_

dpredictions = best_dclf.predict(X_test)
accuracy = accuracy_score(y_test, dpredictions)
print('테스트 세트에서의 결정트리분류기 정확도 : {0:.4f}'.format(accuracy))
```

    Grid 최적 하이퍼 파라미터: {'max_depth': 3, 'min_samples_leaf': 5, 'min_samples_split': 2}
    Grid 최고 정확도: 0.7991825076332119
    테스트 세트에서의 결정트리분류기 정확도 : 0.8715
    

- 실제로는 하이퍼 파라미터를 바꿨다고 해서 예측률이 8%이상 증가하는 경우는 거의 없음
- 테스트 데이터 세트가 작아서 수치상으로 예측성능이 많이 증가한 것으로 생각됨(강사님 의견)

*클론코딩 손으로 하면 시간 엄청 잡아먹는데 확실히 도움 많이 됩니다.. 낼부터는 시간을 갈아 넣어서 공부를 더 해보자 *
