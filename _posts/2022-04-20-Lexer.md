---
layout: single
title:  "Lexical Analysis and Finite State Machine"
categories: [Lexical Analysis,Scanner,Regular Expression,Finite State Machine,Compiler]
tag : [Lexical Analysis,lexer,lexical,scanner,token,parser,Regular Expression,Regular Grammar,Finite automata,Finite State Machine,Lexer impliment of C,C,c]
toc: true
author_profile: false
sidebar:
  nav : "docs"
search: true
---


### 시작하며 

Lexical Analysis(Scanner)에 대해 학습 할 수 있습니다. 
그리고 Regular Expression과 Finite State Machine에 대해서도 학습 할 수 있습니다. 


<a href="{{site.url}}/pdfs/Lexical.pdf">Lexer pdf</a>



### 과제에 대한 본인의 답안 



토론 생각 답

1. pdf에 token의 RE표현식 있다 

2. 여러 token 즉 keyword,id,type,operation등을 인식하는 sub automata를 통합하면 된다 

3. source program에 오류가 있다는게 애매하긴했다. syntax문제가 있다면 parser의 synatax analyzer가 처리를 할 것이고 lexer에 정의 되지 않는 token이 있다면 lexer가 Error를 처리 할 것 이다. 

4. lexer level과 syntax level의 차이점 

  lexer는 token이 정의된 token인지 확인 밖에 못한다. 괄호가 matching이 되는지 적절한 변수명을 썻는지 등에 대한 문법 오류는 syntax level에서 처리 할 수 있다. 


5. syntax error,sementic error,lexical error

6. lexer가 token을 구분하면 parser에게 넘기고 parser가 만든 parse tree를 통해 syntax    analayzer가 syntax level의 Error을 처리하면 될 것 같다 
  
  lexer는 문자가 잘못되었거나 이런건 detect할 수 있지만 문법적으로 틀린것인지는 탐지 할 수 없는데 syntax level의 오류를 어떻게 처리 하라고 한건지 잘 모르겠다 

**Lexical Error는 character의 순서가 token pattern과 맞지 않을 때 발생한다.** 


```c++

#define _CRT_SECURE_NO_WARNINGS 
#define MAX_STR_SIZE 100

/*
*  
* 
* 
	파일 읽고 파일 쓰기로 결과물 출력하는 프로그램 



*/



#include <stdio.h>
#include <string.h>
#include <stdlib.h>
#include <stdint.h>
#include<ctype.h>
void get_token(char* str, FILE* ff);	
int Check_keyword(char buffer[]);
int is_operator(char* str, int* idx);	//operator확인하는 함수
int is_parenthesis(char* str, int* idx); //괄호 확인하는 함수 

//token 자료 구조 
char keywords[13][10] = { "int","float","double","char","if","else","while","for","break","continue","const","return"};
char Operator[10] = {"><=+/*-%!"};
char parenthesis[7] = {"[{()}]"};
char OP[3] = {"=+-"};

int main()
{

	FILE* fp,*fw;	//파일 포인터
	char ch;
	char str_buffer[MAX_STR_SIZE];
	char int_text[20];		//int count를  문자열로 변환할때 사용 
	int count = 1;		//line 행 count를 의미한다
	fp = fopen("input_lexical.txt","r");		//입력 파일 읽기 모드
	fw = fopen("output_lexical.txt", "w+");

	//파일 읽기 이상 없는지 확인
	if (fp == NULL)
	{
		printf("Read the file ERORR\n");
		exit(1);

	}

	//파일 쓰기 이상 없는지 확인
	if (fw == NULL)
	{
		printf("Wirte the file ERORR\n");
		exit(1);
	}

	//파일 읽기 
	while (fgets(str_buffer,MAX_STR_SIZE,fp))
	{
		//LINE n 처리한다 
		//한줄씩 파일에서 가지고 온다 
		fputs("\nLINE ", fw);
		sprintf(int_text, "%d ", count++);
		fputs(int_text, fw);
		fputs(str_buffer, fw);

		//  //주석 처리
		//주석이면 LINE만 출력하고 다음으로 넘어간다 
		if (str_buffer[0] == '/' && str_buffer[1] == '/')
		{
			continue;
		}



		get_token(str_buffer, fw);



		
	}
	

	fclose(fp);
	fclose(fw);


	return 0;
}

//인자로는 파일에서 가지고온 한줄, 파일쓰기 포인터 
void get_token(char *str,FILE *ff) 
{
	int left = 0;
	int len = strlen(str);	//str 길이 
	
	int operator_mode= 0;
	int buffer_idx = 0;			
	char buffer[20];

	char ch[4];		//operator 출력하기 위한 버퍼 
	
	int flag4 = 0;	//goto ONE,TWO 처리 위한 flag
	int flag5 = 0;	////goto THREE,FOUR 처리 위한 flag
	

	
	while (left<len)	
	{
		

		//operator 인지 확인
		if (operator_mode = is_operator(str, &left))
		{
			//이전 버퍼에 저장되어있는 값이 있다면 먼저 출력하기 위해 goto사용
			if (buffer[0] != '\0')
			{
				flag4 = 1;
				goto ONE;
			}
		TWO:	//버퍼를 비우고 나서 operator출력을 한다
			
			
			//<= ,++ 같은 연산자를 처리하기 위한 모드이다
			if (operator_mode == 2)
			{
				ch[0] = str[left];
				ch[1] = str[++left];
				ch[2] = '\n';
				ch[3] = '\0';
				fputs(ch, ff);
				ch[0] = '\0';

			}
			else         //일반적인 연산자 처리위한 모드이다 
			{
				ch[0] = str[left];
				ch[1] = '\n';
				ch[2] = '\0';
				fputs(ch, ff);
				ch[0] = '\0';

			}

			left++;	//left++해줌으로써 다음 인덱스로 넘어간다 
			buffer_idx = 0;		//출력을 하였기 때문에 다시 buffer_idx=0으로 만들어준다 

		}
		else if (is_parenthesis(str, &left))		//괄호 처리 위한 함수 
		{
			//이전에 버퍼 내용이 있다면 비우고 다시 온다 
			if (buffer[0] != '\0')
			{
				flag5 = 1;
				goto THREE;
			}
		FOUR:
			//버퍼 비우고 오면 입력 들어온 괄호 처리한다 
			
			ch[0] = str[left];
			ch[1] = '\n';
			ch[2] = '\0';
			
			fputs(ch, ff);
			ch[0] = '\0';

			left++;
			buffer_idx = 0;

		}

		//연산자 ,괄호를 if - else if문으로 처리하고 if문이 걸리기 때문에 
		//마지막에 left++해주고 buffer_idx=0으로 초기화 해줌 

		//알파벳,숫자를 버퍼에 넣는다
		//소수도 처리하기 위해 .도 넣어준다 
		if (isalnum(str[left]) || str[left] == '.' || str[left] == '$')
		{

			buffer[buffer_idx] = str[left];
		


		}
		//str[left]가 공백이거나 ,이거나 ;일때 들어와서 버퍼를 처리한다 
		else if ((str[left] == ' ' || str[left] == ',' || str[left] == ';'))
		{

		ONE:
		THREE:

			//버퍼가 비어있다면 continue문으로 빠져 나간다 
			if (buffer[0] == '\0')
			{
				buffer_idx = 0;
				left++;
				continue;
			}
		
			//flag2는 정수인지 확인하는 flag이다 
			//flag3은 정수 확인중 실수형이라고 판단되면 활성화하는 flag이다 

			
			int flag2 = 1, flag3 = 0 ,k = 0;
		
			//buffer_idx==0일때 반복문을 돌기 위해 큰값을 넣어준다
			if (buffer_idx == 0)
			{
				k = 30;
			}
			else
			{
				//buffer_idx가 0이아닌 값이 있다면 k에 넣는다 
				k = buffer_idx;
			}
			for(int i=0;i<k;i++)
			{
	
				if (!isdigit(buffer[i]))
				{
					//.이 오면 실수형일수있으니 flag3활성화하고 continue한다 
					if (buffer[i] == '.')
					{
						flag3 = 1;
						continue;
					}
					//문자라면 flag2=0하고 나간다 
					flag2 = 0;
				
					break;
				}

	
			}


		
			//정수형이거나 실수형일때 
			if(flag2 && buffer[0] != ' ' && buffer[0] != ',')
			{
			

				//실수형이라면 
				if (flag3&&( buffer[0] != ' ' && buffer[0] != ',' && buffer[0] != '\0'))
				{
					//실수형 처리 한다 
					buffer[buffer_idx] = '\n';
					buffer[buffer_idx + 1] = '\0';
					fputs("FloatLiteral ", ff);
					fputs(buffer, ff);
					buffer[0] = '\0';
				}
				else if(buffer[0] != ' ' && buffer[0] != ',' && buffer[0] != '\0')
				{
					//정수형 처리 
					buffer[buffer_idx] = '\n';
					buffer[buffer_idx + 1] = '\0';
					fputs("IntLiteral ", ff);
					fputs(buffer, ff);
					buffer[0] = '\0';
				}

					
			}
			else
			{

				//문자 처리 한다 
			
				buffer[buffer_idx] = '\0';	//문자열로 만들기 위함이다 
				
				//만든 문자열이 keyword인지 확인하는 부분
				if (Check_keyword(buffer) == 1 && (buffer[0] != ' ' && buffer[0] != ','))
				{
					buffer[buffer_idx] = '\n';
					buffer[buffer_idx + 1] = '\0';
					fputs(buffer, ff);
					buffer[0] = '\0';
				}
				//keyword가 아니라면 
				else if (buffer[0] != ' ' && buffer[0] != ',' && buffer[0] != '\0')		//ID이면 ID 출력후 값 출력 
				{
					
					buffer[buffer_idx] = '\n';
					buffer[buffer_idx + 1] = '\0';
					fputs("ID ", ff);
					fputs(buffer, ff);
					buffer[0] = '\0';
				}





			}
			if (flag4)		//goto문 활성화 했을시 다시 돌아가기 위함이다 
			{
				flag4 = 0;
				goto TWO;
			}
			if (flag5)	//goto문 활성화 했을시 다시 돌아가기 위함이다 
			{
				flag5 = 0;
				goto FOUR;
			}


			
			buffer_idx = -1;	//바로 밑에 buffer_idx++있기 때문이다 . 0을 만들기 위해 -1로 설정함 

		}

		//세미클론 처리하기 위함이다 
		if (str[left] == ';')
		{
			
			fputs(";\n", ff);
			//buffer[0] = '\0';
		}

		left++;
		buffer_idx++;

	}
	
 
}
int is_operator(char *str,int* idx)
{

	int flag = 0;
	for (int i = 0; i < 10; i++)
	{
		//operator 배열과 일치하는 문자열이 있다면 flag 1로 활성화 
		if (Operator[i] == str[*idx])
		{
			flag = 1;
			
			break;
		}
	}
	if (flag)	//operator문자열이 활성화되었을때 
	{
		for (int i = 0; i < 3; i++)
		{	
			//만약 다음 문자도 OP배열 원소와 일치한다면 return 2
			//<=, == ,++처리하기 위함이다 
			if (OP[i] == str[(*idx) + 1])
			{
				
				return 2;
				
			}
		}
		//일반적인 연산자면 1반환
		return 1;
	}


	//아니라면 0반환 
	return 0;
	
	
}

int is_parenthesis(char* str, int* idx)
{
	for (int i = 0; i < 7; i++)
	{
		//해당 문자열과 괄호 배열 원소와 일치한다면 1반환
		if (parenthesis[i] == str[*idx])
		{
			return 1;
		}
	}
	return 0;
}
int Check_keyword(char buffer[])
{

	//버퍼 내용과 keyword배열의 원소와 일치하는지 확인하는 작업 

	for (int i = 0; i < 12; i++)
	{
		if (strcmp(keywords[i], buffer) == 0)
		{
			return 1;
		}
	}
	return 0;

}
```




**lexer 구현 부족했던 점**

1. push back 기능이 없다 
  
  한줄을 읽고 처리하는 방식이었는데 
  token 구분후 바로 start state로 돌아가는 방식이다 
  그래서 goto문을 쓴게 있다 

2. token을 구분할 마지막 요소 처리가 아쉬웠다
  
  공백이나 다른 문자가 오면 token을 구분하고 pushback 해주어야하는데 이 부분에서 구현이 아쉬웠다

3. sub automata 처리가 아쉬웠다 

  keyword와 id까지는 합쳐서 보는것 나쁘지 않았는데 
  token 구분 이후 integer,float,id,keyword를 한번에 처리하는건 좋지 않은 선택이 었던것 같다


### 내용 요약 및 정리 및 마무리 

* Lexer 
  soucre program에서 Token 추출하는 구문이다 
  lexer가 token 구분하고 parser에게 넘긴다 parser는 parse tree를 build한다 

  실제 parser는 token 필요 할때마다 lexer의 get_token()을 호출한다 

  추가 적으로 공백과 주석을 제거 해 준다 + line counting
	공백이나 연산자기호 또는 특수 기호를 발견하면 단어가 완성됨

* parser의 목적 

	토큰 목록을 가져와서 언어 구문을 검증하고 트리 구조로 변환한다 
	일부 파서는 트리 생성하면서 불필요한 토큰(예 중복괄호)을 생략하여 
	ast를 만든다 

  1. 모든 syntax error를 발견하고 적절한 진단 메시지를 제공하여 error 해결하는것 
  2. 프로그램에 parse tree 제공하는 것 



* lexer는 parser의 sub로 자리 잡고 있는데 이유는?

  lexer의 일을 parser가 다 할 수 있다 하지만 구분하는 이유와 같다 

  -> token은 Regual Language인데 context free language보다 간단한 방법으로 정의 할 수 있다 
  front-end 모듈화  분리하여 더 쉽게 구현 할 수 있다 
  이는 프로그램이 커지고 복잡해지는 것을 방지 할 수 있다. 



* Token is Terminal Symbols 

  문장에서 사용되는 최소 단위의 문법 요소 

  Regular Expression으로 표현한다 
  (위에서 Token은 regual language라고 했는데 RL는 RE로 표현 할수있으므로 의미적으로 통한다) 

* Regular Language 

  RG,RE,finite state machine으로 정의 가능 

  RG에서 집중해서 봐야할것은 정의한 구조이다
  예제를 통해 RG가 정의한 구조를 적용해 보아라 (terminal Nonterminal or non terminal 구조이다)

  **a ㅌ Terminal** 인 a를 Regular Expression으로 표현 할 수 있다


  Finite automata
  RE으로 이루어진 token을 판별할수있게 해주는 model 
  lexer가 token 구분할때 이 model을 사용해서 token을 구분 할 수 있다.


  token구분 하기 위해 마지막에는 반드시 token에 포함되지 않는 symbol 읽어야한다 

  마지막에 읽은건 pushback 처리를 해준다 

* lexer 구현 

  token을 한번에 구분하지 않고 keyword,id,type등의 sub finite automata를 구성해서 통합하는 방식으로 구현 한다 

  이때 마지막 글자는 push back과정을 하고 Start state로 돌아간다 

  push back은 마지막 요소를 읽었으면 읽지 않은 상태로 만들고 start로 돌아오는 역할을 한다 























  

