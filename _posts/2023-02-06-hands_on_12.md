---
layout: single
title:  "핸즈온 머신러닝 - 12"
categories : scikit-learn
tag : [scikit-learn, machine-learning, python]
toc: true
toc_sticky: true
---

![header](https://capsule-render.vercel.app/api?type=waving&color=a2dcec&height=300&section=header&text=핸즈온 머신러닝 - 12&fontSize=40&animation=fadeIn&fontAlignY=38&fontColor=FFFFFF)

- 참고 : [핸즈온 머신러닝 2판](http://www.kyobobook.co.kr/product/detailViewKor.laf?mallGb=KOR&ejkGb=KOR&barcode=9791162242964)

------------------------------------------------------

&nbsp;

## 텐서플로를 사용한 사용자 정의 모델과 훈련

- 자신만의 손실 함수, 지표, 층, 모델, 초기화, 규제, 가중치 규제 등을 만들어 세부적으로 제어하고 싶을 때  필요
- 텐서플로의 자동 그래프 생성 기능을 사용해 사용자 정의 모델과 훈련 알고리즘의 성능을 향상 시킬수 있는 방법

&nbsp;

### 텐서플로 훝어보기

- 이미지 분류, 자연어 처리, 추천 시스템, 시계열 예측 등과 같은 모든 종류의 머신러닝 작업 수행

- tensorflow 구성 이미지

  ![image-20230206180906828](/images/2023-02-06-hands_on_12/image-20230206180906828.png)

  - 가장 저수준의 tensorflow 연산은 매우 효율적인 C++ 코드로 구현

- 대부분의 코드는 고수준 API를 사용, 하지만 더 높은 자유도가 필요하는 경우 저 수준 API를 사용해 직접 텐서를 처리

&nbsp;

### numpy처럼 텐서플로 사용하기

&nbsp;

#### 텐서와 연산

- tf.constant() 함수로 텐서를 생성 할수 있음
- ndarray와 마찬가지로 tf.Tensor는 크기와 데이터 타입(dtype)을 가짐

```python
tf.constant([[1., 2., 3.], [4., 5., 6.]]) # 행렬
tf.constant(42) # 스칼라

t.shape
>> TensorShape([2, 3])
t.dtype
>> tf.float32
```

&nbsp;

- index 참조도 numpy와 유사
- tf.newaxis 는 차원을 하나 추가하는 것 (tf.expand_dims 와 유사)

```python
t[:, 1:]
>> <tf.Tensor: shape=(2, 2), dtype=float32, numpy=
    array([[2., 3.],
           [5., 6.]], dtype=float32)>
    
t[..., 1, tf.newaxis]
>> <tf.Tensor: shape=(2, 1), dtype=float32, numpy=
    array([[2.],
           [5.]], dtype=float32)>
```

&nbsp;

- 모든 종류의 텐서 연산이 가능함
- tf.square 은 제곱을 계산

```python
t + 10
>> <tf.Tensor: shape=(2, 3), dtype=float32, numpy=
    array([[11., 12., 13.],
           [14., 15., 16.]], dtype=float32)>
    
tf.square(t)
>> <tf.Tensor: shape=(2, 3), dtype=float32, numpy=
    array([[ 1.,  4.,  9.],
           [16., 25., 36.]], dtype=float32)>

t @ tf.transpose(t) # (2,3) 행렬곱 (3,2) 결과
>> <tf.Tensor: shape=(2, 2), dtype=float32, numpy=
    array([[14., 32.],
           [32., 77.]], dtype=float32)>
```

&nbsp;

#### 텐서와 넘파이

- 텐서는 넘파이와 함께 사용하기 편리함
- 넘파이 배열로 텐서를 만들 수 있고 그 반대도 가능
- 넘파이 배열에 텐서플로 연산을 적용할 수 있고 텐서에 넘파이 연산을 적용할 수도 있음

```python
a = np.array([2., 4., 5.])
tf.constant(a)
>> <tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>

t.numpy()
>> array([[1., 2., 3.],
       [4., 5., 6.]], dtype=float32)

tf.square(a)
>> <tf.Tensor: shape=(3,), dtype=float64, numpy=array([ 4., 16., 25.])>

np.square(t)
>> array([[ 1.,  4.,  9.],
       [16., 25., 36.]], dtype=float32)
```

&nbsp;

#### 타입 변환

- 타입 변환은 성능을 크게 감소시킬 수 있음
- tensorflow는 어떤 타입 변환도 자동으로 수행하지 않음
- 호환되지 않은 타입의 텐서로 연산을 실행하면 예외가 발생
  - 실수와 정수 텐서는 연산이 불가능
- type 변환시는 tf.cast() 사용

```python
t2 = tf.constant(40., dtype=tf.float64)
tf.constant(2.0) + tf.cast(t2, tf.float32)
>> <tf.Tensor: shape=(), dtype=float32, numpy=42.0>
```

&nbsp;

#### 변수

- tf.Tensor는 변경이 불가능한 객체

  - 일반적인 텐서로는 역전파로 변경되어야 하는 신경망의 가중치를 구현할 수 없음

  - 시간에 따라 변경되어야 할 다른 파라미터도 존재

    > 이를 위해서 tf.Variable이 필요

- tf.variable은 tf.tensor와 비슷하게 동작, 동일한 연산을 수행할 수 있고 넘파이와 호환이 좋음

- assign() 메서드를 통해 변수값 변경이 가능 (assign_add, assign_sub를 통해 주어진 값만큼 변수 증감 가능)

- slide + assign , scatter_update(), scatter_nd_update() 메서드를 사용해 개별 원소 수정

```python
v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])
v.assign(2 * v)
>> [[ 2.,  4.,  6.],[ 8., 10., 12.]]
    
v[0, 1].assign(42)
>> [[ 2., 42.,  6.], [ 8., 10., 12.]]

v[:, 2].assign([0., 1.])
>> [[ 2., 42.,  0.], [ 8., 10.,  1.]]
```

&nbsp;

#### 다른 데이터 구조

- 희소 텐서 (tf.SparseTensor)
  - 대부분 0으로 채워진 텐서를 효율적으로 나타냄, tf.sparse 패키지는 희소 텐서를 위한 연산을 제공
- 텐서 배열 (tf.TensorArray)
  - 텐서의 리스트, 기본적으로 고정된 길이를 가지지만 동적으로 변경가능, 리스트에 포함된 모든 텐서는 크기와 데이터 타입이 동일해야 함
- 래그드 텐서(tf.RaggedTensor)
  - 리스트의 리스트에 해당, 텐서에 포함된 값은 동일한 데이터 타입을 가져야 하지만 리스트의 길이는 다를 수 있음
- 문자열 텐서(tf.string)
- 집합(tf.sets)
  - 일반적인 텐서로 표현, tf.sets를 통해 연산을 사용가능
- 큐
  - 단계별로 텐서를 저장, 여러 종류의 큐를 제공

&nbsp;

### 사용자 정의 모델과 훈련 알고리즘

&nbsp;

#### 사용자 정의 손실 함수

- 희귀 모델을 훈련하는 데 훈련 세트에 잡음 데이터가 조금 있다고 가정시 이상치를 제거하거나 고쳐서 데이터셋을 수정할수 있지만 비효율적
- 사용자 맞춤으로 손실을 정의할 수 있음

- Huber loss 구현예제

  ```python
  from sklearn.datasets import fetch_california_housing
  from sklearn.model_selection import train_test_split
  from sklearn.preprocessing import StandardScaler
  
  housing = fetch_california_housing()
  X_train_full, X_test, y_train_full, y_test = train_test_split(
      housing.data, housing.target.reshape(-1, 1), random_state=42)
  X_train, X_valid, y_train, y_valid = train_test_split(
      X_train_full, y_train_full, random_state=42)
  
  scaler = StandardScaler()
  X_train_scaled = scaler.fit_transform(X_train)
  X_valid_scaled = scaler.transform(X_valid)
  X_test_scaled = scaler.transform(X_test)
  
  
  def huber_fn(y_true, y_pred):
      error = y_true - y_pred
      is_small_error = tf.abs(error) < 1
      squared_loss = tf.square(error) / 2
      linear_loss  = tf.abs(error) - 0.5
      return tf.where(is_small_error, squared_loss, linear_loss)
  
  plt.figure(figsize=(8, 3.5))
  z = np.linspace(-4, 4, 200)
  plt.plot(z, huber_fn(0, z), "b-", linewidth=2, label="huber($z$)")
  plt.plot(z, z**2 / 2, "b:", linewidth=1, label=r"$\frac{1}{2}z^2$")
  plt.plot([-1, -1], [0, huber_fn(0., -1.)], "r--")
  plt.plot([1, 1], [0, huber_fn(0., 1.)], "r--")
  plt.gca().axhline(y=0, color='k')
  plt.gca().axvline(x=0, color='k')
  plt.axis([-4, 4, 0, 4])
  plt.grid(True)
  plt.xlabel("$z$")
  plt.legend(fontsize=14)
  plt.title("Huber loss", fontsize=14)
  plt.show()
  ```

  ![image-20230206184608605](/images/2023-02-06-hands_on_12/image-20230206184608605.png)

  ```python
  input_shape = X_train.shape[1:]
  
  model = keras.models.Sequential([
      keras.layers.Dense(30, activation="selu", kernel_initializer="lecun_normal",
                         input_shape=input_shape),
      keras.layers.Dense(1),
  ])
  
  model.compile(loss=huber_fn, optimizer="nadam", metrics=["mae"])
  
  model.fit(X_train_scaled, y_train, epochs=2,
            validation_data=(X_valid_scaled, y_valid))
  ```

&nbsp;

#### 사용자 정의 요소를 가진 모델을 저장하고 로드하기

- 사용자 정의 객체를 포함한 모델을 로드할때는 해당 객체를 매핑해야 함

```python
model.save("my_model_with_a_custom_loss.h5")
model = keras.models.load_model("my_model_with_a_custom_loss.h5",
                                custom_objects={"huber_fn": huber_fn})
model.fit(X_train_scaled, y_train, epochs=2,
          validation_data=(X_valid_scaled, y_valid))
```

- 매개변수를 받는 huber_fn

```python
def create_huber(threshold=1.0):
    def huber_fn(y_true, y_pred):
        error = y_true - y_pred
        is_small_error = tf.abs(error) < threshold
        squared_loss = tf.square(error) / 2
        linear_loss  = threshold * tf.abs(error) - threshold**2 / 2
        return tf.where(is_small_error, squared_loss, linear_loss)
    return huber_fn


model.compile(loss=create_huber(2.0), optimizer="nadam", metrics=["mae"])
model.fit(X_train_scaled, y_train, epochs=2,
          validation_data=(X_valid_scaled, y_valid))

model.save("my_model_with_a_custom_loss_threshold_2.h5")
model = keras.models.load_model("my_model_with_a_custom_loss_threshold_2.h5",
                                custom_objects={"huber_fn": create_huber(2.0)})

model.fit(X_train_scaled, y_train, epochs=2,
          validation_data=(X_valid_scaled, y_valid))
```

- keras.losses.Loss 클래스 상속하고 get_config() 매서드를 구현하는 방식으로 구현

```python
class HuberLoss(keras.losses.Loss):
    def __init__(self, threshold=1.0, **kwargs):
        self.threshold = threshold
        super().__init__(**kwargs)
    def call(self, y_true, y_pred):
        error = y_true - y_pred
        is_small_error = tf.abs(error) < self.threshold
        squared_loss = tf.square(error) / 2
        linear_loss  = self.threshold * tf.abs(error) - self.threshold**2 / 2
        return tf.where(is_small_error, squared_loss, linear_loss)
    def get_config(self):
        base_config = super().get_config()
        return {**base_config, "threshold": self.threshold}
    
    
model = keras.models.Sequential([
    keras.layers.Dense(30, activation="selu", kernel_initializer="lecun_normal",
                       input_shape=input_shape),
    keras.layers.Dense(1),
])
```

- 모델을 컴파일 할때 이 클래스의 인스턴스 사용 가능
- 모델을 저장할 때 임계값도 같이 저장됨, 모델을 로드할 때 클래스 이름과 클래스 자체를 매핑해야 함

```python'
model.compile(loss=HuberLoss(2.), optimizer="nadam", metrics=["mae"])
model.fit(X_train_scaled, y_train, epochs=2,
          validation_data=(X_valid_scaled, y_valid))

model.save("my_model_with_a_custom_loss_class.h5")
model = keras.models.load_model("my_model_with_a_custom_loss_class.h5",
                                custom_objects={"HuberLoss": HuberLoss})
model.fit(X_train_scaled, y_train, epochs=2,
          validation_data=(X_valid_scaled, y_valid))
```

&nbsp;

#### 활성화 함수, 초기화, 규제, 제한을 커스터마이징하기

- 사용자 정의 함수 예시

```python
# 사용자 정의 tf.nn.softplus(z)
def my_softplus(z): # tf.nn.softplus(z) 값을 반환합니다
    return tf.math.log(tf.exp(z) + 1.0)

# 사용자 정의 글로럿 초기화
def my_glorot_initializer(shape, dtype=tf.float32):
    stddev = tf.sqrt(2. / (shape[0] + shape[1]))
    return tf.random.normal(shape, stddev=stddev, dtype=dtype)

# 사용자 정의 L1 규제
def my_l1_regularizer(weights):
    return tf.reduce_sum(tf.abs(0.01 * weights))

# tf.nn.relu(weight)와 반환값이 같음
def my_positive_weights(weights): # tf.nn.relu(weights) 값을 반환합니다
    return tf.where(weights < 0., tf.zeros_like(weights), weights)
```

- 사용자 정의 함수에 따라서 매개변수가 달라짐, 만들어진 사용자 정의 함수는 보통의 함수와 동일하게 사용할 수 있음
- 아래 예시에서 Dense 층에 가중치 초기화, 규제등 함수를 적용할수 있음

```python
layer = keras.layers.Dense(1, activation=my_softplus,
                           kernel_initializer=my_glorot_initializer,
                           kernel_regularizer=my_l1_regularizer,
                           kernel_constraint=my_positive_weights)

model.save("my_model_with_many_custom_parts.h5")
model = keras.models.load_model(
    "my_model_with_many_custom_parts.h5",
    custom_objects={
       "my_l1_regularizer": my_l1_regularizer,
       "my_positive_weights": my_positive_weights,
       "my_glorot_initializer": my_glorot_initializer,
       "my_softplus": my_softplus,
    })
```

- 함수가 모델과 함께 저장해야 할 파라미터를 가지고 있다면 keras.regularizers.Regularizer, keras.constraints.Constraint, keras.initializers.Initializer, keras.layers.Layer와 같이 적절한 클래스를 상속

```python
class MyL1Regularizer(keras.regularizers.Regularizer):
    def __init__(self, factor):
        self.factor = factor
    def __call__(self, weights):
        return tf.reduce_sum(tf.abs(self.factor * weights))
    def get_config(self):
        return {"factor": self.factor}
    
keras.backend.clear_session()
np.random.seed(42)
tf.random.set_seed(42)

model = keras.models.Sequential([
    keras.layers.Dense(30, activation="selu", kernel_initializer="lecun_normal",
                       input_shape=input_shape),
    keras.layers.Dense(1, activation=my_softplus,
                       kernel_regularizer=MyL1Regularizer(0.01),
                       kernel_constraint=my_positive_weights,
                       kernel_initializer=my_glorot_initializer),
])
```

&nbsp;

#### 사용자 정의 지표

- 손실과 지표는 개넘적으로 다른 것이 아님
- 손실은 모델을 훈련하기 위해 경사 하강법에서 사용하므로 미분 가능해야 하고 그래이디언트가 모든 곳에서 0이 아니어야 함
- 지표는 모델을 평가할때 사용, 미분이 가능하지 않거나 모든 곳에서 그레이디언트가 0이어도 됨

```python
model = keras.models.Sequential([
    keras.layers.Dense(30, activation="selu", kernel_initializer="lecun_normal",
                       input_shape=input_shape),
    keras.layers.Dense(1),
])

model.compile(loss="mse", optimizer="nadam", metrics=[create_huber(2.0)])
model.fit(X_train_scaled, y_train, epochs=2)
```

- 각 배치마다의 손실은 지표가 아님, 수학적으로는 손실 = 지표 * 샘플 가중치의 평균
- 이 때문에 지표를 통해 정확히 계산을 할수 있는 객체가 필요
- 아래와 같은 Precision() 객체는 batch마다 점진적 업데이트가 이루어지기 때문에 이를 **스트리밍 지표**라고 함
  - result()를 호출하면 현재 지푯값을 얻을수 있음
  - variables()를 사용해 변수 확인 가능
  - reset_states()를 사용해 변수 초기화 가능

```python
precision = keras.metrics.Precision()
precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])
>> <tf.Tensor: shape=(), dtype=float32, numpy=0.8>
precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])
>> <tf.Tensor: shape=(), dtype=float32, numpy=0.5>
```

- 사용자 정의 스트리밍 지표
  - keras.metrics.metric 클래스를 상속
  - 전체 후버 손실과 지금까지 처리한 샘플 수를 기록하는 클래스, 결과값 요청시 평균 후버 손실 반환

```python
class HuberMetric(keras.metrics.Metric):
    def __init__(self, threshold=1.0, **kwargs):
        super().__init__(**kwargs) # 기본 매개변수 처리 (예를 들면, dtype)
        self.threshold = threshold
        self.huber_fn = create_huber(threshold)
        self.total = self.add_weight("total", initializer="zeros")
        self.count = self.add_weight("count", initializer="zeros")
    def update_state(self, y_true, y_pred, sample_weight=None):
        metric = self.huber_fn(y_true, y_pred)
        self.total.assign_add(tf.reduce_sum(metric))
        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))
    def result(self):
        return self.total / self.count
    def get_config(self):
        base_config = super().get_config()
        return {**base_config, "threshold": self.threshold}
```

- 지표를 함수로 정의하면 앞서 수동으로 했던 것처럼 keras가 배치마다 자동으로 함수를 호출하고 에포크 동안 평균을 기록
- HuberMetric 클래스 정의하는 유일한 이점은 threshold를 저장하는 것
  - 정밀도와 같이 어떤 지표는 배치에 걸쳐 단순히 평균을 낼 수 없음
  - 해당 경우는 스트리밍 지표를 구현하는 것 외에 다른 방법이 없음

&nbsp;

#### 사용자 정의 층

- tensorflow에는 없는 특이한 층을 가진 네트워크를 만들어야 할 때
- keras.layers.Flatten, keras.layers.ReLU와 같은 층은 가중치가 없음
  - 가중치가 필요 없는 사용자 정의 층을 만들기 위한 가장 간단한 방법은 파이썬 함수를 만든 후 keras.Layers.Lambda 층으로 감싸는 것
- 입력에 지수 함수를 적용하는 층

```python
exponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))

exponential_layer([-1., 0., 1.])
>> <tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.36787948, 1.        , 2.7182817 ], dtype=float32)>
```

> 회귀 모델에서 예측값의 스케일이 매우 다를 때 출력층에 사용

- 가중치가 있는 층을 만들려면 keras.layers.Layer를 상속해야 함
- 여러 가지 입력을 받는 층을 만들때는 call() 메서드에 모든 입력이 포함된 튜플을 매개변수 값으로 전달
- 비슷하게 compute_output_shape() 메서드의 매개변수도 각 입력의 배치 크기를 담은 튜플이어야 함
- 여러 출력을 가진 층을 만들려면 call() 메서드가 출력의 리스트를 반환해야 함, compute_output_shape() 메서드는 배치 출력 크기의 리스트를 반환
- Dense층의 간소화 버전 예시

```python
class MyDense(keras.layers.Layer):
    def __init__(self, units, activation=None, **kwargs):
        super().__init__(**kwargs)
        self.units = units
        self.activation = keras.activations.get(activation)

    def build(self, batch_input_shape):
        self.kernel = self.add_weight(
            name="kernel", shape=[batch_input_shape[-1], self.units],
            initializer="glorot_normal")
        self.bias = self.add_weight(
            name="bias", shape=[self.units], initializer="zeros")
        super().build(batch_input_shape) # must be at the end

    def call(self, X):
        return self.activation(X @ self.kernel + self.bias)

    def compute_output_shape(self, batch_input_shape):
        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])

    def get_config(self):
        base_config = super().get_config()
        return {**base_config, "units": self.units,
                "activation": keras.activations.serialize(self.activation)}
```

- 두 개의 입력과 세 개의 출력을 만드는 층

```python
class MyMultiLayer(keras.layers.Layer):
    def call(self, X):
        X1, X2 = X
        print("X1.shape: ", X1.shape ," X2.shape: ", X2.shape) # 사용자 정의 층 디버깅
        return X1 + X2, X1 * X2

    def compute_output_shape(self, batch_input_shape):
        batch_input_shape1, batch_input_shape2 = batch_input_shape
        return [batch_input_shape1, batch_input_shape2]
    
inputs1 = keras.layers.Input(shape=[2])
inputs2 = keras.layers.Input(shape=[2])
outputs1, outputs2 = MyMultiLayer()((inputs1, inputs2))
```

&nbsp;

#### 사용자 정의 모델

- 사용자 모델은 keras.model 클래스를 상속하여 층과 변수를 만들고 모델이 해야 할 작업을 call() 메서드에 구현
- 아래와 같은 모델을 만들 시

![image-20230206194027025](/images/2023-02-06-hands_on_12/image-20230206194027025.png)

- 입력이 첫 번째 완전 연결 층을 통과하여 두 개의 완전 연결 층과 스킵 연결로 구성된 **잔차 블록**으로 전달

- 그 다음 동일한 잔차 블록에 세 번 더 통과시킴, 그 후 잔차 블록을 지나 마지막 출력이 완전 연결된 출력 층에 전달
- ResidualBlock  예시
  - keras가 알아서 추적해야 할 객체가 담긴 hidden 속성을 감지하고 필요한 변수를 자동으로 이 층의 변수 리스트에 추가
  - 생성자에서 층을 만들고 call() 메서드에서 이를 사용
  - save() 메서드를 사용해 모델 저장하고 keras.models.load_model() 함수를 사용해 저장된 모델을 로드하고 싶다면 get_config() 메서드를 구현해야 함
  - save_weights()와 load_weight() 메서드를 사용해 가중치를 저장하고 로드할 수 있음 

```python
class ResidualBlock(keras.layers.Layer):
    def __init__(self, n_layers, n_neurons, **kwargs):
        super().__init__(**kwargs)
        self.hidden = [keras.layers.Dense(n_neurons, activation="elu",
                                          kernel_initializer="he_normal")
                       for _ in range(n_layers)]

    def call(self, inputs):
        Z = inputs
        for layer in self.hidden:
            Z = layer(Z)
        return inputs + Z
    
class ResidualRegressor(keras.models.Model):
    def __init__(self, output_dim, **kwargs):
        super().__init__(**kwargs)
        self.hidden1 = keras.layers.Dense(30, activation="elu",
                                          kernel_initializer="he_normal")
        self.block1 = ResidualBlock(2, 30)
        self.block2 = ResidualBlock(2, 30)
        self.out = keras.layers.Dense(output_dim)

    def call(self, inputs):
        Z = self.hidden1(inputs)
        for _ in range(1 + 3):
            Z = self.block1(Z)
        Z = self.block2(Z)
        return self.out(Z)
```

