---
layout: single
title: "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"
permalink: /studies/paper/LoRA
tags: [Paper, NLP]
categories:
  - 📄 paper
date: 2024-02-02
use_math: true
---
*시각-언어 사전 학습(Vision-Language Pre-training, VLP)은 많은 시각-언어 작업의 성능을 향상시켰다. 그러나 대부분의 기존 사전 학습된 모델들은 이해 기반 작업이나 생성 기반 작업 중 하나에서만 뛰어나다. 또한, 성능 향상은 대부분 웹에서 수집된 잡음이 많은 이미지-텍스트 쌍을 데이터셋에 확장함으로써 이루어졌는데, 이는 최적의 감독 원천이 아니다. 이 논문에서는 시각-언어 이해와 생성 작업 모두에 유연하게 전환할 수 있는 새로운 VLP 프레임워크인 BLIP을 제안한다. BLIP은 캡셔너가 합성 캡션을 생성하고 필터가 잡음이 많은 것들을 제거하는 캡션 부트스트래핑을 통해 잡음이 많은 웹 데이터를 효과적으로 활용한다. 본 논문에서는 이미지-텍스트 검색(평균 recall@1에서 +2.7%), 이미지 캡셔닝(CIDEr에서 +2.8%), VQA(VQA 점수에서 +1.6%) 등 다양한 시각-언어 작업에서 최고의 성과를 달성했다. BLIP은 또한 제로샷 방식으로 비디오-언어 작업에 직접 전환될 때 강력한 일반화 능력을 보여준다. 코드, 모델, 데이터셋은 이 [HTTP URL](https://github.com/salesforce/BLIP){:target="_blank"}에서 공개된다.*

## 📋 Table of Contents

- [1.Introduction](#1introduction)
- [2.Related Work](#2related-work)
- [3.Method](#3method)
- [4.Experiments and Discussions](#4experiments-and-discussions)
- [5.Comparison with State-of-the-arts](#5comparison-with-state-of-the-arts)
- [6.Additional Ablation Study](#6additional-ablation-study)
- [7.Conclusion](#7conclusion)
- [A.Downstream Task Details](#adownstream-task-details)
- [B.Additional Examples of Synthetic Captions](#badditional-examples-of-synthetic-captions)
- [C.Pre-training Dataset Details](#cpre-training-dataset-details)

## 1.Introduction

## 2.Related Work
### 2.1.Vision-language Pre-training

### 2.2.Knowledge Distillation

### 2.3.Data Augmentation

## 3.Method
### 3.1.Model Architecture

### 3.2.Pre-training Objectives

### 3.3.CapFilt

## 4.Experiments and Discussions
#### 4.1.Pre-training Details

### 4.2.Effect of CapFilt

### 4.3.Diversity is Key for Synthetic Captions

### 4.4.Parameter Sharing and Decoupling

## 5.Comparison with State-of-the-arts

### 5.1.Image-Text Retrieval

### 5.2.Image Captioning

### 5.3.Visual Question Answering(VQA)

### 5.4.Natural Language Visual Reasoning(NLVR2)

### 5.5.Visual Dialog (VisDial)

### 5.6.Zero-shot Transfer to Video-Language Tasks

## 6.Additional Ablation Study

## 7.Conclusion

## A.Downstream Task Details

## B.Additional Examples of Synthetic Captions

## C.Pre-training Dataset Details