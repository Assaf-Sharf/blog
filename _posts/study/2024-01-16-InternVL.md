---
layout: single
title: "InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks"
permalink: /studies/paper/InternVL
tags: [Paper, LVLM]
categories:
  - 📄 paper
date: 2024-01-16
use_math: true
---
*대규모 언어 모델(LLMs)의 기하급수적 성장은 멀티모달 AGI 시스템에 대한 수많은 가능성을 열어주었다. 그러나 비전 및 비전-언어 기반 모델의 발전, 멀티모달 AGI의 중요한 요소로서, LLMs의 발전 속도를 따라가지 못하고 있다. 이 연구에서는 대규모 비전-언어 기반 모델(InternVL)을 설계하였다. 이는 비전 기반 모델을 60억 매개변수로 확장하고 다양한 출처의 웹 규모 이미지-텍스트 데이터를 사용하여 점진적으로 LLM과 조화시킨다. 이 모델은 이미지 수준 또는 픽셀 수준 인식과 같은 시각 인식 작업, 제로샷 이미지/비디오 분류, 제로샷 이미지/비디오-텍스트 검색과 같은 비전-언어 작업에 널리 적용되어 최첨단 성능을 달성할 수 있으며, LLM과 연결하여 멀티모달 대화 시스템을 생성할 수 있다. 이 모델은 강력한 시각 능력을 가지고 있으며, ViT-22B의 좋은 대안이 될 수 있다.*

## 📋 Table of Contents

- [1. Introduction](#1-introduction)
- [2. Related Work](#2-related-work)
- [3. Proposed Method](#3-proposed-method)
- [4. Experiments](#4-experiments)
- [5. Conclusion](#5-conclusion)

## 1. Introduction
 - 대규모 언어 모델(LLMs)은 개방형 언어 작업에서 인상적인 능력을 발휘하며, 인공 일반 지능(AGI) 시스템의 발전을 크게 촉진하고 있다. 
 - LLMs를 활용한 VLLMs는 복잡한 시각-언어 대화와 상호작용을 가능하게 하지만 LLMs의 빠른 성장에 비해 뒤처진다.
 - 기존 VLLMs는 QFormer나  linear projection과 같은 lightweight “glue” layers<sup>1</sup>을 사용하여 시각과 언어 모델의 특징을 조정했다.
 - 하지만 이러한 조정에는 몇 가지 한계가 있다.
   - **(1) Disparity in parameter scales.:** 대규모 LLMs는 최대 1000조 파라미터에 달하지만, VLLMs에서 널리 사용되는 비전 인코더는 여전히 약 10억 파라미터에 불과하다.
   - **(2) Inconsistent representation.:** 순수 비전 데이터나 BERT 시리즈와 맞춰진 비전 모델은 LLMs와의 표현 불일치가 발생한다.
   - **(3) Inefficient connection.:** "접착" 계층은 일반적으로 가볍고 무작위로 초기화되며, 여러 멀티모달 작업에서 필요한 복잡하고 상호 의존적인 데이터 간의 관계를 완전히 포착하고 이해하는 데 있어 부족할 수 있다.
 - 이러한 한계를 극복하기 위해 비전 인코더를 LLM의 파라미터 규모에 맞추어 조정하고 이들의 표현을 조화시켜야한다.
 - 대규모 모델의 학습은 인터넷에서 얻은 대량의 이미지-텍스트 데이터가 필요하다.
 - 학습 효율성을 향상하는 전략으로 대조 학습(contrastive learning)에 보완적인 접근법으로 생성적 지도를 고려한다.
 - 그러나 저품질 데이터가 생성적 학습에 적합한지 여부는 우려된다.
 - ***InternVL**은 대규모 비전-언어 기반 모델로, 확장된 비전 인코더의 표현을 LLM과 조화시키며 다양한 비전 및 비전-언어 작업에서 최첨단 성능을 달성한다.*
  <div align="center">
  <img src="../../assets/images/2024-01-16-InternVL/Definition.jpg" alt="Definition" style="zoom:30%;"/>
  </div>
  
 - Fig 1 (c)과 같이 InternVL은 세 가지 주요 설계를 가진다.
   - **(1) Parameter-balanced vision and language components:** 6억 개의 파라미터를 가진 비전 인코더와 8억 개의 파라미터를 가진 LLM 미들웨어를 포함하다. 여기서 미들웨어는 prior vision-only (Fig 1 (a)) or dual-tower (Fig 1 (b)) 구조와 달리 대조 및 생성 작업에 유연한 조합을 제공한다.
   - **(2) Consistent representations:** 비전 인코더와 LLM 사이의 표현 일관성을 유지하기 위해 사전 학습된 다국어 LLaMA를 사용하여 미들웨어를 초기화하고 비전 인코더를 맞춘다.
   - **(3) Progressive image-text alignment:** 다양한 출처에서 이미지-텍스트 데이터를 활용하고, 점진적 정렬 전략을 통해 학습 안정성을 확보한다. 구체적으로 대규모 잡음이 많은 이미지-텍스트 데이터에서 대조 학습을 시작하고, 세밀한 데이터에서 생성적 학습으로 전환한다. 
 - 모델의 장점
   - **(1) Versatile.:** standalone 비전 인코더로서 인식 작업에 사용되거나 언어 미들웨어와 함께 시각-언어 작업 및 다중 모달 대화 시스템에 적용될 수 있다.
   - **(2) Strong.:** 학습 전략, 대규모 파라미터, 웹 규모 데이터를 활용하여 다양한 시각 및 시각-언어 작업에서 최고의 결과를 달성한다(Fig 2 참조).
   - **(3) LLM-friendly.:** LLMs와의 일치된 특징 공간으로 인해 기존 LLMs(LLaMA 시리즈, Vicuna, InternLM 등)와 원활하게 통합될 수 있다.
 - 모델 기여점
   - 대규모 시각-언어 기반 모델 제시: 시각 인식 작업, 시각-언어 작업, 다중 모달 대화 등 다양한 시각-언어 작업에서 우수한 성능을 보인다.
   - 점진적 이미지-텍스트 정렬 전략 도입: 웹 규모의 잡음이 많은 이미지-텍스트 데이터를 대조 학습에 최대한 활용하고, 세밀하고 고품질의 데이터를 생성적 학습에 사용하는 대규모 시각-언어 기반 모델의 효율적인 학습을 위한 전략을 제시한다.
   - 현재 SOTA 모델과 비교: InternVL이 이미지 분류(ImageNet), 의미론적 분할(ADE20K), 비디오 분류(Kinetics), 이미지-텍스트 검색(Flickr30K & COCO), 비디오-텍스트 검색(MSR-VTT), 이미지 캡셔닝(COCO & Flickr30K & NoCaps), 멀티모달 대화(MME & POPE & Tiny LVLM)을 포함한 다양한 일반적인 시각-언어 작업에서 선도적인 성능을 달성한다

**lightweight “glue” layers<sup>1</sup>는 대규모 언어 모델(Large Language Models, LLMs)과 비전 모델을 연결하는 데 사용되는 상대적으로 간단하고 경량화된 네트워크 레이어를 의미한다. 이러한 레이어는 비전 데이터와 언어 데이터 간의 특징을 조화시키고, 서로 다른 두 모델 간의 정보를 효과적으로 전달하도록 설계되었다.*

  <div align="center">
  <img src="../../assets/images/2024-01-16-InternVL/Fig1.jpg" alt="Figure_1" style="zoom:80%;"/>
  <br>
  <img src="../../assets/images/2024-01-16-InternVL/Fig2.jpg" alt="Figure_2" style="zoom:80%;"/>
  </div>

## 2. Related Work
### 2.1. Vision Foundation Models
 - AlexNet을 시작으로 지난 10년 동안 다양한 컨볼루션 신경망(CNNs)이 등장하여 ImageNet 벤치마크를 지속적으로 갱신했다.
 - residual connections은 gradients vanishing 문제를 해결하여 규모와 깊이가 큰 모델이 더 나은 성능을 낼 수 있다는 것을 시사했다.
 - 최근 ViT가 컴퓨터 비전 분야에서 새로운 네트워크 구조의 가능성을 보이며, 그 변형들은 용량을 크게 증가시키고 다양한 중요한 시각 작업에서 뛰어난 성능을 보였다.
 - 비전 기반 모델은 LLM과 lightweight “glue” layers을 통해 연결되지만, ImageNet이나 JFT와 같은 순수 시각 데이터셋에서 유래하거나, BERT 시리즈와의 이미지-텍스트 쌍을 통해 정렬되어 LLM과의 직접적인 정렬이 부족하다.

### 2.2. Large Language Models
 - 대규모 언어 모델(LLMs)은 자연어 처리 작업을 가능하게 하여 인공 지능 분야에 혁명을 가져왔으며, 특히 GPT-3의 등장은 few-shot 및 zero-shot 학습에서 큰 도약을 가져왔다.
 - ChatGPT, GPT-4를 포함한 다양한 오픈소스 LLMs가 등장했으며, 이러한 모델들은 자연어에 국한되지 않은 상호작용을 위한 새로운 가능성을 열고 있다.

### 2.3. Vision Large Language Models
 - 비전 대규모 언어 모델(VLLMs)은 언어 모델에 시각 정보 처리 및 해석 능력을 추가하려는 목표를 갖는다.
 - Flamingo, GPT-4, LLaVA 시리즈 등은 시각적 질문 답변에서 뛰어난 few-shot 성능을 보였으며 VisionLLM, KOSMOS-2, Qwen-VL 등은 지역 설명 및 위치 확인과 같은 비주얼 그라운딩 능력 즉, 시각적 근거 능력을 향상시켰다.
 - PaLM-E, EmbodiedGPT와 같은 모델은 실제 응용에서 VLLMs를 활용하는 데 중요한 진전을 이뤘다.
 - 시각 및 시각-언어 기반 모델의 발전은 VLLMs에 필수적이지만, 아직 그 발전 속도가 따라가지 못하고 있다.

## 3. Proposed Method
### 3.1. Overall Architecture
 - Fig 3와 같이 전통적인 비전 전용 백본과 듀얼 인코더 모델과 달리, InternVL은 6억 개 파라미터를 가진 비전 트랜스포머 InternViT-6B와 8억 개 파라미터를 가진 언어 미들웨어 QLLaMA로 설계되었다.
훈련 전략: 두 대규모 구성 요소를 정렬하기 위해 점진적 정렬 훈련 전략을 도입함. 대규모 잡음 데이터에서 대비 학습을 시작하여 고품질 데이터로 생성 학습으로 점차 이동함.
  <div align="center">
  <img src="../../assets/images/2024-01-16-InternVL/Fig3.jpg" alt="Figure_3" style="zoom:80%;"/>  
  </div>

### 3.2. Model Design
대규모 비전 인코더 InternViT-6B: InternVL의 비전 인코더는 6억 개 파라미터를 가진 ViT로 구현됨. 정확도, 속도, 안정성 간의 좋은 절충점을 얻기 위해 하이퍼파라미터 탐색을 수행함.
언어 미들웨어 QLLaMA: 시각과 언어적 특징을 정렬하기 위해 제안됨. 다국어 LLaMA를 기반으로 하며, 새로운 학습 가능한 쿼리와 크로스 어텐션 레이어를 추가함.
"스위스 아미 나이프" 모델 InternVL: 비전 인코더와 언어 미들웨어의 유연한 조합을 통해 다양한 시각-언어 작업을 지원함.

### 3.3. Alignment Strategy
시각-언어 대비 훈련: 첫 단계에서는 대규모 잡음 데이터에서 InternViT-6B와 다국어 LLaMA-7B에 대한 대비 학습을 수행함.
시각-언어 생성 훈련: 두 번째 단계에서는 InternViT-6B를 QLLaMA와 연결하고 생성 훈련 전략을 채택함. 고품질 데이터에 대한 추가적인 필터링을 수행함.
감독된 미세 조정: 다중 모달 대화 시스템을 만드는 데 InternVL의 이점을 보여주기 위해, 외부 LLM 디코더(e.g., Vicuna, InternLM)와 MLP 계층을 통해 연결하고 감독된 미세 조정을 수행함.

## 4. Experiments
### 4.1. Implementation Details
단계 1: InternViT-6B는 임의로 초기화되고, LLaMA-7B는 사전 학습된 가중치로 초기화됨. 모든 파라미터는 완전히 학습 가능함.
단계 2: InternViT-6B와 QLLaMA는 첫 번째 단계에서 가중치를 상속받고, QLLaMA의 새로운 학습 가능한 쿼리와 크로스-어텐션 레이어는 임의로 초기화됨.
단계 3: 두 가지 다른 구성이 사용됨. 하나는 InternViT-6B를 별도로 사용하고, 다른 하나는 전체 InternVL 모델을 동시에 사용함.

### 4.2. Visual Perception Benchmarks
이미지 분류로의 전이: ImageNet-1K 데이터셋을 사용하여 InternViT-6B의 시각적 표현 품질을 평가함.
시맨틱 세그멘테이션으로의 전이: ADE20K 데이터셋에서 InternViT-6B의 픽셀 수준 인식 능력을 평가함.

### 4.3. Vision-Language Benchmarks
제로샷 이미지 분류: InternVL-C는 다양한 ImageNet 변형 및 ObjectNet에서 선도적인 성능을 보임.
제로샷 비디오 분류: Kinetics-400/600/700 데이터셋에서 상위 1위 정확도 및 상위 1위 및 5위 평균 정확도를 보고함.
제로샷 이미지-텍스트 검색: InternVL은 영어와 중국어에서 이미지-텍스트 검색 능력을 보여줌.
제로샷 이미지 캡셔닝: InternVL은 COCO Karpathy 테스트 세트에서 다른 모델을 능가하는 제로샷 성능을 보임.

### 4.4. Multi-Modal Dialogue Benchmarks
다중 모달 대화 평가: InternVL-Chat 모델은 MME 및 POPE 같은 두 가지 주요 다중 모달 대화 벤치마크에서 우수한 성능을 보임.

### 4.5. Ablation Study
InternViT-6B의 하이퍼파라미터: 모델 깊이, 헤드 차원 및 MLP 비율의 변형을 탐색하여 최적의 모델을 선택함.
특징 표현의 일관성: InternVL과 기존 LLM 간의 특징 표현 일관성을 검증함. QLLaMA를 "접착" 계층으로 사용할 때 모든 세 가지 작업에서 성능이 크게 향상됨.

## 5. Conclusion
이 논문에서는 6억 개 파라미터로 확장된 비전 기반 모델과 일반적인 시각-언어 작업을 위해 조정된 대규모 시각-언어 기반 모델인 InternVL을 제시합니다. 구체적으로, 대규모 비전 기반 모델 InternViT-6B를 설계하고, 이를 LLM으로 초기화된 언어 미들웨어 QLLaMA와 점진적으로 정렬하며, 다양한 출처에서 웹 규모의 이미지-텍스트 데이터를 활용하여 효율적인 학습을 진행합니다. 이는 비전 기반 모델과 LLM 사이의 격차를 메우고, 이미지/비디오 분류, 이미지/비디오-텍스트 검색, 이미지 캡셔닝, 시각적 질문 답변, 다중 모달 대화와 같은 다양한 일반적인 시각-언어 작업에서 능숙함을 보여줍니다. 이 작업이 VLLM 커뮤니티의 발전에 기여하기를 바랍니다.