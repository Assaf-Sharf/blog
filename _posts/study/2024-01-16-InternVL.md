---
layout: single
title: "InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks"
permalink: /studies/paper/InternVL
tags: [Paper, LVLM]
categories:
  - 📄 paper
date: 2024-01-16
use_math: true
---
*대규모 언어 모델(LLMs)의 기하급수적 성장은 멀티모달 AGI 시스템에 대한 수많은 가능성을 열어주었다. 그러나 비전 및 비전-언어 기반 모델의 발전, 멀티모달 AGI의 중요한 요소로서, LLMs의 발전 속도를 따라가지 못하고 있다. 이 연구에서는 대규모 비전-언어 기반 모델(InternVL)을 설계하였다. 이는 비전 기반 모델을 60억 매개변수로 확장하고 다양한 출처의 웹 규모 이미지-텍스트 데이터를 사용하여 점진적으로 LLM과 조화시킨다. 이 모델은 이미지 수준 또는 픽셀 수준 인식과 같은 시각 인식 작업, 제로샷 이미지/비디오 분류, 제로샷 이미지/비디오-텍스트 검색과 같은 비전-언어 작업에 널리 적용되어 최첨단 성능을 달성할 수 있으며, LLM과 연결하여 멀티모달 대화 시스템을 생성할 수 있다. 이 모델은 강력한 시각 능력을 가지고 있으며, ViT-22B의 좋은 대안이 될 수 있다.*

## 📋 Table of Contents

- [1. Introduction](#1-introduction)
- [2. Related Work](#2-related-work)
- [3. Proposed Method](#3-proposed-method)
- [4. Experiments](#4-experiments)
- [5. Conclusion](#5-conclusion)

## 1. Introduction
LLMs와 AGI 시스템의 발전: 대규모 언어 모델(LLMs)은 개방형 언어 작업에서 인상적인 능력을 발휘하며, 인공 일반 지능(AGI) 시스템의 발전을 크게 촉진하고 있음. 이들의 모델 규모와 성능은 여전히 빠르게 증가 중임.
시각 대규모 언어 모델(VLLMs)의 도약: LLMs를 활용한 VLLMs는 복잡한 시각-언어 대화와 상호작용을 가능하게 하는 중요한 돌파구를 달성함. 그러나 시각 및 시각-언어 기반 모델의 발전은 LLMs의 빠른 성장에 비해 뒤처짐.
LLMs와 시각 모델의 결합 방법: 기존 VLLMs는 QFormer나 선형 투영과 같은 가벼운 "접착" 계층을 사용하여 시각과 언어 모델의 특징을 조정함. 그러나 이러한 조정에는 몇 가지 한계가 있음:
파라미터 규모의 불균형: 대규모 LLMs는 최대 1000조 파라미터에 달하지만, VLLMs에서 널리 사용되는 시각 인코더는 여전히 약 10억 파라미터에 불과함.
일관성 없는 표현: 순수 시각 데이터나 BERT 시리즈와 맞춰진 시각 모델은 종종 LLMs와의 표현 불일치를 나타냄.
비효율적인 연결: "접착" 계층은 일반적으로 가볍고 무작위로 초기화되며, 다중 모달 이해와 생성에 필수적인 풍부한 교차 모달 상호작용과 의존성을 포착하지 못할 수 있음.
새로운 접근 방식: 이러한 한계를 극복하기 위해, 시각 인코더를 LLM의 파라미터 규모에 맞추어 조정하고 이들의 표현을 조화시키는 것에 착안함. 그러나 이러한 대규모 모델의 학습은 인터넷에서 얻은 대량의 이미지-텍스트 데이터가 필요함. 이 데이터 내의 상당한 이질성과 품질 변동은 학습 과정에 상당한 도전을 제기함.
학습 전략: 효율적인 학습을 위해 대비 학습과 생성 학습을 보완적 접근법으로 고려함. 이 전략은 모델 학습 중 추가적인 지도를 제공하려는 목적임. 그러나 저품질 데이터의 생성 학습 적합성은 여전히 우려되는 문제임.
InternVL의 개발

InternVL 모델: 대규모 시각-언어 기반 모델인 InternVL을 제안함. 이 모델은 확장된 시각 인코더의 표현을 LLM과 조정하며, 다양한 시각 및 시각-언어 작업에서 최고의 성능을 달성함.
핵심 설계: InternVL은 세 가지 주요 설계를 포함함:
파라미터 균형화된 시각 및 언어 구성요소: 6억 개의 파라미터를 가진 시각 인코더와 8억 개의 파라미터를 가진 LLM 미들웨어를 포함함.
일관된 표현: 시각 인코더와 LLM 사이의 표현 일관성을 유지하기 위해 사전 학습된 다국어 LLaMA를 사용하여 미들웨어를 초기화하고 시각 인코더를 맞춤.
점진적 이미지-텍스트 정렬: 다양한 출처에서 이미지-텍스트 데이터를 활용하고, 점진적 정렬 전략을 통해 학습 안정성을 확보함.
모델의 장점

다재다능함: 독립적인 시각 인코더로서 인식 작업에 사용되거나, 언어 미들웨어와 함께 시각-언어 작업 및 다중 모달 대화 시스템에 적용될 수 있음.
강력함: 학습 전략, 대규모 파라미터, 웹 규모 데이터를 활용하여 다양한 시각 및 시각-언어 작업에서 최고의 결과를 달성함.
LLM 친화적: LLMs와의 조화된 특징 공간으로 인해 기존 LLMs와 원활하게 통합될 수 있음.
기여

대규모 시각-언어 기반 모델 제시: 시각 인식 작업, 시각-언어 작업, 다중 모달 대화 등 다양한 시각-언어 작업에서 우수한 성능을 보임.
점진적 이미지-텍스트 정렬 전략 도입: 대규모 시각-언어 기반 모델의 효율적인 학습을 위한 전략을 제시함.
현재 최고 

## 2. Related Work
### 2.1. Vision Foundation Models

### 2.2. Large Language Models

### 2.3. Vision Large Language Models

## 3. Proposed Method
### 3.1. Overall Architecture

### 3.2. Model Design

### 3.3. Alignment Strategy

## 4. Experiments
### 4.1. Implementation Details

### 4.2. Visual Perception Benchmarks

### 4.3. Vision-Language Benchmarks

### 4.4. Multi-Modal Dialogue Benchmarks

### 4.5. Ablation Study

## 5. Conclusion
