---
layout: single
title: "InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks"
permalink: /studies/paper/InternVL
tags: [Paper, LVLM]
categories:
  - 📄 paper
date: 2024-01-16
use_math: true
---
*대규모 언어 모델(LLMs)의 기하급수적 성장은 멀티모달 AGI 시스템에 대한 수많은 가능성을 열어주었다. 그러나 비전 및 비전-언어 기반 모델의 발전, 멀티모달 AGI의 중요한 요소로서, LLMs의 발전 속도를 따라가지 못하고 있다. 이 연구에서는 대규모 비전-언어 기반 모델(InternVL)을 설계하였다. 이는 비전 기반 모델을 60억 매개변수로 확장하고 다양한 출처의 웹 규모 이미지-텍스트 데이터를 사용하여 점진적으로 LLM과 조화시킨다. 이 모델은 이미지 수준 또는 픽셀 수준 인식과 같은 시각 인식 작업, 제로샷 이미지/비디오 분류, 제로샷 이미지/비디오-텍스트 검색과 같은 비전-언어 작업에 널리 적용되어 최첨단 성능을 달성할 수 있으며, LLM과 연결하여 멀티모달 대화 시스템을 생성할 수 있다. 이 모델은 강력한 시각 능력을 가지고 있으며, ViT-22B의 좋은 대안이 될 수 있다.*

## 📋 Table of Contents

- [1. Introduction](#1-introduction)
- [2. Related Work](#2-related-work)
- [3. Proposed Method](#3-proposed-method)
- [4. Experiments](#4-experiments)
- [5. Conclusion](#5-conclusion)

## 1. Introduction
 - 대규모 언어 모델(LLMs)은 개방형 언어 작업에서 인상적인 능력을 발휘하며, 인공 일반 지능(AGI) 시스템의 발전을 크게 촉진하고 있다. 
 - LLMs를 활용한 VLLMs는 복잡한 시각-언어 대화와 상호작용을 가능하게 하지만 LLMs의 빠른 성장에 비해 뒤처진다.
 - 기존 VLLMs는 QFormer나  linear projection과 같은 lightweight “glue” layers<sup>1</sup>을 사용하여 시각과 언어 모델의 특징을 조정했다.
 - 하지만 이러한 조정에는 몇 가지 한계가 있다.
   - **(1) Disparity in parameter scales.:** 대규모 LLMs는 최대 1000조 파라미터에 달하지만, VLLMs에서 널리 사용되는 비전 인코더는 여전히 약 10억 파라미터에 불과하다.
   - **(2) Inconsistent representation.:** 순수 비전 데이터나 BERT 시리즈와 맞춰진 비전 모델은 LLMs와의 표현 불일치가 발생한다.
   - **(3) Inefficient connection.:** "접착" 계층은 일반적으로 가볍고 무작위로 초기화되며, 여러 멀티모달 작업에서 필요한 복잡하고 상호 의존적인 데이터 간의 관계를 완전히 포착하고 이해하는 데 있어 부족할 수 있다.
 - 이러한 한계를 극복하기 위해 비전 인코더를 LLM의 파라미터 규모에 맞추어 조정하고 이들의 표현을 조화시켜야한다.
 - 대규모 모델의 학습은 인터넷에서 얻은 대량의 이미지-텍스트 데이터가 필요하다.
 - 학습 효율성을 향상하는 전략으로 대조 학습(contrastive learning)에 보완적인 접근법으로 생성적 지도를 고려한다.
 - 그러나 저품질 데이터가 생성적 학습에 적합한지 여부는 우려된다.
 - ***InternVL**은 대규모 비전-언어 기반 모델로, 확장된 비전 인코더의 표현을 LLM과 조화시키며 다양한 비전 및 비전-언어 작업에서 최첨단 성능을 달성한다.*
  <div align="center">
  <img src="../../assets/images/2024-01-16-InternVL/Definition.jpg" alt="Definition" style="zoom:30%;"/>
  </div>
  
 - Fig 1 (c)과 같이 InternVL은 세 가지 주요 설계를 가진다.
   - **(1) Parameter-balanced vision and language components:** 6억 개의 파라미터를 가진 비전 인코더와 8억 개의 파라미터를 가진 LLM 미들웨어를 포함하다. 여기서 미들웨어는 prior vision-only (Fig 1 (a)) or dual-tower (Fig 1 (b)) 구조와 달리 대조 및 생성 작업에 유연한 조합을 제공한다.
   - **(2) Consistent representations:** 비전 인코더와 LLM 사이의 표현 일관성을 유지하기 위해 사전 학습된 다국어 LLaMA를 사용하여 미들웨어를 초기화하고 비전 인코더를 맞춘다.
   - **(3) Progressive image-text alignment:** 다양한 출처에서 이미지-텍스트 데이터를 활용하고, 점진적 정렬 전략을 통해 학습 안정성을 확보한다. 구체적으로 대규모 잡음이 많은 이미지-텍스트 데이터에서 대조 학습을 시작하고, 세밀한 데이터에서 생성적 학습으로 전환한다. 
 - 모델의 장점
   - **(1) Versatile.:** standalone 비전 인코더로서 인식 작업에 사용되거나 언어 미들웨어와 함께 시각-언어 작업 및 다중 모달 대화 시스템에 적용될 수 있다.
   - **(2) Strong.:** 학습 전략, 대규모 파라미터, 웹 규모 데이터를 활용하여 다양한 시각 및 시각-언어 작업에서 최고의 결과를 달성한다(Fig 2 참조).
   - **(3) LLM-friendly.:** LLMs와의 일치된 특징 공간으로 인해 기존 LLMs(LLaMA 시리즈, Vicuna, InternLM 등)와 원활하게 통합될 수 있다.
 - 모델 기여점
   - 대규모 시각-언어 기반 모델 제시: 시각 인식 작업, 시각-언어 작업, 다중 모달 대화 등 다양한 시각-언어 작업에서 우수한 성능을 보인다.
   - 점진적 이미지-텍스트 정렬 전략 도입: 웹 규모의 잡음이 많은 이미지-텍스트 데이터를 대조 학습에 최대한 활용하고, 세밀하고 고품질의 데이터를 생성적 학습에 사용하는 대규모 시각-언어 기반 모델의 효율적인 학습을 위한 전략을 제시한다.
   - 현재 SOTA 모델과 비교: InternVL이 이미지 분류(ImageNet), 의미론적 분할(ADE20K), 비디오 분류(Kinetics), 이미지-텍스트 검색(Flickr30K & COCO), 비디오-텍스트 검색(MSR-VTT), 이미지 캡셔닝(COCO & Flickr30K & NoCaps), 멀티모달 대화(MME & POPE & Tiny LVLM)을 포함한 다양한 일반적인 시각-언어 작업에서 선도적인 성능을 달성한다

**“glue” layers<sup>1</sup>는 대규모 언어 모델(Large Language Models, LLMs)과 비전 모델을 연결하는 데 사용되는 상대적으로 간단하고 경량화된 네트워크 레이어를 의미한다. 이러한 레이어는 비전 데이터와 언어 데이터 간의 특징을 조화시키고, 서로 다른 두 모델 간의 정보를 효과적으로 전달하도록 설계되었다.*

  <div align="center">
  <img src="../../assets/images/2024-01-16-InternVL/Fig1.jpg" alt="Figure_1" style="zoom:90%;"/>
  <img src="../../assets/images/2024-01-16-InternVL/Fig2.jpg" alt="Figure_2" style="zoom:90%;"/>
  </div>

## 2. Related Work
### 2.1. Vision Foundation Models

### 2.2. Large Language Models

### 2.3. Vision Large Language Models

## 3. Proposed Method
### 3.1. Overall Architecture

### 3.2. Model Design

### 3.3. Alignment Strategy

## 4. Experiments
### 4.1. Implementation Details

### 4.2. Visual Perception Benchmarks

### 4.3. Vision-Language Benchmarks

### 4.4. Multi-Modal Dialogue Benchmarks

### 4.5. Ablation Study

## 5. Conclusion
