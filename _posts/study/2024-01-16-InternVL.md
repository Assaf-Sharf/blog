---
layout: single
title: "InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks"
permalink: /studies/paper/InternVL
tags: [Paper, LVLM]
categories:
  - 📄 paper
date: 2024-01-16
use_math: true
---
*대규모 언어 모델(LLMs)의 기하급수적 성장은 멀티모달 AGI 시스템에 대한 수많은 가능성을 열어주었다. 그러나 비전 및 비전-언어 기반 모델의 발전, 멀티모달 AGI의 중요한 요소로서, LLMs의 발전 속도를 따라가지 못하고 있다. 이 연구에서는 대규모 비전-언어 기반 모델(InternVL)을 설계하였다. 이는 비전 기반 모델을 60억 매개변수로 확장하고 다양한 출처의 웹 규모 이미지-텍스트 데이터를 사용하여 점진적으로 LLM과 조화시킨다. 이 모델은 이미지 수준 또는 픽셀 수준 인식과 같은 시각 인식 작업, 제로샷 이미지/비디오 분류, 제로샷 이미지/비디오-텍스트 검색과 같은 비전-언어 작업에 널리 적용되어 최첨단 성능을 달성할 수 있으며, LLM과 연결하여 멀티모달 대화 시스템을 생성할 수 있다. 이 모델은 강력한 시각 능력을 가지고 있으며, ViT-22B의 좋은 대안이 될 수 있다.*

## 📋 Table of Contents

- [1. Introduction](#1-introduction)
- [2. Related Work](#2-related-work)
- [3. Proposed Method](#3-proposed-method)
- [4. Experiments](#4-experiments)
- [5. Conclusion](#5-conclusion)

## 1. Introduction

## 2. Related Work
### 2.1. Vision Foundation Models

### 2.2. Large Language Models

### 2.3. Vision Large Language Models

## 3. Proposed Method
### 3.1. Overall Architecture

### 3.2. Model Design

### 3.3. Alignment Strategy

## 4. Experiments
### 4.1. Implementation Details

### 4.2. Visual Perception Benchmarks

### 4.3. Vision-Language Benchmarks

### 4.4. Multi-Modal Dialogue Benchmarks

### 4.5. Ablation Study

## 5. Conclusion
