---
categories: 
   - Linux
tags:
   - Linux 내부 구조
   - Linux 1.2
last_modified_at: 2020-01-15T17:00:00
---
> 개인이 공부하고, 정리한 걸 기록하는 공간입니다.
**오타, 오류**가 존재할 수 있습니다. 댓글을 달아주시면 수정할 수 있도록 하겠습니다.

---
title: Linux 내부 구조 정리 - (2)
categories: 
   - Linux
tags:
   - Linux 내부 구조
   - Linux 1.2
author_profile: true #작성자 프로필 출력여부
read_time: true # read_time을 출력할지 여부 1min read 같은것!

toc: true #Table Of Contents 목차 보여줌
toc_label: "My Table of Contents" # toc 이름 정의
<!-- toc_icon: "cog" #font Awesome아이콘으로 toc 아이콘 설정 -->
toc_sticky: true # 스크롤 내릴때 같이 내려가는 목차

date: 2020-01-15T17:00:00 # 최초 생성일
last_modified_at: 2020-01-17T12:00:00 # 마지막 변경일

comments: true  # 댓글 시스템 이용
---

<!-- intro -->
{% include intro %}

해당 포스팅은 지속적으로 업데이트 될 수 있습니다.
{: .notice--success}

# CHAPTER 01 모르면 손해본다! <br>꼭 알아두면 좋은 Linux 내부 구조
## 1.2 디스크와 파일에 관련된 이것저것
### 1.2.2 오래됐지만 새로운 파티션 테이블
■	**4KB 섹터 디스크**

기존의 하드 디스크는 1섹터의 사이즈가 512byte로 정해져 있다.
하지만, 대용량 디스크는 섹터의 사이즈가 커지므로 섹터 단위 접근이 오버헤드를 줄일 수 있다.
그리고 하드 디스크 내부에는 섹터마다 에러 체크용 정보가 저장되어 있는데 섹터 사이즈를 크게 함으로써 부가 정보가 차지하는 비율을 줄이고 기록 영역을 효과적으로 사용할 수 있다.
{% capture comment1 %} 
즉, **섹터 사이즈**를 키움으로서 **두 가지의 이득**이 있다.
  1. 섹터 사이즈가 작을 경우 여러번 접근해야하는 것을 섹터 사이즈가 큼으로서 한번만 접근해도 되게 된다. 
  즉, 접근에 대한 오버헤드가 준다.
  2. 에러 체크하는 정보가 동일하게 섹터마다 들어있다고 할때 섹터의 사이즈가 크다면 작은 사이즈 일때 보다 중복적인 부가 정보가 적어지게 된다.
{% endcapture %}

{% include notice--info title="정리 & 요약" content=comment1%}

하지만, 하드 디스크에 접근하는 서버, HW, OS(디바이스 드라이버)의 경우 512byte의 섹터 크기를 전제로 설계되어 있다. 단순히 하드 디스크에서 섹터 사이즈를 크게 한다고 해결되는 문제가 아니다. 이 문제를 해결하기 위해서 사용한 방식은 하드 디스크에 내장된 컨트롤러가 논리적으로 512byte의 섹터를 에뮬레이션하는 방식이다.

![](https://user-images.githubusercontent.com/49507736/72482500-4501d880-3841-11ea-9c0a-2ea52823a73d.png)

그림과 같이 서버에서는 기존 방식과 동일한 512byte의 섹터가 존재하는 것처럼 보인다. 하지만 **실제로는 4KB 섹터로 실제로는 읽고 쓰기**를 한다.

이것은 얼핏보면 사이즈를 교환하기 위해 오버헤드가 발생하는 것처럼 보인다.
4KB 섹터 가운데 논리 섹터만을 읽고 쓰는 경우, 512byte의 데이터를 읽고 쓰기 위해 4KB 데이터의 읽고 쓰기가 발생한다. 하지만, 실제로 읽고 쓰기 패턴이 4KB 섹터의 처음부터 시작해 어느 정도 큰 사이즈의 데이터를 다룰 경우에 이런 오버헤드는 사라진다.

{% capture comment2 %} 
이해하기 쉽게 풀어쓰자면, 기존에 1섹터(512byte) 하나만 읽더라도 읽기 위해서는 4KB 섹터를 가져와야한다. 
이 경우 4KB - 512byte 만큼의 오버헤드가 발생한다. 하지만 가져오는 데이터의 크기가 커질 경우 그 오버헤드는 작아진다.
ex) 18KB가 필요한 경우 4KB * 4 + 2KB 이자만 총 5개의 4KB 섹터를 가져온다. 4개의 섹터는 오버헤드가 없고 마지만 섹터에만 2KB의 오버헤드가 발생한다.
이는 **필요한 데이터가 점점 커질 수록 오버헤드는 점점 작아진다.**
{% endcapture %}

{% include notice--info title="정리 & 요약" content=comment2%}


앞에서 본 디스크 부트 /dev/sda1의 시작 위치는 2048섹터부터 시작된다.
이것은 4KB 섹터라고 생각하면, 256번째의 4KB 섹터 시작점과 일치한다.(2048 / 8 = 256)
/dev/sda2, /dev/sda3도 동일하게 8의 배수이다. 인스톨러는 꼼꼼하게 이러한 조건을 고려하고 있는 것이다.
물론, 이것은 4KB 섹터를 채용하고 있는 경우 하드 디스크에만 적용되는 주의 사항이다. 이렇게 파티션의 시작 위치와 사이즈를 8의 배수로 준비하는 것을 **파티션의 정렬(alignment)**이라고 한다.

parted 커맨드로 GPT 형식의 파티션을 생성할 때는 기본적으로 파티션의 시작 위치와 종료 위치를 용량으로 지정한다. 이때 parted 커맨드는 자동으로 파티션을 정렬하며 생성한다. 혹시나 그래도 파티션이 걱정된다면 `unit s`를 커맨드로 파티션의 시작 위치와 종료 위치를 섹터 단위로 표시/지정해 두면 된다.

{% capture comment3 %}
실제로는 4KB 섹터이든, 아니든 서버에서는 512byte의 섹터를 전제로 하며, 이를 위해서는 파티션을 생성할 때 4KB 섹터를 다룰 수 있도록 (512byte * 8 = 4KB) 섹터를 8배수로 짜주는 것이 좋다.
그렇게 짜지 않더라도 parted 커맨드는 자동으로 파티션 정렬을 통해 8배수 생성해준다.
{% endcapture %}

{% include notice--info title="정리 & 요약" content=comment3%}

### 1.2.3 파일 시스템과 I/O 서브시스템을 생각하다
#### ■	파일 시스템의 블록 사이즈

Linux에는 파일 시스템을 전체적으로 이용하기 위한 도구인 `VFS(Virtual File System, 가상 파일 시스템)레이어`와 디바이스 드라이버를 사용해 물리 디스크에 데이터의 읽기/쓰기를 담당하는 `블록 레이어`가 있다.

![](https://user-images.githubusercontent.com/49507736/72482560-79759480-3841-11ea-8956-46fdb11de316.png)

`파일 시스템의 블록 사이즈`는 블록 레이어에 따른 디바이스 드라이버가 물리 디스크에 데이터를 읽고 쓰는 **최소 단위**이다.
물리 디스크의 설계상 512byte의 섹터 단위로 데이터를 읽 쓰곤 하지만, 대부분의 경우 더욱 **큰 단위로 데이터를 읽고 쓰는 것이 효율적인 데이터 처리**를 할 수 있다. 이 단위를 지정하는 것이 **블록 사이즈**이다.

Linux에서는 파일 시스템의 블록 사이즈로 1024byte, 2048byte, 4096byte를 선택할 수 있다. 디폴트 블록 사이즈는 /etc/mke2fx.conf에 기록되어 있지만 `mke2fs 커맨드의 -b 옵션`으로 **사이즈를 바꿀 수 도 있다.** **이미 생성된 파일 시스템에 설정되어 있는 블록 사이즈를 확인**할 때는 `tune2fs 커맨드`를 사용한다.

이전 장에서는 4KB 섹터 디스크의 경우, 파티션의 시작 위치를 4KB 섹터의 앞부분에 갖추고 블록 사이즈도 4KB(4096byte)로 하는 것이 좋다고 설명했다.
물리 디스크는 4KB 섹터의 일부분만을 바꿀 수 없기 때문이다.
밑의 그림을 보면 이해하기가 쉽다.

![](https://user-images.githubusercontent.com/49507736/72482564-7da1b200-3841-11ea-80d6-5d997e3beaf2.png)

첫 번째 그림은 파티션의 시작 위치와 4KB 섹터의 시작 위치를 맞춘 상태이다.
이런 경우 물리 디스크가 실제로 4KB 섹터 단위로 정확하게 접근하여 쓸데없는 동작이 발생하지 않는다.

![](https://user-images.githubusercontent.com/49507736/72482568-7f6b7580-3841-11ea-8409-98c779416ed7.png)

하지만, 두 번째 그림의 경우 파티션의 시작 위치와 4KB 섹터의 시작위치가 맞지 않는다. 이럴 경우 **드라이버는 63블록과 64블록을 읽어오는 것이 아니라** 디스크는 **4KB 섹터 단위로 읽기 때문에 60, 61, 62, 63 블록(= 물리적인 한 덩어리 4KB 섹터)을 한번에 가져온 뒤 63블럭(=논리적인 단위인 1섹터(512byte))만 읽고 나머지를 버린 후, 다시 64, 65, 66, 67블록을 가져와서 읽는다.**
이렇게 파티션의 시작 위치가 다른 경우 두번의 불필요한 접근이 발생하고, 이는 명백한 오버헤드라고 할 수 있다.

위의 경우와는 다르지만 512byte를 1섹터(컴퓨터가 주소지정을 할 수 있는 최소의 단위 저장공간)로 사용하는 디스크에서도 오버헤드가 발생할 수 있는데, 이는 파티션의 전체 사이즈를 블록 사이즈(디스크 드라이버가 디스크에 읽고 쓰는 최소 단위)의 정수배로 저장하지 않는 경우다.
파티션이 블록 사이즈의 정수배가 아니라면 마지막 몇 섹터는 읽지 못할 것이다.(읽거나 쓰게 된다면 다른 파티션 영역까지 읽거나 쓰는 것이니)
그렇기 때문에 마지막 남는 섹터는 파일 시스템에서 사용하지 않고 남게된다.

그리고 `fdisk 커맨드`의 출력을 보면 블록 수 끝에 + 기호가 붙어 있는 때가 있다. 위에서의 /dev/sda4의 블록 수에서도 붙어 있음을 확인할 수 있다. 이것은 **파티션의 포함된 섹터 수가 홀수의 경우에 이 기호가 붙도록 되어 있기 때문**이다.
이유는 **예전 Linux 파일 시스템의 블록 사이즈는 1024byte가 표준**이였고, 따라서 **블록 수가 홀수 일 경우 마지막 섹터(512byte)는 쓰일 수 가 없었기 때문**에 +기호는 "이 파티션은 쓰지 않아요~" 라는 것을 표시하기 위해 일부러 붙인 것이다. 지금은 필요없는 표시이다.

{% capture comment4 %}
오버헤드가 발생하는 2가지 경우를 배웠다.
섹터는 디스크에 저장되는 최소한의 저장 공간 단위이고, 블록은 디바이스 드라이버가 디스크에 한번에 저장하는 최소한의 저장 단위이다.
  1.  물리 디스크가 4KB 섹터일 경우 파티션의 시작 위치가 4KB 섹터 와 맞을 땐, 상관없지만 파티션 시작 위치가 4KB 섹터와 맞지 않을 땐 오버헤드가 발생한다.
  왜냐하면, 디스크에 2번의 접근이 발생하기 때문이다.
  2번의 접근이 발생하는 이유는 서버나 OS(디바이스 드라이버)는 섹터가 512byte라고 전제로 설계가 되었기 때문에 512byte 기준으로 접근을 한다.
  하지만 실제 물리 디스크에는 4KB 단위로 읽고 쓸 수 있기 때문에 필요없는 필요 없는 부분까지 읽어 온뒤 필요한 부분을 읽/쓰고 버린고, 다시 다음 섹터를 읽/쓰는 행위를 한다. 이때 2번의 불필요한 접근일 발생한다.
  
  2. 실제 물리 디스크가 512byte 섹터를 가지고 있더라고 하더라도 파티션의 크기가 블록 사이즈의 정배수가 아닐 경우 몇 개의 섹터가 삐져나오게 된다.
  **파티션 전체 사이즈 = 블록 사이즈 * N + 몇 개의 섹터**
  이 경우 몇 개의 섹터를 이용할 수 없는데, 이유는 **몇 개의 섹터**를 읽/쓰기 위해서는 블록 사이즈가 파티션의 범위를 초과해야 하기 때문이다.
  그렇기 때문에 마지막 블록은 사용하지 않고 **몇 개의 섹터**는 남겨둔다.

2번의 경우 때문에 `fdisk 커맨드` 출력 시 파티션 사이즈 뒤에 +가 붙는 경우가 있는데 이는 사이즈가 홀수 인 경우 붙는다.
기존 Linux는 블록 사이즈가 1024byte로 고정이였고, 섹터는 512byte였는데 섹터 개수가 홀수 인 경우 필연적으로 한 개의 섹터는 사용하지 못했기 때문이다. 
즉, `+`는 **사용하지 못하는 섹터가 존재한다는 메세지**이다. 지금은 필요없는 표시다.
{% endcapture %}

{% include notice--info title="정리 & 요약" content=comment4%}

#### ■	I/O 서브시스템의 전체 모습
![위의 그림](#■-	파일-시스템의-블록-사이즈)으로 I/O 서브 시스템의 구조에 대해 알아보자.
- 데이터를 쓰는 경우
	1. _파일 시스템에서  데이터를 쓸 경우, 그 내용을 먼저 메모리의 디스크 캐시에 쓴다._ 데이터의 읽고 쓰기를 요청한 애플리케이션은 이 단계에서 읽거나 쓰는 단계를 마쳤다고 생각하고 처리를 계속한다.
	2. 캐시에만 등록되어 있고, 물리 디스크에는 아직 기록되지 않은 데이터가 생기게 되고, 이를 `더티 데이터` 라고 부른다. _디스크 캐시에 더티 데이터가 어느 정도 쌓이게 되면 파일 시스템은 I/O 스케줄러에게 더티 데이터를 물리 디스크에 기록하도록 요청_한다.
	3.  이 요청은 I/O 스케줄러의 `리퀘스트 큐`에 추가된다.
	4.  I/O 스케줄러는 `리퀘스트 큐`에 쌓여 있는 요청에 응답하여 디스크 드라이브를 이용해 물리 디스크에 기록한다.
- 데이터를 읽는 경우
	- 대상이 되는 데이터가 이미 디스크 캐시에 존재하는 경우
		1. 해당 데이터를 프로세스에 반환하고 마친다.
	- 대상 데이터가 디스크 캐시에 없는 경우
		1. 호출을 한 프로세스는 I/O 를 기다리는 상태가 되어 멈춘다.
		2. 파일 시스템은 I/O 스케줄러에 데이터의 읽기/쓰기를 요청한다.
		3. I/O 스케줄러가 요청에 응답하여 디스크 드라이브를 이용해 물리 디스크의 데이터를 읽어온다.
		4. I/O 스케줄러는 프로세스의 I/O 대기 상황을 해제한 뒤 디스크 캐시의 데이터를 받아서 전달한다.

{% capture comment5 %}
- 데이터를 쓰는 경우
- 데이터를 읽는 경우
	- 데이터가 디스크 캐시에 존재하는 경우
	- 데이터가 디스크 캐시에 존재하지 않는 경우:
		디스크 캐시에 존재하도록 만든다.

 데이터를 쓰는 경우는 프로세스가 `System Call Interface`를 통해 요청을 하면 VFS Layer에 있는 파일 시스템이 디스크 캐시에 데이터를 적재(더티 데이터)한다.
일정량 이상 적재될 경우 파일 시스템은 I/O 스케줄러에게 물리 디스크에 기록할 것을 요청하고, I/O 스케줄러는 내부의 리퀘스트 큐에 추가한다.
I/O 스케줄러는 리퀘스트 큐에 있는 요청을 디스크 드라이버를 이용하여 순서대로 처리한다.

 데이터를 읽는 경우는 이미 디스크에 캐시가 존재한다면, 바로 프로세스에게 반환해주면 된다. 
만약 존재하지 않는다면, 프로세스는 대기 상태가 되고, 파일 시스템은 I/O 스케줄러에게 읽기를 요청한다. 
I/O 스케줄러가 디스크 드라이버를 이용하여 물리 디스크의 데이터를 읽어 온뒤 디스크 캐쉬에 저장한 뒤 프로세스에게 반환한다.
{% endcapture %}

{% include notice--info title="정리 & 요약" content=comment5 %}

극단적인 예로, 물리 디스크가 없는 파일 시스템도 만들 수 있다.
ramfs 는 메모리를 이용한 RAM 디스크의 기능을 제공하는 파일 시스템이다.
이 파일 시스템은 데이터 쓰기의 2번( 데이터 전송 리퀘스트 )를 하지 않는다.
데이터는 디스크 캐쉬에 모이게 된다.
일반 파일 시스템이라면, 필요에 따라 물리 디스크에 저장된 클린 데이터를 디스크 캐시에서 제거함으로서 용량을 확보한다.
반면, ramfs는 디스크 캐시의 데이터가 영원히 더티 데이터로 


