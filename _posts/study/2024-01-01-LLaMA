---
layout: single
title: "LLaMA: Open and Efficient Foundation Language Models"
tags:
  - Paper
categories:
  - 📄 paper

use_math: true
---

1. Introduction
 - 모델의 규모를 더 크게 확장하는 것이 성능 향상에 중점을 둔 연구의 방향이 되었다. (Chowdhery et al., 2022; Rae et al., 2021)
 - Hoffmann et al. (2022)의 연구에서는 주어진 컴퓨팅 예산에 대해 가장 큰 모델이 아닌, 더 많은 데이터에 대한 학습이 최고 성능을 달성할 수 있음을 보여줬다.
 - 큰 모델을 학습하는 것보다 작은 모델을 더 오래 학습하는 것이 추론 비용을 줄일 수 있다. (서비스 시 중요도: 학습 시간 < 추론 시간)
 - LLaMA 모델은 다양한 추론 예산에 대해 최적의 성능을 달성하도록 개발되었으며, 7B에서 65B 매개변수 범위에 걸쳐 있다.
 - LLaMA-13B는 GPT-3보다 우수한 성능을 보이며 단일 GPU에서 실행 가능하고, LLaMA-65B는 Chinchilla와 PaLM-540B와 비교하여 경쟁력 있는 성능을 제공한다.
 - Chinchilla, PaLM, 또는 GPT-3와 달리 LLaMA는 공개 데이터를 사용하여 훈련되며, 이는 모델의 공개와 연구를 위한 접근성을 높인다.
 - 트랜스포머 아키텍처에 대한 수정 사항과 학습 방법을 소개한다.
 - 모델에 인코딩된 편향과 독성을 평가하고, 이를 최신 벤치마크 세트에서 다른 LLM과 비교한다.
 - 향후 학습 데이터셋과 모델 크기의 확장을 통한 성능 개선 가능성 제시한다.

2. Approach
 2.1 Pre-training Data
 - 학습 데이터셋은 여러 소스를 혼합하여 다양한 도메인을 포함했다.(Table 1)
 - 공개적으로 사용 가능하고, 오픈 소싱과 호환되는 데이터만 사용했다.
 - 데이터 구성 요소:
  ① English CommonCrawl [67%]: 2017년부터 2020년까지의 CommonCrawl 덤프를 CCNet 파이프라인을 통해 전처리했다., fastText linear classifier로 비영어 페이지를 제거하고, n0gram 언어 모델로 저품질 콘테츠를 필터링했다.
  ② C4 [15%]: C4 데이터셋(Raffel et al., 2020) 포함했다., C4의 전처리는 중복 제거와 언어 식별 단계가 포함되었다., 구두점의 존재나 웹페이지의 단어 및 문장 수와 같은 휴리스틱에 주로 의존한다.
  ③ GitHub [4.5%]: Google BigQuery에서 제공하는 공개 GitHub 데이터셋 사용했다. 라인 길이나 영숫자 문자의 비율에 기반한 휴리스틱을 사용하여 저품질 파일을 필터링하고, 정규 표현식으로 헤더와 같은 보일러플레이트(boilerplate)를 제거한다.
  ④ Wikipedia [4.5%]: 2022년 6월-8월 기간의 위키피디아 덤프를 사용했다. 라틴 및 키릴 문자를 사용하는 20개 언어 포함한다. 하이퍼링크, 댓글, 기타 서식 보일러플레이트를 제거하여 데이터를 처리했다.
  ⑤ Gutenberg와 Books3 [4.5%]: 공개 도메인의 Gutenberg 프로젝트와 ThePile의 Books3 섹션을 포함했다. 90% 이상 내용이 중복되는 책을 제거하였다.
  ⑥ ArXiv [2.5%]: 과학적 데이터를 위해 ArXiv의 LaTeX 파일 처리했다. 첫 번째 섹션 이전의 모든 것과 참고 문헌을 제거했다.
  ⑦ Stack Exchange [2%]: Stack Exchange의 덤프를 포함했다. 가장 큰 28개 웹사이트에서 데이터를 유지하고, 텍스트에서 HTML 태그를 제거하였다. 다양한 도메인의 고품질 질문과 답변을 포함한다.
 - 데이터는 바이트쌍 인코딩(BPE) 알고리즘을 사용하는 토크나이저를 사용했다.
 - 전체 학습 데이터셋 규모는 토큰화 후 약 1.4T 토큰을 포함한다.
 - 대부분의 학습 데이터는 학습 중 한 번만 사용되었고, 위키피디아와 책 도메인에서만 약 두 번의 epoch을 진행했다.