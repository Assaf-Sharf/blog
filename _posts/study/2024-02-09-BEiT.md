---
layout: single
title: "BEiT-3: Image as a Foreign Language: BEIT Pretraining for All Vision and Vision-Language Tasks"
permalink: /studies/paper/BEiT-3
tags: [Paper, LVLM]
categories:
  - 📄 paper
date: 2024-02-09
use_math: true
published: false
---
*언어, 비전, 그리고 멀티모달 사전학습의 큰 융합이 등장하고 있다. 본 논문에서는 비전 및 비전-언어 작업 모두에서 최고의 전이 성능을 달성하는 범용 멀티모달 기반 모델 BEIT-3를 소개한다. 구체적으로 백본 아키텍처, 사전학습 작업, 그리고 모델 스케일링 업의 세 가지 측면에서 큰 융합을 발전시킨다. 일반적인 목적의 모델링을 위한 멀티웨이 트랜스포머를 소개한다. 여기서 모듈식 아키텍처는 깊은 융합과 모달리티 특정 인코딩을 모두 가능하게 한다. 공유된 백본을 기반으로, 이미지(Imglish), 텍스트(English), 그리고 이미지-텍스트 쌍("병렬 문장")에 대해 통일된 방식으로 마스크된 "언어" 모델링을 수행한다. 실험 결과에 따르면 BEIT-3는 객체 탐지(COCO), 의미 분할(ADE20K), 이미지 분류(ImageNet), 시각적 추론(NLVR2), 시각적 질문 응답(VQAv2), 이미지 캡셔닝(COCO), 그리고 크로스-모달 검색(Flickr30K, COCO)에서 최고의 성능을 보여준다.*

## 📋 Table of Contents

- [1 Introduction: The Big Convergence](#1-introduction-the-big-convergence)
- [2 BEIT-3: A General-Purpose Multimodal Foundation Model](#2-beit-3-a-general-purpose-multimodal-foundation-model)
- [3 Experiments on Vision and Vision-Language Tasks](#3-experiments-on-vision-and-vision-language-tasks)
- [4 Conclusion](#4-conclusion)

## 1 Introduction: The Big Convergence

## 2 BEIT-3: A General-Purpose Multimodal Foundation Model
### 2.1 Backbone Network: Multiway Transformers

### 2.2 Pretraining Task: Masked Data Modeling

### 2.3 Scaling Up: BEIT-3 Pretraining
- Backbone Network
   - 

- Pretraining Data
   - 

- Pretraining Settings
   - 

## 3 Experiments on Vision and Vision-Language Tasks
### 3.1 Vision-Language Downstream Tasks
- Visual Question Answering (VQA)
   - 

- Visual Reasoning
   - 

- Image Captioning
   - 

- Image-Text Retrieval
   - 

### 3.2 Vision Downstream Tasks
- Object Detection and Instance Segmentation
   - 

- Semantic Segmentation
   - 

- Image Classification
   - 

## 4 Conclusion