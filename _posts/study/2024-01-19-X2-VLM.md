---
layout: single
title: "X2-VLM: All-In-One Pre-trained Model For Vision-Language Tasks"
permalink: /studies/paper/X2-VLM
tags: [Paper, LVLM]
categories:
  - 📄 paper
date: 2024-01-19
use_math: true
---
*비전 언어 사전 학습은 대량의 데이터로부터 비전과 언어 간의 정렬을 학습하는 것을 목표로 한다. 기존의 대부분의 방법들은 이미지-텍스트 정렬만을 학습하지만, 일부 다른 방법들은 사전 학습된 객체 감지기를 활용하여 객체 수준에서 비전 언어 정렬을 활용한다. 본 논문에서는 다중 정밀도 비전 언어 정렬을 통합 사전 학습 프레임워크를 통해 동시에 다중 정밀도 정렬 및 다중 정밀도 위치 결정을 학습하는 것을 제안한다. 이를 바탕으로 이미지-텍스트 사전 학습과 비디오-텍스트 사전 학습을 하나의 모델에서 통합하는 유연한 모듈식 아키텍처를 가진 X2-VLM이라는 모든 것을 포함하는 모델을 제시한다. X2-VLM은 다양한 텍스트 설명과 연관된 무한한 시각적 개념을 학습할 수 있다. 실험 결과에 따르면 X2-VLM은 이미지-텍스트 및 비디오-텍스트 작업 모두에서 기본 및 대규모 규모에서 최고의 성능을 보여주며, 성능과 모델 규모 사이에서 좋은 절충을 이룬다. 또한, X2-VLM의 모듈식 설계는 어떤 언어나 도메인에서도 활용될 수 있도록 높은 전이성을 제공한다. 예를 들어 텍스트 인코더를 XLM-R로 간단히 교체함으로써, X2-VLM은 어떠한 다국어 사전 학습 없이도 최신의 다국어 다중 모달 사전 학습 모델들을 능가한다. 코드와 사전 학습된 모델은 [github.com/zengyan-97/X2-VLM](https://github.com/zengyan-97/X2-VLM){:target="_blank"}에서 이용 가능하다.*

## 📋 Table of Contents

- [1. Introduction](#1-introduction)
- [2. Related Work](#2-related-work)
- [3. Method](#3-method)
- [4. Experiment](#4-experiment)
- [5. Conclusion and Discussion](#5-conclusion-and-discussion)
- [A Appendix](#a-appendix)

## 1. Introduction
 - 비전 언어 사전 학습(Vision language pre-training)은 다수의 이미지-텍스트 또는 비디오-텍스트 쌍에서 비전 언어 정렬을 학습하는 것을 목표로 한다.
 - 기존 연구의 접근법은 크게 Coarse-grained와 Fine-grained로 구분된다.
 - Coarse-grained 접근법은 전체 이미지 특징을 인코딩하는 데 사용되지만, 세부적인 시각-언어 정렬 학습에 어려움이 있다.
 - Fine-grained 접근법은 이미지 인코더로 사전 학습된 객체 탐지기를 사용하여 미세한 정렬을 학습하지만, 다수의 객체 간 관계를 인코딩할 수 없고 제한된 수의 객체 클래스만 인식할 수 있다.
 - multi-grained alignments 학습의 챌린지
   - 1) multi-grained 정렬을 학습할 데이터 유형
   - 2) 다양한 데이터 유형을 통합적으로 집계하는 방법
   - 3) multi-grained 시각적 개념을 단일 비전 인코더로 표현하는 방법
   - 4) 데이터에서 효율적으로 multi-grained 비전 언어 정렬을 학습하는 방법
 - 본 논문에서는 모든 비전 언어 작업에 대해 동시에 multi-grained 정렬을 학습할 수 있는 통합 프레임워크로 사전 학습된 all-in-one VLM. 즉, X2-VLM을 제안한다.
 - 이를 위해 이미지에 대한 객체 레이블(ex. “남자” 또는 “배낭”), 이미지에 대한 영역 레이블(ex. “배낭을 메고 있는 소년”), 이미지에 대한 텍스트 설명(ex. “학교 첫날은 학생과 부모 모두에게 복합적인 감정을 준다”) 등 세 가지 유형의 데이터를 사용한다.
 - 모델은 시각적 개념을 클래스 라벨 대신 다양한 텍스트로 설명된 무한한 시각적 개념을 통합된 방식으로 학습할 수 있다.
 - X2-VLM의 모듈러 구조는 비전, 텍스트, 융합의 세 가지 모듈로 구성되며, 모두 트랜스포머 기반으로 되어 있다.
 - 비전 트랜스포머를 사용하여 이미지를 인코딩하고, 패치 기능을 사용하여 이미지 내의 multi-grained 시각적 개념을 표현한다.
 - 비디오-텍스트 사전 학습 확장 가능성이 있다.
 - 비디오 프레임 샘플링 및 비전 트랜스포머로 각 프레임을 인코딩하여 모델이 시간적 차원에서의 시각적 개념 이해 및 다양한 VLM 학습을 지원한다.
 - 영어 데이터에 대한 비전 언어 사전 학습 후 다른 언어/도메인 특정 텍스트 모듈(XLM-R)로 교체 가능한 모듈식 구조 유연성을 갖는다.
 - 다국어 비전-언어 작업에서 최고 성능 방법을 능가한다.
 - X2-VLM의 효과성 검증을 위한 실험 수행했고, 이미지 캡션 생성, 교차 모달 이해 작업에서 우수한 성능을 갖는다.
 - Fig 1 (a)와 같이 X2-VLM은 성능과 모델 크기 사이에서 좋은 절충을 이루고 있다.
 - Fig 2 (b)와 같이 비디오-텍스트 검색과 비디오 VQA 두 유형의 작업에서 최신 성능을 달성한다.
 - 논문의 주요 기여도
   - multi-grained 비전 언어 정렬을 동시에 학습하고 위치시키는 통합 사전 학습 프레임워크 제안한다.
   - 이미지-텍스트 및 비디오-텍스트 작업 처리 가능한 전체 사전 학습된 VLM인 X2-VLM 제시한다.
   - 대규모 데이터 및 큰 모델 크기로 확장 가능한 프레임워크로 확인되었다.
   - 다양한 언어와 도메인에서 사용 가능한 X2-VLM의 모듈식 디자인 잠재력 드러냈다.

 <div align="center">
 <img src="../../assets/images/2024-01-19-X2-VLM/Fig1.jpg" alt="Figure_1"  style="zoom:80%;"/>
 </div>

## 2. Related Work
### 2.1 Image-Text Pre-training
 - Fine-grained 접근 방식은 객체에 대한 레이블을 기반으로 사전 학습된 객체 탐지기를 이미지 인코더로 사용한다.
 - 객체 탐지기는 객체가 포함된 것으로 보이는 모든 영역을 식별한 후 각 영역에 대해 객체 분류를 수행한다.
 - 이미지는 식별된 영역의 수십 개의 객체 중심 특성으로 표현되며, 다양한 영역의 여러 객체 간 관계를 표현할 수 없다.
 - 객체 탐지기는 일반적인 객체만 감지할 수 있어, 실제 응용에서 다양한 시각적 개념을 인코딩하는 데 제한적이다.(ex. Pepsi와 Coca Cola 또는 Audi와 BMW 구별하기 힘듦)
 - Coarse-grained 접근 방식은 컨볼루션 네트워크나 비전 트랜스포머를 사용하여 전체 이미지 특성을 추출하고 인코딩한다.
 - 최신 비전 트랜스포머(e.g., Swin-Transformer, BEiT-2)를 사용하는 METER 및 VL-BEiT와 같은 최근 방법은 가장 강력한 fine-grained을 능가할 수 있다.
 - 일부 방법들은 객체 수준과 이미지 수준의 정렬을 모두 학습하려고 시도한다.
 - 그러나 이러한 접근 방식은 여전히 객체 탐지기에 의존하므로 앞서 언급한 문제에 직면한다.
 - 본 연구의 multi-grained 비전 언어 사전 학습 프레임워크는 객체 탐지에 의존하지 않고 객체 수준이나 이미지 수준에 제한되지 않는 통합된 방식으로 비전 언어 정렬을 학습한다.

### 2.2 Video-Text Pre-training
 - 대부분의 기존 VLM은 이미지-텍스트 작업만 다루고, 소수의 VLM만 비디오-텍스트 사전 학습을 수행한다.
 - 비디오-텍스트 모델은 여러 이미지로 구성된 비디오의 특성상 이미지-텍스트 모델과 많은 유사점을 공유한다.
 - 비디오-텍스트 사전 학습은 이미지-텍스트 사전 학습과 유사점을 공유하지만, 기존 방법은 두 유형의 작업 모두에서 최신 성능을 달성하지 못했다.
 - 대표적인 비디오-텍스트 사전 학습 작업으로는 ClipBERT, Frozen, ALPRO, VIOLET, All-in-one 등이 있다.
 - 최근에는 OmniVL이 제안되어 이미지-텍스트 작업과 비디오-텍스트 작업 모두를 지원한다.
 - OmniVL은 비디오에 대해 3D 패치 임베딩을 사용하고 이미지에 대해서는 2D 패치 임베딩을 사용하며, 비전 인코딩에 TimeSformer를 채택한다.

### 2.3 Multilingual Multi-modal Pre-training
 - 다국어 다중 모달 사전 학습(Multilingual multi-modal pre-training)은 다중 모달 모델을 비영어 텍스트에 적용할 수 있게 하려는 목표를 가진다.
 - 다국어 사전 학습(multilingual pre-training)과 다중 모달 사전 학습(multi-modal pre-training)에서 사용할 수 있는 상대적으로 많은 병렬 데이터와 달리, 다국어 다중 모달 코퍼스는 소수이며 제한된 언어 범위를 갖는다.
 - $M^3P$, $UC^2$, MURAL, CCLM 등과 같은 방법들은 추가 데이터를 필요로 한다.
 - 대조적으로 X2-VLM은 모듈식 구조의 잠재력을 활용하여 다국어 다중 모달 사전 학습 과정 없이도 다국어 V+L 작업에 적응할 수 있다.

## 3. Method
### 3.1 Overview
 - **Architecture**
   - X2-VLM은 비전, 텍스트, 다중 모달 융합 모듈(multi-modal fusion modules)로 구성된다.
   - 모든 모듈은 Transformer 기반으로 되어 있다.
   - 융합 모듈은 텍스트 특성을 입력으로 받아 각 층에서 cross-attention를 통해 비전 특성과 텍스트 특성을 융합한다.
   - 사전 학습에서 이 세 모듈은 인코더로 작동하며, 이미지 캡션 생성을 위한 실험에서 보여진 것처럼 텍스트 및 융합 모듈은 left-to-right self-attention를 적용하여 생성 작업에도 적용될 수 있다.
   - Fig 3은 X2-VLM의 구조와 multi-grained 정렬 및 위치 지정 방법을 보여준다.
 - **Data**
   - X2-VLM은 이미지-텍스트 쌍, 비디오-텍스트 쌍, 객체 및 영역에 대한 이미지 주석을 포함하여 모든 시각적 개념을 텍스트 설명과 연관시키는 통합 접근 방식이다. 즉, 하나의 이미지는 여러 시각적 개념을 포함할 수 있으며, 각각은 텍스트 설명과 연관된다.

### 3.2 Unified Vision Encoding
시각적 개념 표현: X2-VLM은 비전 변환기를 한 번만 전달하여 이미지 내 모든 multi-grained 시각적 개념을 효율적으로 얻는 방법을 제안한다. 우리는 이미지를 패치 특성으로 처리한 후, 경계 상자에 해당하는 패치 집합을 집계하여 객체나 영역을 표현한다.
비디오 표현: 비디오는 여러 이미지로 구성되므로, 비디오 인코딩과 이미지 인코딩을 단순하고 효율적인 방식으로 통합한다. 우리는 비디오 프레임을 샘플링하고 각 프레임을 패치 특성으로 인코딩한다. 그런 다음, 각 프레임의 패치 특성에 시간 정보를 추가하고 시간 차원에서 평균을 계산하여 비디오를 표현한다.

### 3.3 Multi-Grained Vision Language Pre-training

#### 3.3.1 Multi-Grained Aligning

#### 3.3.2 Multi-Grained Localization

## 4. Experiment
### 4.1 Pre-training Datasets

### 4.2 Implementation Details

### 4.3 Image-Text Downstream Tasks
#### 4.3.1 Image-Text Retrieval

#### 4.3.2 Visual Question Answering

#### 4.3.3 Visual Reasoning

#### 4.3.4 Visual Grounding

#### 4.3.5 Image Captioning

#### 4.3.6 Winoground

#### 4.3.7 Open-vocabulary Attribute Detection

### 4.4 Video-Text Downstream Tasks

### 4.5 Multilingual Multi-modal Tasks

### 4.6 Ablation Study

### 4.7 Qualitative Study of Multi-Grained Alignments

## 5. Conclusion and Discussion

## A Appendix
### A.1 Pre-training Datasets

### A.2 Implementation Details

### A.3 Ablation Study

### A.4 Qualitative Study of Multi-Grained Alignments