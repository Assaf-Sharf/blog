---
layout: single
title: "X2-VLM: All-In-One Pre-trained Model For Vision-Language Tasks"
permalink: /studies/paper/X2-VLM
tags: [Paper, LVLM]
categories:
  - 📄 paper
date: 2024-01-19
use_math: true
---
*비전 언어 사전 학습은 대량의 데이터로부터 비전과 언어 간의 정렬을 학습하는 것을 목표로 한다. 기존의 대부분의 방법들은 이미지-텍스트 정렬만을 학습하지만, 일부 다른 방법들은 사전 학습된 객체 감지기를 활용하여 객체 수준에서 비전 언어 정렬을 활용한다. 본 논문에서는 다중 정밀도 비전 언어 정렬을 통합 사전 학습 프레임워크를 통해 동시에 다중 정밀도 정렬 및 다중 정밀도 위치 결정을 학습하는 것을 제안한다. 이를 바탕으로 이미지-텍스트 사전 학습과 비디오-텍스트 사전 학습을 하나의 모델에서 통합하는 유연한 모듈식 아키텍처를 가진 X2-VLM이라는 모든 것을 포함하는 모델을 제시한다. X2-VLM은 다양한 텍스트 설명과 연관된 무한한 시각적 개념을 학습할 수 있다. 실험 결과에 따르면 X2-VLM은 이미지-텍스트 및 비디오-텍스트 작업 모두에서 기본 및 대규모 규모에서 최고의 성능을 보여주며, 성능과 모델 규모 사이에서 좋은 절충을 이룬다. 또한, X2-VLM의 모듈식 설계는 어떤 언어나 도메인에서도 활용될 수 있도록 높은 전이성을 제공한다. 예를 들어 텍스트 인코더를 XLM-R로 간단히 교체함으로써, X2-VLM은 어떠한 다국어 사전 학습 없이도 최신의 다국어 다중 모달 사전 학습 모델들을 능가한다. 코드와 사전 학습된 모델은 [github.com/zengyan-97/X2-VLM](https://github.com/zengyan-97/X2-VLM){:target="_blank"}에서 이용 가능하다.*

## 📋 Table of Contents

- [1. Introduction](#1-introduction)
- [2. Related Work](#2-related-work)
- [3. Method](#3-method)
- [4. Experiment](#4-experiment)
- [5. Conclusion and Discussion](#5-conclusion-and-discussion)
- [A Appendix](#a-appendix)

## 1. Introduction
 - 비전 언어 사전 학습(Vision language pre-training)은 다수의 이미지-텍스트 또는 비디오-텍스트 쌍에서 비전 언어 정렬을 학습하는 것을 목표로 한다.
 - 기존 연구의 접근법은 크게 Coarse-grained와 Fine-grained로 구분된다.
 - Coarse-grained 접근법은 전체 이미지 특징을 인코딩하는 데 사용되지만, 세부적인 시각-언어 정렬 학습에 어려움이 있다.
 - Fine-grained 접근법은 이미지 인코더로 사전 학습된 객체 탐지기를 사용하여 미세한 정렬을 학습하지만, 다수의 객체 간 관계를 인코딩할 수 없고 제한된 수의 객체 클래스만 인식할 수 있다.
 - multi-grained alignments 학습의 챌린지
   - 1) multi-grained 정렬을 학습할 데이터 유형
   - 2) 다양한 데이터 유형을 통합적으로 집계하는 방법
   - 3) multi-grained 시각적 개념을 단일 비전 인코더로 표현하는 방법
   - 4) 데이터에서 효율적으로 multi-grained 비전 언어 정렬을 학습하는 방법
 - 본 논문에서는 모든 비전 언어 작업에 대해 동시에 multi-grained 정렬을 학습할 수 있는 통합 프레임워크로 사전 학습된 all-in-one VLM. 즉, X2-VLM을 제안한다.
 - 이를 위해 이미지에 대한 객체 레이블(ex. “남자” 또는 “배낭”), 이미지에 대한 영역 레이블(ex. “배낭을 메고 있는 소년”), 이미지에 대한 텍스트 설명(ex. “학교 첫날은 학생과 부모 모두에게 복합적인 감정을 준다”) 등 세 가지 유형의 데이터를 사용한다.
 - 모델은 시각적 개념을 클래스 라벨 대신 다양한 텍스트로 설명된 무한한 시각적 개념을 통합된 방식으로 학습할 수 있다.
 - X2-VLM의 모듈러 구조는 비전, 텍스트, 융합의 세 가지 모듈로 구성되며, 모두 트랜스포머 기반으로 되어 있다.
 - 비전 트랜스포머를 사용하여 이미지를 인코딩하고, 패치 기능을 사용하여 이미지 내의 multi-grained 시각적 개념을 표현한다.
 - 비디오-텍스트 사전 학습 확장 가능성이 있다.
 - 비디오 프레임 샘플링 및 비전 트랜스포머로 각 프레임을 인코딩하여 모델이 시간적 차원에서의 시각적 개념 이해 및 다양한 VLM 학습을 지원한다.
 - 영어 데이터에 대한 비전 언어 사전 학습 후 다른 언어/도메인 특정 텍스트 모듈(XLM-R)로 교체 가능한 모듈식 구조 유연성을 갖는다.
 - 다국어 비전-언어 작업에서 최고 성능 방법을 능가한다.
 - X2-VLM의 효과성 검증을 위한 실험 수행했고, 이미지 캡션 생성, 교차 모달 이해 작업에서 우수한 성능을 갖는다.
 - Fig 1 (a)와 같이 X2-VLM은 성능과 모델 크기 사이에서 좋은 절충을 이루고 있다.
 - Fig 2 (b)와 같이 비디오-텍스트 검색과 비디오 VQA 두 유형의 작업에서 최신 성능을 달성한다.
 - 논문의 주요 기여도
   - multi-grained 비전 언어 정렬을 동시에 학습하고 위치시키는 통합 사전 학습 프레임워크 제안한다.
   - 이미지-텍스트 및 비디오-텍스트 작업 처리 가능한 전체 사전 학습된 VLM인 X2-VLM 제시한다.
   - 대규모 데이터 및 큰 모델 크기로 확장 가능한 프레임워크로 확인되었다.
   - 다양한 언어와 도메인에서 사용 가능한 X2-VLM의 모듈식 디자인 잠재력 드러냈다.

 <div align="center">
 <img src="../../assets/images/2024-01-19-X2-VLM/Fig1.jpg" alt="Figure_1"  style="zoom:80%;"/>
 </div>

## 2. Related Work
### 2.1 Image-Text Pre-training
 - Fine-grained 접근 방식은 객체에 대한 레이블을 기반으로 사전 학습된 객체 탐지기를 이미지 인코더로 사용한다.
 - 객체 탐지기는 객체가 포함된 것으로 보이는 모든 영역을 식별한 후 각 영역에 대해 객체 분류를 수행한다.
 - 이미지는 식별된 영역의 수십 개의 객체 중심 특성으로 표현되며, 다양한 영역의 여러 객체 간 관계를 표현할 수 없다.
 - 객체 탐지기는 일반적인 객체만 감지할 수 있어, 실제 응용에서 다양한 시각적 개념을 인코딩하는 데 제한적이다.(ex. Pepsi와 Coca Cola 또는 Audi와 BMW 구별하기 힘듦)
 - Coarse-grained 접근 방식은 컨볼루션 네트워크나 비전 트랜스포머를 사용하여 전체 이미지 특성을 추출하고 인코딩한다.
 - 최신 비전 트랜스포머(e.g., Swin-Transformer, BEiT-2)를 사용하는 METER 및 VL-BEiT와 같은 최근 방법은 가장 강력한 fine-grained을 능가할 수 있다.
 - 일부 방법들은 객체 수준과 이미지 수준의 정렬을 모두 학습하려고 시도한다.
 - 그러나 이러한 접근 방식은 여전히 객체 탐지기에 의존하므로 앞서 언급한 문제에 직면한다.
 - 본 연구의 multi-grained 비전 언어 사전 학습 프레임워크는 객체 탐지에 의존하지 않고 객체 수준이나 이미지 수준에 제한되지 않는 통합된 방식으로 비전 언어 정렬을 학습한다.

### 2.2 Video-Text Pre-training
 - 대부분의 기존 VLM은 이미지-텍스트 작업만 다루고, 소수의 VLM만 비디오-텍스트 사전 학습을 수행한다.
 - 비디오-텍스트 모델은 여러 이미지로 구성된 비디오의 특성상 이미지-텍스트 모델과 많은 유사점을 공유한다.
 - 비디오-텍스트 사전 학습은 이미지-텍스트 사전 학습과 유사점을 공유하지만, 기존 방법은 두 유형의 작업 모두에서 최신 성능을 달성하지 못했다.
 - 대표적인 비디오-텍스트 사전 학습 작업으로는 ClipBERT, Frozen, ALPRO, VIOLET, All-in-one 등이 있다.
 - 최근에는 OmniVL이 제안되어 이미지-텍스트 작업과 비디오-텍스트 작업 모두를 지원한다.
 - OmniVL은 비디오에 대해 3D 패치 임베딩을 사용하고 이미지에 대해서는 2D 패치 임베딩을 사용하며, 비전 인코딩에 TimeSformer를 채택한다.

### 2.3 Multilingual Multi-modal Pre-training
 - 다국어 다중 모달 사전 학습(Multilingual multi-modal pre-training)은 다중 모달 모델을 비영어 텍스트에 적용할 수 있게 하려는 목표를 가진다.
 - 다국어 사전 학습(multilingual pre-training)과 다중 모달 사전 학습(multi-modal pre-training)에서 사용할 수 있는 상대적으로 많은 병렬 데이터와 달리, 다국어 다중 모달 코퍼스는 소수이며 제한된 언어 범위를 갖는다.
 - $M^3P$, $UC^2$, MURAL, CCLM 등과 같은 방법들은 추가 데이터를 필요로 한다.
 - 대조적으로 X2-VLM은 모듈식 구조의 잠재력을 활용하여 다국어 다중 모달 사전 학습 과정 없이도 다국어 V+L 작업에 적응할 수 있다.

## 3. Method
### 3.1 Overview
 - **Architecture**
   - X2-VLM은 비전, 텍스트, 다중 모달 융합 모듈(multi-modal fusion modules)로 구성된다.
   - 모든 모듈은 Transformer 기반으로 되어 있다.
   - 융합 모듈은 텍스트 특성을 입력으로 받아 각 층에서 cross-attention를 통해 비전 특성과 텍스트 특성을 융합한다.
   - 사전 학습에서 이 세 모듈은 인코더로 작동하며, 이미지 캡션 생성을 위한 실험에서 보여진 것처럼 텍스트 및 융합 모듈은 left-to-right self-attention를 적용하여 생성 작업에도 적용될 수 있다.
   - Fig 3은 X2-VLM의 구조와 multi-grained 정렬 및 위치 지정 방법을 보여준다.
   <details>
   <summary>Figure 3 펼치기/접기</summary>
   <div align="center">
   <img src="../../assets/images/2024-01-19-X2-VLM/Fig3.jpg" alt="Figure_3"  style="zoom:60%;"/> 
   </div>
   </details>
 - **Data**
   - X2-VLM은 이미지-텍스트 쌍, 비디오-텍스트 쌍, 객체 및 영역의 이미지 주석을 포함한 모든 시각적 개념을 텍스트 설명과 연결하는 통합 접근 방식입니다.
   - 하나의 이미지는 여러 시각적 개념(visual concept)을 포함할 수 있으며, 각각은 텍스트 설명(text description)과 연관된다.
   - 시각적 개념은 객체 또는 영역의 좌표로 표현된다.
   - 텍스트 설명은 객체의 속성(ex. color)을 객체 레이블과 연결하여 만든다.
   - Table 1과 같이 일부 이미지는 관련 텍스트가 없고(T=NaN), 일부 이미지는 레이블이 없다(N=0).
   - 그럼에도 불구하고, **본 논문에서는 모든 유형의 데이터를 학습 배치에 혼합하고, 따라서 각 학습 반복에서 multi-grained 정렬 및 위치 지정을 동시에 최적화하여 모델을 최적화한다**.
   <details>
   <summary>Table 1 펼치기/접기</summary>
   <div align="center">
   <img src="../../assets/images/2024-01-19-X2-VLM/Table1.jpg" alt="Table_3"  style="zoom:60%;"/> 
   </div>
   </details>

### 3.2 Unified Vision Encoding
 - Fig 2와 같이 X2-VLM은 이미지와 비디오 인코딩을 통합한다.
 - 입력에 관계없이 비전 모듈은 비전 트랜스포머의 잠재 특성 공간에서 hidden state를 생성하여 이미지-텍스트 사전 학습과 세부적 사전 학습이 상호 강화한다.
 - **Visual Concept Representation**
   - X2-VLM은 비전 트랜스포머를 one forward pass하여 이미지 내 모든 multi-grained 시각적 개념을 효율적으로 얻는 방법을 제안한다.
   - 이미지를 패치 특성으로 처리한 후, 경계 상자에 해당하는 패치 집합을 집계하여 객체나 영역을 표현한다.
 - **Video Representation**
   - 비디오는 여러 이미지로 구성되므로, 비디오 인코딩과 이미지 인코딩을 단순하고 효율적인 방식으로 통합한다.
   - 초당 비디오 프레임을 샘플링하고 각 프레임을 패치 특성으로 인코딩한다. 그런 다음, 각 프레임의 패치 특성에 시간 정보를 추가하고 시간 차원에서 평균을 계산하여 비디오를 표현한다.
   - 비디오-텍스트 쌍과 객체/영역/이미지-텍스트 쌍 모두에 통합 사전 학습 프레임워크를 적용할 수 있다.

 <div align="center">
 <img src="../../assets/images/2024-01-19-X2-VLM/Fig2.jpg" alt="Figure_2" style="zoom:80%;"/>
 </div>

### 3.3 Multi-Grained Vision Language Pre-training
 - Fig 3과 같이 X2-VLM을 두가지 목표로 동시에 최적화한다.
 - 1) 시각적 개념과 텍스트 간 multi-grained 정렬 학습
 - 2) 다양한 텍스트 설명이 주어진 이미지 내에서 multi-grained 시각적 개념 위치 지정
#### 3.3.1 Multi-Grained Aligning
 - 시각적 개념(객체, 지역, 이미지, 비디오 등)과 텍스트 설명을 정렬한다.
 - 최적화를 위해 대조 손실(Contrastive Loss), 매칭 손실(Matching Loss), MLM 손실(Masked Language Modeling Loss)를 활용한다.
 - **대조 손실(Contrastive Loss)**
   - 각 미니 배치 내에서 (시각적 개념, 텍스트) 쌍을 예측한다.
   - 긍정적 예시(T)는 V에 대한 것이며, 나머지 (N-1) 텍스트들은 부정적 예시로 처리한다.
   - 시각 인코더와 텍스트 인코더의 [CLS] 임베딩 출력을 사용하여 유사도(s(V, T)) 정의한다. $s(V, T) = g_v(v_(cls))^⊤g_w(w_cls)$ (1)
   - vision-to-text 유사도와  text-to-vision 유사도를 각각 계산한다.
   - 학습 가능한 온도 매개변수(τ) 사용한다.
   - 교차 엔트로피를 사용하여 실제 유사도와 예측 유사도 간의 손실 계산한다.
   - $L_{cl} =\frac{1}{2}\mathbb{E}_{V,T∼D}[H(y^{v2t}(V),p^{v2t}(V))+H(y^{t2v}(T),p^{t2v}(T))]$
 - **매칭 손실 (Matching Loss)**
   - 시각적 개념과 텍스트 쌍이 일치하는지 판단한다.
   - 미니 배치 내에서 가장 어려운 부정적 텍스트를 샘플링한다.
   - 각 텍스트에 대해 어려운 부정적인 시각적 개념을 샘플링한다.
   - 퓨전 모듈을 통해 입력된 쌍으로부터 매칭 확률($p^match$)을 예측한다.
   - 실제 레이블과 예측된 매칭 확률 간의 손실을 계산한다.
   - $L_{match} = \mathbb{E}_{V,T∼D}H(y^{match},p^{match}(V, T))$
 - **마스크드 언어 모델링 손실 (MLM Loss)**
   - 텍스트 내 마스크된 단어를 시각적 개념에 기반하여 예측한다.
   - 텍스트의 40%를 임의로 마스킹하고, 10%는 무작위 토큰, 10%는 변경하지 않고, 80%는 [MASK]로 대체.
   - 퓨전 인코더의 출력과 선형 층을 사용하여 마스크된 토큰의 확률을 예측한다.
   - 실제 단어와 예측된 단어 간의 교차 엔트로피 손실을 최소화한다.
   - $L_{mlm} = \mathbb{E}_{tj∼T;(V,T)∼D}H(y^j,p^j(V, \widehat{T}))$

#### 3.3.2 Multi-Grained Localization
 - 다양한 multi-grained에서 시각적 개념과 텍스트를 정렬했다.
 - X2-VLM을 최적화하기 위해 이미지 내 다양한 시각적 개념을 찾도록(바운딩 박스 예측 작업) 학습했다.
 - ℓ1 손실이 가장 일반적으로 사용되지만, 박스 크기에 따라 다른 스케일을 갖는다.
 - 상대적 오류가 유사해도 작고 큰 박스의 스케일 차이를 완화하기 위해 ℓ1 손실과 일반화된 IoU(Intersection over Union) 손실의 선형 조합을 사용한다.
 - $L_{bbox} = \mathbb{E}_{(V^i,T^i)∼I;I∼D}[L_{iou}(b^i, \widehat{b}ˆi) + ||b^i − \widehat{b}ˆi||_1]$
 - X2-VLM의 사전 학습 목표는 $L = Lbbox + Lcl + Lmatch + Lmlm$ 이다.

## 4. Experiment
### 4.1 Pre-training Datasets

### 4.2 Implementation Details

### 4.3 Image-Text Downstream Tasks
#### 4.3.1 Image-Text Retrieval

#### 4.3.2 Visual Question Answering

#### 4.3.3 Visual Reasoning

#### 4.3.4 Visual Grounding

#### 4.3.5 Image Captioning

#### 4.3.6 Winoground

#### 4.3.7 Open-vocabulary Attribute Detection

### 4.4 Video-Text Downstream Tasks

### 4.5 Multilingual Multi-modal Tasks

### 4.6 Ablation Study

### 4.7 Qualitative Study of Multi-Grained Alignments

## 5. Conclusion and Discussion

## A Appendix
### A.1 Pre-training Datasets

### A.2 Implementation Details

### A.3 Ablation Study

### A.4 Qualitative Study of Multi-Grained Alignments