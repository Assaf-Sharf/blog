---
layout: single
title: "X2-VLM: All-In-One Pre-trained Model For Vision-Language Tasks"
permalink: /studies/paper/X2-VLM
tags: [Paper, LVLM]
categories:
  - 📄 paper
date: 2024-01-19
use_math: true
---
*비전 언어 사전 학습은 대량의 데이터로부터 비전과 언어 간의 정렬을 학습하는 것을 목표로 한다. 기존의 대부분의 방법들은 이미지-텍스트 정렬만을 학습하지만, 일부 다른 방법들은 사전 학습된 객체 감지기를 활용하여 객체 수준에서 비전 언어 정렬을 활용한다. 본 논문에서는 다중 정밀도 비전 언어 정렬을 통합 사전 학습 프레임워크를 통해 동시에 다중 정밀도 정렬 및 다중 정밀도 위치 결정을 학습하는 것을 제안한다. 이를 바탕으로 이미지-텍스트 사전 학습과 비디오-텍스트 사전 학습을 하나의 모델에서 통합하는 유연한 모듈식 아키텍처를 가진 X2-VLM이라는 모든 것을 포함하는 모델을 제시한다. X2-VLM은 다양한 텍스트 설명과 연관된 무한한 시각적 개념을 학습할 수 있다. 실험 결과에 따르면 X2-VLM은 이미지-텍스트 및 비디오-텍스트 작업 모두에서 기본 및 대규모 규모에서 최고의 성능을 보여주며, 성능과 모델 규모 사이에서 좋은 절충을 이룬다. 또한, X2-VLM의 모듈식 설계는 어떤 언어나 도메인에서도 활용될 수 있도록 높은 전이성을 제공한다. 예를 들어 텍스트 인코더를 XLM-R로 간단히 교체함으로써, X2-VLM은 어떠한 다국어 사전 학습 없이도 최신의 다국어 다중 모달 사전 학습 모델들을 능가한다. 코드와 사전 학습된 모델은 [github.com/zengyan-97/X2-VLM](https://github.com/zengyan-97/X2-VLM){:target="_blank"}에서 이용 가능하다.*

## 📋 Table of Contents

- [1. Introduction](#1-introduction)
- [2. Related Work](#2-related-work)
- [3. Method](#3-method)
- [4. Experiment](#4-experiment)
- [5. Conclusion and Discussion](#5-conclusion-and-discussion)
- [A Appendix](#a-appendix)

## 1. Introduction
 - 비전 언어 사전 학습(Vision language pre-training)은 다수의 이미지-텍스트 또는 비디오-텍스트 쌍에서 비전 언어 정렬을 학습하는 것을 목표로 한다.
 - 기존 연구의 접근법은 크게 Coarse-grained와 Fine-grained로 구분된다.
 - Coarse-grained 접근법은 전체 이미지 특징을 인코딩하는 데 사용되지만, 세부적인 시각-언어 정렬 학습에 어려움이 있다.
 - Fine-grained 접근법은 이미지 인코더로 사전 학습된 객체 탐지기를 사용하여 미세한 정렬을 학습하지만, 다수의 객체 간 관계를 인코딩할 수 없고 제한된 수의 객체 클래스만 인식할 수 있다.
 - multi-grained alignments 학습의 챌린지
   - 이상적으로 VLM은 객체-텍스트 정렬이나 이미지-텍스트 정렬에 제한되지 않고 사전 학습 중 비전과 언어 간의 다중 입자 정렬을 동시에 학습해야 한다. 그러나 이는 네 가지 측면에서 도전적이다: 다중 입자 정렬을 학습할 데이터 유형, 다양한 데이터 유형을 통합적으로 집계하는 방법, 다중 입자 시각적 개념을 단일 비전 인코더로 표현하는 방법, 데이터에서 효율적으로 다중 입자 비전 언어 정렬을 학습하는 방법.

X2-VLM의 제안: 모든 비전 언어 작업에 대해 동시에 다중 입자 정렬을 학습할 수 있는 통합 프레임워크로 사전 학습된 '모두를 위한 하나의 VLM', X2-VLM을 제안한다. 이를 위해 이미지에 대한 객체 레이블, 이미지에 대한 영역 주석, 이미지에 대한 텍스트 설명 등 세 가지 유형의 데이터를 사용한다.

X2-VLM의 모듈러 구조: X2-VLM은 비전, 텍스트, 융합의 세 가지 모듈로 구성되며, 모두 트랜스포머 기반으로 되어 있다. 비전 트랜스포머를 사용하여 이미지를 인코딩하고, 패치 기능을 사용하여 이미지 내의 다중 입자 시각적 개념을 표현한다.

X2-VLM의 확장성과 실험 결과: X2-VLM은 비디오-텍스트 사전 학습으로 확장 가능하며, 이미지-텍스트 및 비디오-텍스트 벤치마크에서 최고 성능을 보여준다. 또한, 영어 데이터로 사전 학습 후 텍스트 인코더를 XLM-R로 교체함으로써 다국어 비전-언어 작업에서 최고 성능 방법을 능가한다.

## 2. Related Work
### 2.1 Image-Text Pre-training
 - 세부적 접근: 사전 학습된 객체 탐지기를 이미지 인코더로 사용한다. 예를 들어, COCO나 Visual Genome 데이터셋에서 공통 객체에 대한 주석을 기반으로 학습된다.

객체 탐지기는 객체가 포함된 것으로 보이는 모든 영역을 식별한 후 각 영역에 대해 객체 분류를 수행한다.

이미지는 식별된 영역의 수십 개의 객체 중심 특성으로 표현된다.

객체 중심 특성은 다양한 영역의 여러 객체 간 관계를 표현할 수 없다.

객체 탐지기는 일반적인 객체만 감지할 수 있어, 실제 응용에서 다양한 시각적 개념을 인코딩하는 데 제한적이다.

대략적 접근: 컨볼루션 네트워크나 비전 변환기를 사용하여 전체 이미지 특성을 추출하고 인코딩한다.

이 방법은 보다 효율적이지만, 세부적 접근에 비해 성능이 좋지 않을 수 있다.

그러나 최신 비전 변환기(e.g., Swin-Transformer, BEiT-2)를 사용하는 최근 방법들이 가장 강력한 세부적 접근을 능가할 수 있다.

일부 방법들은 객체 수준과 이미지 수준의 정렬을 모두 학습하려고 시도한다.

그러나 이러한 접근 방식은 여전히 객체 탐지기에 의존하므로 앞서 언급한 문제에 직면한다.

우리의 다중 입자 비전 언어 사전 학습 프레임워크는 객체 탐지에 의존하지 않고 객체 수준이나 이미지 수준에 제한되지 않는 통합된 방식으로 비전 언어 정렬을 학습한다.

### 2.2 Video-Text Pre-training
 - 대부분의 기존 VLM은 이미지-텍스트 작업만 다룬다. 소수의 VLM만 비디오-텍스트 사전 학습을 수행한다.
비디오-텍스트 모델은 여러 이미지로 구성된 비디오의 특성상 이미지-텍스트 모델과 많은 유사점을 공유한다.
비디오-텍스트 사전 학습은 이미지-텍스트 사전 학습과 유사점을 공유하지만, 기존 방법은 두 유형의 작업 모두에서 최신 성능을 달성하지 못한다.
대표적인 비디오-텍스트 사전 학습 작업으로는 ClipBERT, Frozen, ALPRO, VIOLET, All-in-one 등이 있다.
최근에는 OmniVL이 제안되어 이미지-텍스트 작업과 비디오-텍스트 작업 모두를 지원한다.

### 2.3 Multilingual Multi-modal Pre-training
- 다국어 다중 모달 사전 학습은 다중 모달 모델을 비영어 텍스트에 적용할 수 있게 하려는 목표를 가진다.
이 분야는 도전적이며, 다국어 사전 학습과 다중 모달 사전 학습에서 사용할 수 있는 상대적으로 많은 병렬 데이터와 달리, 제한된 언어 범위를 가진 소수의 다국어 다중 모달 코퍼스만 존재한다.
M3P, UC2, MURAL, CCLM 등과 같은 방법들은 추가 데이터를 필요로 한다.
이와 대조적으로, X2-VLM은 모듈식 구조의 잠재력을 활용하여 다국어 다중 모달 사전 학습 과정 없이도 다국어 V+L 작업에 적응할 수 있다.

## 3. Method
### 3.1 Overview

### 3.2 Unified Vision Encoding

### 3.3 Multi-Grained Vision Language Pre-training

#### 3.3.1 Multi-Grained Aligning

#### 3.3.2 Multi-Grained Localization

## 4. Experiment
### 4.1 Pre-training Datasets

### 4.2 Implementation Details

### 4.3 Image-Text Downstream Tasks
#### 4.3.1 Image-Text Retrieval

#### 4.3.2 Visual Question Answering

#### 4.3.3 Visual Reasoning

#### 4.3.4 Visual Grounding

#### 4.3.5 Image Captioning

#### 4.3.6 Winoground

#### 4.3.7 Open-vocabulary Attribute Detection

### 4.4 Video-Text Downstream Tasks

### 4.5 Multilingual Multi-modal Tasks

### 4.6 Ablation Study

### 4.7 Qualitative Study of Multi-Grained Alignments

## 5. Conclusion and Discussion

## A Appendix
### A.1 Pre-training Datasets

### A.2 Implementation Details

### A.3 Ablation Study

### A.4 Qualitative Study of Multi-Grained Alignments