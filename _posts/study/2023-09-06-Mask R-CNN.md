---
layout: single
title: "Mask R-CNN"
permalink: /studies/paper/Mask R-CNN
tags: [Paper, Vision AI]
categories:
  - 📄 paper
use_math: true
---
*본 논문은 객체 인스턴스 분할을 위한 개념적으로 간단하고, 유연하며, 일반적인 프레임워크를 제시한다. 본 논문의 접근 방식은 이미지 내 객체를 효율적으로 탐지하는 동시에 각 인스턴스에 대한 고품질 분할 마스크를 생성한다. Mask R-CNN이라고 불리는 이 방법은 기존의 경계 상자 인식을 위한 분기에 객체 마스크를 예측하는 분기를 추가함으로써 Faster R-CNN을 확장한다. Mask R-CNN은 학습하기 간단하며 Faster R-CNN에 비해 작은 오버헤드만을 추가하고, 초당 5 프레임으로 작동한다. 또한, Mask R-CNN은 다른 태스크로 쉽게 일반화될 수 있는데, 예를 들어 같은 프레임워크에서 인간의 포즈를 추정할 수 있다. 본 논문에서는 COCO 챌린지의 세 가지 트랙인 인스턴스 분할, 경계 상자 객체 탐지, 사람 키포인트 탐지에서 모두 최고의 결과를 보여준다. 별도의 추가 기능 없이도, Mask R-CNN은 기존의 모든 단일 모델 항목들을 포함하여 COCO 2016 챌린지 우승자들보다 모든 태스크에서 더 우수한 성능을 보인다. 이 간단하고 효과적인 접근 방식이 인스턴스 수준 인식 연구에 있어 견고한 기준점을 제공하고, 미래 연구를 용이하게 할 것을 기대한다. 코드는 다음에서 사용할 수 있다: https://github.com/facebookresearch/Detectron.*

## 📋 Table of Contents

- [1. Introduction](#1-introduction)
- [2. Related Work](#2-related-work)
- [3. Mask R-CNN](#3-mask-r-cnn)
- [4. Experiments: Instance Segmentation](#4-experiments-instance-segmentation)
- [5. Mask R-CNN for Human Pose Estimation](#5-mask-r-cnn-for-human-pose-estimation)
- [Appendix](#appendix)

## 1. Introduction
<div align="center">
<img src="../../assets/images/2023-09-06-Mask R-CNN/Fig1.jpg" alt="Figure_1" style="zoom:50%;"/>
</div>
 - 2017년 당시 비전 커뮤니티는 Fast/Faster R-CNN과 Fully Convolutional Network(FCN) 프레임워크를 통해 객체 탐지와 의미론적 분할에서 빠르게 발전하고 있다.
 - 본 연구의 목표는 인스턴스 분할을 위한 유사하게 유연하고 강력한 프레임워크를 개발하는 것이다.
 - 인스턴스 분할(Instance segmentation)은 객체 탐지(각 객체를 분류하고 경계 상자를 사용해 위치를 특정하는 작업)와 의미론적 분할(픽셀을 카테고리별로 분류하지만 객체 인스턴스를 구분하지 않는 작업)의 요소를 결합한다.
 - Mask R-CNN은 Faster R-CNN을 확장하여, 기존의 분류 및 경계 상자 회귀 분기와 병렬로 각 관심 영역(Region of Interest, RoI)에 대한 분할 마스크를 예측하는 분기를 추가한다(Fig 1).
 - 마스크 분기는 각 RoI에 적용되는 작은 FCN으로, 픽셀 대 픽셀 방식으로 분할 마스크를 예측한다.
 - Mask R-CNN은 Faster R-CNN 프레임워크를 기반으로 구현 및 학습이 간단하며, 아키텍처 설계에 있어 다양한 유연성을 제공한다.
 - Mask R-CNN의 핵심은 RoIPool 작업에서 발생하는 한계점인 오정렬(misalignment)를 해결하기 위해 개발된 RoIAlign 레이어(quantization-free layer)로, 정확한 공간 위치를 유지한다.
 - 클래스와 마스크 예측을 분리하는 것이 중요하며, 이는 인스턴스 분할에서 더 나은 결과를 달성하기 위해 필요하다.
 - Mask R-CNN은 복잡한 기술 없이도 COCO 인스턴스 분할 작업에서 모든 단일 모델 결과를 뛰어넘는다.
 - Mask R-CNN은 약 200ms/frame의 속도로 GPU에서 실행되며, COCO 데이터셋 학습에 1-2일이 소요된다.
 - Mask R-CNN은 다른 태스크에도 쉽게 일반화될 수 있으며, 사람의 포즈 추정과 같은 추가적인 태스크에 대해서도 높은 성능을 보인다.

## 2. Related Work
### R-CNN
 - R-CNN은 후보 객체 영역인 관심 영역(Region of Interest, RoI)에 주목하여 개별 RoI마다 독립적으로 컨볼루셔널 네트워크를 평가하는 경계 상자 객체 탐지 방법이다.
 - R-CNN은 RoIPool을 사용하여 특징 맵상의 RoI를 처리하도록 확장되었으며, 이로 인해 속도가 빨라지고 정확도가 향상되었다.
 - Faster R-CNN은 영역 제안 네트워크(Region Proposal Network, RPN)를 통해 Attention 메커니즘을 학습하며, 다양한 후속 개선에도 유연하고 견고하다.

### Instance Segmentation
 - 인스턴스 분할에 대한 접근 방식 중 하나는 세그먼트 제안에 기반을 둔 것으로, 초기 방법들은 하향식 세그먼트에 의존했다.
 - DeepMask와 이어지는 연구들은 세그먼트 후보를 제안하고, 이들을 Fast R-CNN으로 분류하는 방법을 학습한다.
 - 이 접근법은 인식보다 세분화에 먼저 집중하는 것으로, 속도가 느리고 정확도가 떨어지는 문제가 있다.
 - Li et al.의 Fully Convolutional Instance Segmentation(FCIS)는 세그먼트 제안 시스템과 객체 탐지 시스템을 결합하여 '완전 컨볼루셔널 인스턴스 분할'을 구현했다.
 - FCIS는 빠르지만, 인스턴스 간 겹침에 대한 시스템 오류와 가장자리 생성 문제를 드러냈다.
 - 기존 '세분화 우선' 전략과 달리 Mask R-CNN은 '인스턴스 우선' 전략에 기반하고 있으며 마스크와 클래스 레이블을 병렬로 예측한다.

## 3. Mask R-CNN
 - Mask R-CNN은 Faster R-CNN에 기반하여, 후보 객체에 대해 클래스 레이블과 바운딩 박스 오프셋을 출력하는 두 가지 출력 외에도 객체 마스크를 출력하는 세 번째 브랜치를 추가한다.
 - Mask R-CNN은 두 단계(2-stage)로 구성되어 있다.
 - 첫 번째 스테이지인 Region Proposal Network (RPN)은 후보 객체 경계 상자를 제안한다.
 - 두 번째 스테이지에서 클래스와 상자 오프셋을 예측하는 것과 병렬로 각 RoI에 대한 이진 마스크를 출력한다.
 - 학습 중에는 각 샘플링된 RoI에 대해 다중 작업 손실 $L$을 $L_{cls} + L_{box} + L_{mask}$로 정의한다.
 - 마스크 분기는 각 RoI에 대해 $Km^2$차원 출력을 가지며, $K$개 클래스 각각에 대해 $m x m$ 해상도의 $K$ 이진 마스크를 인코딩한다.
 - 마스크 표현(Mask Representation)은 입력 객체의 공간 레이아웃을 인코딩하므로, 완전 연결(fully-connected, fc) 층에 의해 단축된 출력 벡터로 축소될 수 없다.
 - RoIAlign 레이어는 RoIPool의 엄격한 양자화를 제거하고 입력과 추출된 특징 간에 제대로 정렬하여 마스크 예측의 중요한 역할을 한다(Fig 3).
<div align="center">
<img src="../../assets/images/2023-09-06-Mask R-CNN/Fig3.jpg" alt="Figure_3" style="zoom:50%;"/>
</div>
 - Mask R-CNN은 다양한 아키텍처로 구현될 수 있으며, 특징 추출을 위한 컨볼루셔널 백본 아키텍처와 각 RoI에 적용되는 경계 상자 인식(분류 및 회귀) 및 마스크 예측을 위한 네트워크 헤드로 구분된다.
 - ResNet과 ResNeXt 네트워크를 사용하여 Mask R-CNN을 평가한다.

### 3.1. Implementation Details
 - 하이퍼파라미터(hyper-parameter) 설정은 기존 Fast/Faster R-CNN 작업에 따라 정해졌으며, 이는 객체 탐지에 초점을 맞춘 기존 논문에서 결정된 사항이다.
 - 학습 중에는 IoU가 최소 0.5인 RoI를 긍정적으로 간주하고, 그렇지 않으면 부정적으로 간주한다.
 - 마스크 손실 $L_{mask}$는 긍정적인 RoI에서만 정의된다.
 - 이미지 중심 트레이닝(image-centric training)을 채택하며, 이미지는 짧은 에지가 800 픽셀이 되도록 크기를 조정된다.
 - 각 미니배치는 GPU당 2개 이미지를 포함하며, 각 이미지는 긍정 대 부정의 비율이 1:3인 N개의 샘플링된 RoI를 가진다.
 - C4 백본에 대해서는 N이 64이고, FPN에 대해서는 N이 512이다.
 - 8개의 GPU에서 160k 반복에 걸쳐 훈련하며, 학습률은 0.02에서 시작하여 120k 반복에서 10으로 감소한다.
 - ResNeXt를 사용할 때는 GPU당 1개 이미지로 훈련하며, 시작 학습률은 0.01이다.
 - RPN 앵커는 5개의 스케일과 3개의 종횡비를 가지며, RPN은 별도로 훈련되고 Mask R-CNN과 특징을 공유하지 않는다.
 - 추론 시에는 C4 백본에 대해 제안 수가 300이고 FPN에 대해서는 1000이다.
 - 박스 예측 분기는 이러한 제안들에 대해 실행된 후 비최대 억제(Non-Maximum Suppression, NMS)가 수행된다.
 - 마스크 분기는 가장 높은 점수를 받은 100개의 탐지 상자에 적용된다.
 - 마스크 분기는 각 RoI에 대해 K개의 마스크를 예측할 수 있지만, 분류 분기에 의해 예측된 k번째 클래스에 해당하는 마스크만 사용한다.
 - m×m 부동 소수점 마스크 출력은 RoI 크기로 조정되고, 0.5의 임계값에서 이진화된다.
 - Mask R-CNN은 상위 100개 탐지 상자에만 마스크를 계산하기 때문에, Faster R-CNN 대비 소량의 오버헤드를 추가한다(예: 일반 모델에서 약 20%).

## 4. Experiments: Instance Segmentation
### 4.1. Main Results
 - Mask R-CNN은 인스턴스 분할 분야에서 최신 기술과 비교되며, 모든 구현은 이전 최신 모델들의 기본 버전들을 능가한다.
 - COCO 2015와 2016 세그멘테이션 챌린지의 승자인 MNC와 FCIS를 포함한 기존 모델들보다 Mask R-CNN이 더 우수한 성능을 보인다.
 - Mask R-CNN은 ResNet-101-FPN 백본을 사용하고, 다중 스케일 학습/테스트, 수평 플립 테스트, Online Hard Example Mining(OHEM)을 포함한 FCIS+++보다 더 나은 성능을 보인다.
 - Mask R-CNN의 결과는 Figure 5와 Table1과 같다.
 - FCIS+++는 겹치는 인스턴스에 대한 오류가 보이며, 이는 인스턴스 분할의 근본적인 한계가 있다.
 - Mask R-CNN은 이러한 한계를 해결했다.

### 4.2. Ablation Experiments
 - Mask R-CNN은 다양한 제거 실험(ablation experiments)을 통해 분석된다.
 - 아키텍처: Mask R-CNN은 다양한 백본 구조에서 이점을 얻으며, 깊은 네트워크(50 대 101)와 FPN, ResNeXt와 같은 고급 설계에서 혜택을 본다.
 - 다항식 대 이진 마스크: Mask R-CNN은 마스크와 클래스 예측을 분리한다. 박스 분기가 클래스 레이블을 예측하면, 클래스 간 경쟁 없이 각 클래스에 대한 마스크를 생성한다.
 - 클래스별 대 클래스 불특정 마스크: 기본 설정은 클래스별 마스크를 예측하지만, 클래스 불특정 마스크(클래스에 관계없이 단일 m×m 출력을 예측)도 거의 같은 효과를 보인다.
 - RoIAlign: RoIAlign 레이어의 평가는 ResNet-50-C4 백본을 사용하여 수행된다. RoIAlign은 RoIPool보다 약 3점 높은 AP를 달성한다.
 - RoIAlign은 또한 stride가 32 픽셀인 ResNet-50-C5 백본에서 평가되며, 여기서는 마스크 AP를 7.3점, AP75를 10.5점 향상시킨다.
 - 마스크 분기: 세분화는 픽셀 대 픽셀 작업이므로, FCN을 사용하여 마스크의 공간 레이아웃을 활용한다. FCN을 사용하면 MLP에 비해 2.1점 높은 마스크 AP를 얻는다.

### 4.3. Bounding Box Detection Results
 - Mask R-CNN은 COCO 경계 상자 객체 탐지 분야에서 최신 기술과 비교된다.
 - 추론 시에는 Mask R-CNN 모델 전체가 훈련되지만, 분류 및 상자 출력만 사용되며 마스크 출력은 무시된다.
 - ResNet-101-FPN을 사용하는 Mask R-CNN은 이전 최신 모델들의 기본 변형을 포함하여 모든 모델을 능가한다.
 - COCO 2016 탐지 챌린지 우승자인 GRMI의 단일 모델 변형을 포함한 이전 모델들보다 성능이 우수하다.
 - ResNeXt-101-FPN을 사용할 때 Mask R-CNN은 결과를 더욱 향상시키며, 이전 최고의 단일 모델 성능과 3.0 포인트 차이의 상자 AP를 달성한다.
 - 마스크 분기가 없는 Mask R-CNN 버전인 "Faster R-CNN, RoIAlign"은 RoIAlign 덕분에 [27]에서 제시된 모델보다 더 나은 성능을 보인다.
 - 그러나 이 모델은 Mask R-CNN보다 0.9 포인트 낮은 상자 AP를 기록한다.
 - 이러한 Mask R-CNN의 상자 탐지 성능 차이는 다중 작업 훈련의 이점 때문이다.
 - Mask R-CNN은 마스크 AP와 상자 AP 사이의 작은 차이를 보인다(예: 마스크 AP 37.1과 상자 AP 39.8 사이의 2.7 포인트 차이).
 - 이는 Mask R-CNN 접근 방식이 객체 탐지와 더 도전적인 인스턴스 세그멘테이션 작업 사이의 격차를 상당 부분 해소한다는 것을 나타낸다.

### 4.4. Timing
 - 추론 시간: ResNet-101-FPN 모델은 RPN과 Mask R-CNN 단계 간의 특징을 공유하며, Faster R-CNN의 4단계 훈련을 따른다.
 - 이 모델은 Nvidia Tesla M40 GPU에서 이미지당 195ms, CPU 시간으로 출력을 원래 해상도로 조정하는 데 15ms가 소요된다.
 - 공유되지 않은 모델과 통계적으로 동일한 마스크 AP를 달성한다.
 - ResNet-101-C4 변형은 더 무거운 상자 헤드를 가지고 있어, 이미지당 약 400ms가 소요된다.
 - 실제 사용에는 C4 변형을 권장하지 않는다.
 - Mask R-CNN은 빠른 속도로 추론하지만, 설계는 속도 최적화에 초점을 맞추지 않았으며, 이미지 크기와 제안 수를 변화시킴으로써 더 나은 속도/정확도  - 트레이드오프를 달성할 수 있다.
 - 훈련 시간: Mask R-CNN은 빠르게 훈련된다.
 - COCO trainval35k에서 ResNet-50-FPN으로 훈련하는 데 32시간, ResNet-101-FPN으로 훈련하는 데 44시간이 소요된다.
 - 실제 훈련 세트에서 빠른 프로토타이핑은 하루 미만으로 완료될 수 있다.
 - 이러한 빠른 훈련은 연구의 주요 장애물을 제거하고 이 도전적인 분야에 더 많은 연구를 수행하도록 장려할 수 있다.

## 5. Mask R-CNN for Human Pose Estimation
 - Mask R-CNN 프레임워크는 인간 자세 추정(human pose estimation)으로 쉽게 확장된다.
 - 키포인트의 위치는 원-핫(one-hot) 마스크로 모델링되며, K개의 키포인트 유형(예: 왼쪽 어깨, 오른쪽 팔꿈치)마다 하나씩 K개의 마스크를 예측한다.
 - 이 작업은 Mask R-CNN의 유연성을 보여준다.
 - 인간 자세에 대한 최소한의 도메인 지식만이 시스템에 의해 활용되며, 주로 Mask R-CNN 프레임워크의 일반성을 시연하기 위함이다.
 - 구현 세부 사항:
 - 5. 키포인트에 맞게 세그멘테이션 시스템에 소규모 수정을 한다.
 - 
 - 각 인스턴스의 K개 키포인트에 대해, 훈련 대상은 단 하나의 픽셀만 전경으로 레이블이 붙은 m×m 이진 원-핫 마스크이다.
 - 훈련 중에 각 보이는 ground-truth 키포인트에 대해 m2-way softmax 출력에 대한 교차 엔트로피 손실을 최소화한다.
 - 인스턴스 세분화와 마찬가지로 K개의 키포인트는 독립적으로 처리된다.
 - ResNet-FPN 변형을 채택하고, 키포인트 헤드 아키텍처는 그림 4(오른쪽)과 유사하다.
 - 키포인트 헤드는 3×3 512차원 컨볼루션 레이어 8개의 스택으로 구성되며, 이후 디컨볼루션 레이어와 2배 바이리니어 업스케일링을 거쳐 56×56의 출력 해상도를  - 생성한다.
 - 주요 결과 및 제거 연구:
 - 11. 인물 키포인트 AP(APkp)를 평가하고 ResNet-50-FPN 백본을 실험한다.
 - 
 - 결과(62.7 APkp)는 COCO 2016 키포인트 탐지 챌린지 우승자보다 0.9점 높다.
 - Mask R-CNN 방법은 상당히 간단하고 빠르다.
 - 통합 모델은 상자, 세그먼트, 키포인트를 동시에 예측하며 초당 5프레임(fps)으로 실행된다.
 - 사람 카테고리에 세그먼트 분기를 추가하면 APkp가 테스트-개발에서 63.1로 향상된다.
 - 다중 작업 학습에 대한 더 많은 제거 연구가 minival에서 수행된다.
 - 상자만 있는 버전(즉, Faster R-CNN) 또는 키포인트만 있는 버전에 마스크 분기를 추가하면 이러한 작업들이 일관되게 향상된다.
 - RoIAlign이 키포인트 탐지에 미치는 영향도 조사한다.
 - RoIAlign은 RoIPool보다 키포인트 탐지에서 APkp를 4.4점 증가시킨다.
 - 이는 키포인트 탐지가 위치 정확도에 더 민감하기 때문이며, 픽셀 수준의 위치화를 포함한 마스크 및 키포인트에 정렬이 필수적임을 다시 한번 나타낸다.
 - 객체의 경계 상자, 마스크, 키포인트를 추출하는 데 효과적인 Mask R-CNN은 다른 인스턴스 수준 작업에도 효과적인 프레임워크가 될 것으로 기대된다.

## Appendix
### Appendix A: Experiments on Cityscapes
 - Cityscapes 데이터셋에 대한 인스턴스 세분화 결과를 보고한다.
 - 이 데이터셋은 2975개의 훈련 이미지, 500개의 검증 이미지, 1525개의 테스트 이미지로 구성된다.
 - 2048×1024 픽셀 크기의 모든 이미지에는 세밀한 주석이 있다.
 - 인스턴스 세분화 과제는 8개의 객체 범주(사람, 기수, 자동차, 트럭, 버스, 기차, 오토바이, 자전거)를 포함한다.
 - 이 훈련 세트에서 각 범주의 인스턴스 수는 각각 17.9k, 1.8k, 26.9k, 0.5k, 0.4k, 0.2k, 0.7k, 3.7k이다.
 - 성능은 COCO 스타일 마스크 AP(평균 IoU 임계값에 대한)로 측정되며, AP50(즉, IoU가 0.5인 마스크 AP)도 보고된다.
 - 구현 세부 사항:
 - 7. Mask R-CNN 모델에는 ResNet-FPN-50 백본을 적용한다.
 - 
 - 훈련 시 이미지 크기는 무작위로 [800, 1024] 픽셀에서 샘플링되며, 추론은 1024 픽셀 단일 스케일에서 수행된다.
 - 미니배치 크기는 GPU당 1개 이미지이며(총 8개 GPU에서), 모델은 24k 반복 동안 훈련된다.
 - 훈련에는 단일 8-GPU 머신에서 약 4시간이 소요된다.
 - 결과:
 - 11. 검증 및 테스트 세트에서의 결과를 최신 기술과 비교한다.
 - 
 - 조잡한 훈련 세트를 사용하지 않고, 테스트에서 26.2 AP를 달성한다.
 - 이는 이전 최고 성능(DIN [3])보다 30% 이상 향상되었으며, SGN의 25.0 [29]보다도 더 나은 성능이다.
 - 사람과 자동차 범주에서, 이 데이터셋은 범주 내 중복 인스턴스가 많다(평균적으로 이미지당 6명의 사람과 9대의 자동차).
 - 범주 내 중복은 인스턴스 세분화의 핵심적인 어려움이다.
 - Mask R-CNN은 이 두 범주에서 큰 향상을 보여준다(사람의 경우 21.8에서 30.5로, 자동차의 경우 39.4에서 46.9로).
 - COCO 사전 훈련을 사용하는 추가 결과를 보고한다.
 - COCO 사전 훈련된 Mask R-CNN 모델은 테스트에서 32.0 AP를 달성한다.
 - 이는 훈련 데이터의 양이 중요한 역할을 한다는 것을 나타낸다.
 - Cityscapes에서의 방법들은 저샷 학습 성능에 의해 영향을 받을 수 있다.
 - 검증 및 테스트 AP 간에 편향이 관찰된다(이는 [23, 4, 29]의 결과에서도 관찰된다).
 - 이 편향은 주로 트럭, 버스, 기차 범주에서 발생하며, 이러한 범주들은 훈련 샘플이 적다.
 - COCO 사전 훈련은 이러한 범주에서 가장 큰 개선을 가져온다.
 - Cityscapes 데이터셋에서의 예시 결과는 그림 8에 나타난다.

### Appendix B: Enhanced Results on COCO
 - Mask R-CNN은 Fast/Faster R-CNN 및 FCN 개선을 포함한 검출/분할 기술과 호환된다.
 - COCO 2017 인스턴스 세분화 대회에서 3개 우승 팀이 Mask R-CNN 프레임워크를 사용했다.
 - 향상된 결과는 마스크 AP를 36.7에서 41.8로, 박스 AP를 39.6에서 47.3로 증가시켰다.
 - 업데이트된 베이스라인: 학습 기간을 180k 반복까지 연장하고, NMS 임계값을 0.5로 변경했다.
 - 종단 간(end-to-end) 훈련: RPN과 Mask R-CNN을 동시에 훈련시키는 방식을 채택했다.
 - ImageNet-5k 사전 훈련: ImageNet의 5k 클래스 하위 집합으로 사전 훈련한 모델을 사용했다.
 - 훈련 시간 증대: [640, 800] 픽셀에서 무작위로 스케일을 샘플링하는 방식을 채택했다.
 - 모델 아키텍처: 101-층 ResNeXt를 152-층으로 업그레이드하고, 최근 제안된 non-local (NL) 모델을 사용했다.
 - 테스트 시간 증대: [400, 1200] 픽셀에서 샘플링하고 수평 뒤집기를 적용하여 결과를 결합했다.
 - COCO 2017 대회 제출 기반: 위의 결과를 기반으로 하며, 논의되지 않은 앙상블도 사용했다.
 - 키포인트 검출: 훈련 일정을 130k 반복까지 연장하고, 데이터 증류 방법을 사용했다.
 - ResNet-50을 ResNet-101 및 ResNeXt-101로 교체하고, COCO에서 제공한 120k 레이블 없는 이미지를 활용했다.
 - 데이터 증류를 통해 APkp를 69.1로 향상시켰고, 테스트 시간 증대를 통해 APkp를 70.4로 더욱 향상시켰다.