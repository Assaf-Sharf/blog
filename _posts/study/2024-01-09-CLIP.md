---
layout: single
title: "CLIP: Learning Transferable Visual Models From Natural Language Supervision"
permalink: /studies/paper/CLIP
tags: [Paper, LVLM]
categories:
  - 📄 paper
date: 2024-01-09
use_math: true
---
*State-of-the-art 컴퓨터 비전 시스템은 사전에 정해진 객체 카테고리를 예측하도록 학습된다. 이 제한된 형태의 지도는 일반성과 사용성을 제한한다. 왜냐하면 다른 시각적 개념을 명시하기 위해 추가적인 레이블이 필요하기 때문이다. 이미지에 대한 원시 텍스트로부터 직접 학습하는 것은 훨씬 넓은 범위의 지도를 활용하는 유망한 대안이다. 인터넷에서 수집한 4억 개의 (이미지, 텍스트) 쌍 데이터셋에서 처음부터 SOTA 이미지 표현을 학습하는 방법으로, 어떤 캡션과 어떤 이미지가 매치되는지 예측하는 단순한 사전 학습 작업이 효율적이고 확장 가능하다는 것을 보여준다. 사전 학습 후, 자연어는 학습된 시각적 개념을 참조하거나 새로운 것을 설명하는 데 사용되어 다운스트림 작업에 대한 모델의 제로샷 전이를 가능하게 한다. OCR, 비디오 내 동작 인식, 지리적 위치 파악, 그리고 다양한 종류의 세부적인 객체 분류 등과 같은 30개 이상의 다양한 기존 컴퓨터 비전 데이터셋에서 이 접근 방식의 성능을 연구한다. 이 모델은 대부분의 작업에 비일상적으로 전이되며, 데이터셋 특정 학습 없이도 종종 완전히 지도된 기준과 경쟁한다. 예를 들어, 우리는 ImageNet에서 원래 ResNet-50의 정확도와 일치하는데, 이는 128만 개의 학습 예제 없이 제로샷으로 이루어진다. 우리는 코드와 사전 학습된 모델 가중치를 https://github.com/OpenAI/CLIP 에서 공개한다.*

## 📋 Table of Contents

- [1. Introduction and Motivating Work](#1-introduction-and-motivating-work)
- [2. Approach](#2-approach)
- [3. Experiments](#3-experiments)
- [4. Comparison to Human Performance](#4-comparison-to-human-performance)
- [6. Limitation](#6-limitation)

## 1. Introduction and Motivating Work
 - NLP 분야는 autoregressive와 masked language modeling과 같은 방법으로 계산, 모델 용량 및 데이터 측면에서 확장되며 task-agnostic 목표를 달성하였다.
 - Task-agnostic 아키텍처는 특화된 output heads나 데이터셋 커스텀마이징 없이 downstream dataset으로 제로샷 트랜스퍼가 가능해졌다.
 - NLP 분야는 컴퓨터 비전 분야와 다르게 적은 양의 레이블링된 고품질 데이터셋보다 웹 상에서 수집된 많은 양의 레이블링 없는 데이터셋이 학습에 더 용이하다.
 - weakly supervised model이 효과적일 수 있다는 연구결과가 있었다.
 - 4억개의 이미지-테게스트 쌍으로 구성된 데이터셋을 생성했고, ConVIRT로부터 영향을 받은 CLIP(Contrastive Language-Image Pre-training)이라는 모델을 제안한다.

## 2. Approach
### 2.1. Natural Language Supervision
 - 핵심 아이디어는 자연어 감독을 통한 인식 학습한다.
 - Zhang et al. (2020), Gomez et al. (2017), Joulin et al. (2016), Desai & Johnson (2020)은 이미지에 텍스트를 짝지어 시각적 표현을 학습하지만, 각기 다른 용어(비지도, 자기지도, 약한 감독, 감독)를 사용했다.
 - 과거 n-gram 등과 비해 deep contextual representation learning을 통해 효과적으로 자연어의 representation을 추출할 수 있게 되었다.
 - 자연어 지도는 클래식한 레이블링 방식에 비해 데이터 확장성이 좋다.
 - 비지도나 자기지도 학습 방식보다 더 유연하고 전이 가능하므로 다른 종류의 task로 zero-shot trasfer가 가능하다.

### 2.2. Creating a Sufficiently Large Dataset
 - 기존의 연구들은 주로 세 가지 데이터셋, MS-COCO (Lin et al., 2014), Visual Genome (Krishna et al., 2017), 그리고 YFCC100M (Thomee et al., 2016)을 사용했다.
 - MS-COCO와 Visual Genome은 고품질이지만 규모가 작다. (약 10만 개)
 - YFCC100M은 1억 개의 사진을 가지고 있지만, 메타데이터가 부족하고 품질이 다양다. 필터링하면 1500만 개로 줄어든다.(ImageNet과 크기가 비슷)
 - 인터넷에서 공개적으로 사용 가능한 다양한 소스에서 4억 개의 (이미지, 텍스트) 쌍으고 구성된 새로운 데이터셋(WebImageText(WIT))을 구축했다.
 - 500,000개의 쿼리를 포함하는 (이미지, 텍스트) 쌍의 검색했다.
 - 쿼리당 최대 20,000개의 (이미지, 텍스트) 쌍 포함하여 클래스 균형을 맞췄다.
 - 결과적으로 생성된 데이터셋은 GPT-2의 WebText 데이터셋과 유사한 총 단어 수를 가진다.

### 2.3. Selecting an Efficient Pre-Training Method
 - 1000개 클래스 분류 예측을 위해 매우 많은 GPU가 필요하다.
 - 초기 VirTex와 유사하게 이미지는 CNN, 텍스트는 Transformer로 공동 학습하여 이미지 캡션을 예측하였다. 
 - 63백만 파라미터를 가진 transformer language model이 ResNet-50 image encoder 보다 2배 연산량을 사용하여 학습였지만, bag-of-words(BoW) 인코딩 방식보다 ImageNet 클래스를 인식하는 데 세 배 느린 속도로 학습 하였다.
 - Bag of Words란 단어들의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도(frequency)에만 집중하는 텍스트 데이터의 수치화 표현 방법이다.
 -  contrastive representation learning을 통해 이미지와 연관된 전체 텍스트를 예측하는 대신, 어떤 텍스트가 어떤 이미지와 짝을 이루는지 예측하는 대조적 목표로 전환하였고, ImageNet으로의 zero-shot transfer에서 효율성이 4배 향상되었다.
 - CLIP은 N개의 (이미지, 텍스트) 쌍에 대해 실제로 일어난 N x N 쌍을 예측하도록 학습한다.
 - N개의 텍스트와 이미지는 각각 텍스트 인코더와 이미지 인코더를 통해 N 개의 text representation과 image representation이 만들어진다.
 - 이미지/텍스트 representation은 동일한 차원을 가지며 두 벡터 간의 consine similarity(N x N개)를 구한다.
 - N개의 positive pair의 consine similarity를 최대화하고 N^2 - N개의 negative pair의 consine similarity를 최소화한다.
 - 유사도 점수에 대해 symmetric cross entropy loss를 최적화한다.

### 2.4. Choosing and Scaling a Model
 - 이미지 인코더에 대해 ResNet-50과 Vision Transformer(ViT) 두 가지 아키텍처를 고려한다.
 - 텍스트 인코더는 Transformer(63M)를 사용했다.

### 2.5. Training
 - 5가지 ResNet 모델과 3가지 ViT 모델을 고려했다.
     - RN-① ResNet-50
     - RN-② ResNet-101
     - RN-③ ResNet50x4 (EfficientNet-style model)
     - RN-④ ResNet50x16 (EfficientNet-style model)
     - RN-⑤ ResNet50x64 (EfficientNet-style model)
     - ViT-① ViT-B/32
     - ViT-② ViT-B/16
     - ViT-③ ViT-B/14
 - 모든 모델은 32 epoch 동안 학습했다.
 - Adam optimizer를 사용했다.
 - 학습률 스케줄링을 위해 코사인 스케줄을 사용하여 학습률을 감소시켰다.
 - 초기 하이퍼파라미터는 그리드 검색, 무작위 검색, 수동 조정의 조합을 통해 설정했다.
 - 학습 가능한 온도 파라미터(learnable temperature parameter, τ)는 0.07로 초기화하고, 로짓(logits)을 100배 이상 스케일링하지 않기 위해 클리핑하여 학습 불안정 방지했다.
 - 매우 큰 미니배치 크기인 32,768을 사용했다.
 - 학습 가속화 및 메모리 절감을 위한 mixed precision 사용했다.
 - 가장 큰 ResNet 모델(RN50x64)는 592개의 v100 gpu에서 18일동안 학습했다.
 - 가장 큰 ViT 모델(ViT-L/14)는 256개의 v100 gpu에서 12일동안 학습했다.

## 3. Experiments
### 3.1. Zero-Shot Transfer
#### 3.1.1. MOTIVATION
 - 일반적으로 컴퓨터 비전분야에서 zero-shot transfer는 이미지 분류에서 보이지 않는 객체 카테고리에 일반화하는 연구를 의미한다.
 - 더 넓은 의미에서 미분류된 데이터셋에 대한 일반화 연구를 의미한다.
 - Zero-shot learning을 통해 본질적으로 '미분류된 태스크' 수행 능력을 평가한다.
 - CIFAR-10 또는 SVHN 데이터셋처럼 벤치마크 데이터셋이 "실제" task를 측정하는지 명확하지 않다.(특정 태스크에 대한 성능을 평가하기 위해 데이터셋을 사용함)
 - 특정 데이터셋에 대한 성능을 평가하기 위해 CLIP의 zero-shot trasnfer를 사용한다.

#### 3.1.2. USING CLIP FOR ZERO-SHOT TRANSFER
 - 제로샷 분류를 수행하기 위해 CLIP은 사전 학습 과정에서 이미지와 텍스트가 데이터셋 내에서 짝을 이루는지 예측하는 능력을 학습한다.
 - 이미지의 특징 임베딩과 가능한 텍스트 세트의 특징 임베딩을 각각의 인코더를 통해 계산한다.
 - 임베딩 간 코사인 유사도를 계산하고, 온도 매개변수 τ로 조정한 후 소프트맥스를 통해 확률 분포로 정규화한다.
 - 이미지 인코더는 이미지에 대한 특징 표현(feature representation)(예: 색상, 형태, 질감 등)을 계산하는 역할을 하고, 텍스트 인코더는 클래스가 나타내는 시각적 개념(예: "고양이", "자동차")을 기반으로 선형 분류기(linear classifier)의 가중치를 생성하는 하이퍼네트워크(하나의 신경망이 다른 신경망의 가중치를 생성하는 방식)로 작동한다.
 - CLIP의 사전 학습 과정을 매 epoch마다 "optimizing the performance of a randomly created proxy to a computer vision dataset"으로 볼 수 있다.
 - 이 데이터셋은 각 클래스당 하나의 예시를 가지며, 총 32,768개의 클래스로 구성되어있다.
 - 방대한 양의 시각적 개념과 그에 해당하는 자연어 설명을 학습하며, 이미지와 텍스트 간의 관계를 이해하도록 학습된다.
 - Zero-shot evaluation에서는 텍스트 인코더에 의해 계산된 zero-shot classifier를 캐시하고, 이 classifier가 텍스트 인코더에 의해 한 번 계산되면 후속 예측에 대해 모두 재사용되어 효율적이다.(비용 분산)

#### 3.1.3. INITIAL COMPARISON TO VISUAL N-GRAMS
 - Visual N-grams에 비해 CLIP이 더 향상된 성능을 보였다.
 - aYahoo, ImageNet, SUN 3개의 데이터셋 모두에서 zero-shot 성능이 뛰어났다.

#### 3.1.4. PROMPT ENGINEERING AND ENSEMBLING
 - 대부분의 이미지 분류 데이터셋이 클래스의 이름이나 설명을 부차적으로 다룰 뿐 자연어 기반의 zero-shot transfer에 필요한 정보를 제공하지 못한다.
 - 일반적으로 라벨을 숫자 ID로 주석을 달고, 이를 영어 이름으로 매핑한다.
 - CLIP의 텍스트 인코더는 문맥 부족으로 다의어 문제(Polysemy)가 야기된다.
 - CLIP의 사전 학습 데이터셋에서 대부분의 텍스트는 단일 단어보다 완전한 문장으로 이루어져 있다.
 - 위 두 문제를 해결하기 위해 "A photo of a {label}"과 같은 프롬프트 엔지니어링을 사용하면 성능이 향상될 수 있다.
 - 데이터셋과 태스크에 따라 맞춤화된 프롬프트 엔지니어링을 적용하여 zero-shot 성능을 크게 향상시킬 수 있다. 즉 카테고리를 명시하는 문구를 사용하는 것이 문맥을 제공하는 데 효과적이다. (예, A poth of a {label}, a type of pet.")
 - 여러 가지 다른 컨텍스트 프롬프트를 사용하여 여러 zero-shot classifiers를 ensembling하여 성능을 향상시킬 수 있다.
 - 다양한 프롬프트 예시
     - 다양한 크기에 대한 프롬프트: "A photo of a big {label}"과 "A photo of a small {label}"는 물체의 크기에 따라 다른 해석을 할 수 있다.
     - 특정 카테고리에 대한 프롬프트: Oxford-IIIT Pets에 대해서 "A photo of a {label}, a type of pet" // Food101에 대해서 "A type of food" // FGVC Aircraft에 대해서는 "A type of aircraft".
     - OCR(광학 문자 인식) 데이터셋: 인식해야 하는 텍스트나 숫자 주변에 따옴표를 사용한다. "{label}"
 -  ImageNet 데이터셋에서 80가지 다른 컨텍스트 프롬프트를 앙상블하여 성능을 기본 프롬프트보다 추가적으로 3.5% 향상시켰다.

#### 3.1.5. ANALYSIS OF ZERO-SHOT CLIP PERFORMANCE
 - Zero-Shot CLIP은 27개 중 16개의 데이터셋에서 기본 베이스라인을 초과하는 성능을 보였다.(fig. 5)
 - 대부분 우수했지만 위성 이미지 분류, 림프절 종양 감지 등과 같은 복잡하고 추상적인 태스크에서는 상대적으로 약했다.
 - Supervised learning 모델(ResNet-50)과 CLIP을 비교하는 것보다 Few-shot 모델(SimCLR)과 비교를 해야한다. 결과적으로 16-shot SimCLR과 Zero-shot CLIP과 비슷한 성능을 확인할 수 있다.

### 3.2. Representation Learning
 - Representation Learning의 품질을 평가하기 위해서 모델에서 추출한 representation을 사용하여 각 데이터셋에 대한 linear classifier를 학습시키고 성능을 측정해야 한다.
 - 66개의 다양한 모델과 27개의 서로 다른 데이터셋을 평가했다.
 - Natural language supervision으로 학습한 CLIP 모델은 전통적인 이미지 분류 기반 사전 학습 접근법보다 우수한 성능을 보였다.
 - CLIP은 27개 데이터셋 중 21개 데이터셋(OCR, geo-localization, scene recognition, activity recognition in videos 포함)에서 우수했다.
 - ImageNet과 CIFAR10 및 CIFAR100과 같은 저해상도 데이터셋에 대해 상대적으로 떨이지는데 이는 데이터 증강이 부족한 것과 관련이 있을 수 있다.

### 3.3. Robustness to Natural Distribution Shift
 - 모델들이 ImageNet과 같은 벤치마크 데이터를 다 외웠기 때문에 over-fitting이 발생하였고, 이로 인해 간단한 태스크라도 실수를 범하며, 새로운 벤치마크에서 인간보다 정확도가 훨씬 낮다.
 - CLIP 모델은 다른 분포의 데이터에 대해서도 훨씬 robustness하다고 볼 수 있다.(Fig. 12)
 - ImageNet 정확도와 분포 이동 정확도 사이의 격차를 최대 75% 감소시켰다.

## 4. Comparison to Human Performance
 - Oxford IIT Pets 데이터셋에 대해 인간은 zero-shot일 때 53.7% 정확도를 보였고, one-shot일 때는 75.7%로 향상되었으며, two-shot일 때는 미미하게 향상되었다.
 - 인간은 새로운 정보를 빠르게 학습하고 개선할수 있음을 나타낸다.
 - CLIP 모델은 zero-shot일 때 93.5% 정확도로 뛰어난 성능을 보였다. 하지만 few-shot에서 인간만큼의 학습 효과를 보지 못했다.
 - 딥러닝 모델이 인간의 학습 효율성에 도달하기 위해 여전히 개선해야할 여지가 있다.

## 6. Limitation
 - Zero-shot CLIP은 간단한 지도학습 모델보다 뛰어날 수 있지만, SOTA 모델보다 성능이 낮다.
 - SOTA모델 성능에 도달하기 위해서는 약 1000배의 컴퓨팅 증가가 필요하지만, 현실적으로 어려우므로 컴퓨팅을 줄이는 연구가 이루어져야 한다.
 - CLIP은 복잡한 시스템과 차 모델, 꽃 종류 등 세부적인 분류 작업에서 약한 성능을 보인다.
 - CLIP은 학습 데이터셋과 실제로 다른 분포의 데이터에 대해 여전히 취약하다.
 - CLIP은 이미지 캡셔닝과 같은 다양한 작업을 수행할 수 있지만, 모든 개념을 텍스트로 명확하게 표현하는 데는 한계가 있다.
 - 4억 개의 이미지를 1초에 하나씩 32 epoch을 수행하면 405년이나 걸리므로 데이터 효율성의 부족 한계가 있다.
 - 인터넷에서 무작위로 수집한 이미지-텍스트 쌍을 학습 데이터로 사용함으로써, 다양한 사회적 편견을 학습할 수 있다.(7.1. Bias)
 - 여전히 다양한 예시가 필요하다.
 - 복잡한 시각적 개념을 텍스트로만 정의하는 데는 한계가 있다.
 - 대규모 데이터셋을 구성하면 downstream task에도 겹치는 데이터가 존재할 수 있으므로(원래는 중복되는 데이터없이 분리해야한다.) 실제 모델의 성능보다 과장될 수 있다.(5. Data Overlap Analysis)

