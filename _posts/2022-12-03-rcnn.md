---
title : '[DL/CV] 객체 탐지 - RCNN 📦'
layout : single
toc : true
toc: true
toc_sticky: true
categories:
  - cv-objectdetection
---

## RCNN

### 0. 들어가며
RCNN 모델은 딥러닝 기반의 Object Detection의 시작을 연 모델이다. 이번 글에서는 이후에 이어지는 RCNN 시리즈들의 기반이라고 할 수 있는 RCNN을 다룬 논문을 다뤄보겠다.

다음은 RCNN의 개략적인 알고리즘이다.

이미지1

1. input 이미지에 Selective Search 알고리즘을 적용해서 물체가 있을 법한 영역 2천여개를 추출한다.
2. 추출한 영역들은 227x227 크기로 resize(warp)한다. 
3. ImageNet으로 사전 학습된 CNN을 통과시켜서 4096 차원의 특징 벡터를 추출한다.
4. 추출된 벡터를 각가의 Object 종류의 클래스마다 학습시켜놓은 SVM 분류기에 통과시킨다.
5. Bounding Box Regression(회기)를 통해 박스 영역들의 위치를 점점 조절한다.

### 1. Region Proposal
Region Propsal은 입력 이미지에서 물체가 있을 법한 영역을 추출하는 것이다. RCNN은 Selective Search라는 알고리즘을 적용해서 2천여개의 물체가 있을 법한 영역을 추출한다. Selective Search는 다음 이미지처럼 주변 픽셀 간의 유사도를 기준으로 Segmantation을 진행한 후 이를 기준으로 물체가 있을 법한 박스를 추출한다.

이미지2

### 2. Feature Extraction
Selective Search를 통해 추출한 2천여개의 박스 영역을 227x227 크기로 resize 한 후, 사전 훈련된 CNN 모델을 통과시켜서 4096 차원의 특징 벡터를 추출한다. 논문에서는 이미지넷 데이터(ILSVRC2012 classification)로 미리 학습된 CNN 모델을 가져온 다음, fine tune하는 방식을 취했다. 그리고 분류기의 마지막 층을 Object Detection의 클래스 수 N개에 아무 물체 없는 배경까지 포함한 N+1로 맞췄다.

다음 표에서 fine tuning을 했을 때 더 좋은 성능을 얻은 것을 알 수 있다. 마지막 행의 BB라고 적힌 것은 Bounding Box Regression을 적용한 것인데, 이는 아래에서 다룬다.(mAP는 Object Detection에서 많이 사용되는 정확도 측정 지표이다.)

이미지3

### 3. Classification
CNN을 통해 추출한 벡터를 각각의 클래스 별로 SVM 분류기를 학습시킨다. 입력된 벡터를 놓고 이것이 해당 물체가 맞는지 아닌지를 구분하는 분류기 모델을 학습시키는 것이다. 굳이 이미 학습된 CNN 분류기 대신 SVM을 사용하는 이유는 명확하지는 않지만 논문에서는 더 좋은 성능을 내기 때문이라고 나와있기 때문에 그냥 넘어가겠다.

### 4. Non-Maximum Suppression
SVM을 통과한 후 각각의 박스 영역들은 어떤 물체일 확률(Score)을 가지게 되었다. 하지만 모든 박스 영역이 필요한것은 아니다. 아래 이미지처럼 만약 동일한 물체에 여러개의 박스가 겹쳐있는 것이라면 가장 Score가 높은 박스만 남기면 된다. 이러한 과정을 Non-Maximum Suppression이라고 한다.

이미지4

서로 다른 두 박스가 동일한 물체에 쳐져 있다는 것을 어떻게 알 수 있을까? 이때 사용되는 개념이 IoU이다. 

이미지5

IoU는 쉽게 말해서 두 박스가 일치할 수록 1에 가까운 값이 나오게 되는 지표다. 논문에서는 IoU > 0.5일때 두개의 박스가 겹쳐 있다고 판단하고 Non-Maximum Suppression을 적용한다.

### 5. Bounding Box Regression
지금까지 처리한 박스 영역의 정확도는 상당히 낮다. 그렇기 때문에 박스 영역의 위치를 조정하는 Bounding Box Regression을 통해 모델의 성능을 향상시킨다. 먼저 박스 영역 하나의 위치를 다음과 같이 표기할 수 있다.

이미지6

여기서 x,y는 이미지의 중심점 w,h는 이미지의 너비와 높이다. Ground Truth(정답)에 해당하는 박스의 위치는 다음과 같이 표기할 수 있다.

이미지7

우리의 목표는 P의 위치를 G에 최대한 가깝게 이동시키는 함수를 학습시키는 것이다. P를 input으로 받아서 x,y,w,h를 각각 이동시키는 함수들을 다음과 같이 표현할 수 있다.

이미지8

x,y는 점이기 때문에 이미지 크기에 상관 없이 위치만 이동시키고, 너비와 높이는 이미지 크기에 비례해서 조정하는 특성을 반영해서 P를 이동시키는 함수는 다음과 같다.

이미지9

우리가 학습을 통해 얻고 싶은 함수는 d함수들이다. d함수들을 구하기 위해서 앞서 CNN을 통과할 때 pool5 층에서 얻어낸 feature 벡터를 사용한다. 그리고 함수에 학습 가능한 웨이트 벡터를 주어 계산한다. 식으로 나타내면 다음과 같다.

이미지10

가중치(w)를 학습시킬 손실 함수는 다음과 같다.(MSE 에러 함수에 L2 규제를 적용한 함수. 람다는 1000으로 지정.)

이미지11

이 식에서 t는 P를 G로 이동시키기 위해 필요한 이동량이다.

이미지12

### 6. 모델의 성능
테스트 시에 RCNN은 이미지 하나당 GPU에서 13초, CPU에서 54초가 걸린다고 한다. 속도저하의 가장 큰 이유는 selective search를 통해서 찾은 2천여개의 영역에 모두 CNN 추론을 진행하기 때문이다. 정확도는 Pascal VOC를 기준으로 53.7%를 기록했는데 이는 당시 기존의 기록을 모두 뛰어넘는 엄청난 정확도이다.

참고 자료
[1] Ross et al, Rich feature hierarchies for accurate object detection and semantic segmentation, 2014

[2] Lunit, R-CNNs Tutorial, https://blog.lunit.io/2017/06/01/r-cnns-tutorial/  

[3] 개인 블로그 ,
https://yeomko.tistory.com/13

[4] standford 231b, selective search, http://vision.stanford.edu/teaching/cs231b_spring1415/slides/ssearch_schuyler.pdf 

 
