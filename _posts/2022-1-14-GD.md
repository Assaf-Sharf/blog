---
layout: single
title: "경사하강법(입문)"
categories: Data_ML
tag:
  [
    python,
    머신러닝,
    blog,
    github,
    경사하강법,
    멀티캠퍼스,
    국비지원,
    교육,
    분석,
    데이터,
    파이썬,
  ]
toc: true
sidebar:
  nav: "docs"
---

## 1. 경사하강법(입문)

- 자료 출처 : 모두의 딥러닝

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 공부시간 X와 성적 Y의 리스트
data = [[2, 81], [4,93],[6,91],[8,97]]
x = [i[0] for i in data] # 리스트컴프리핸션
y = [i[1] for i in data]

# 그래프로 표시
plt.figure(figsize=(8,5))
plt.scatter(x,y)
plt.show()
```

![output_1_0](https://user-images.githubusercontent.com/67591105/149451575-55819b61-3a40-4d92-9b01-2c6a978752c6.png)

```python
# 변수 정의

# 리스트 x, y를 넘파이 배열로 변환
    # 인덱스를 부여해 하나씩 계산이 가능하도록 바꿔주는 과정
x_data = np.array(x)
y_data = np.array(y)

# 기울기 a, 절편 b
a = 0
b = 0

y_hat = a * x_data + b   #  y예측치
error = y_data - y_hat   # 오차

lr = 0.03 # 학습률 (너무 큰 오차함수값을 조절해주는 역할 )

epochs = 2001 # 반복횟수

print(y_hat, error)
```

    [0 0 0 0] [81 93 91 97]

```python
# a,b 값 업데이트

a_diff = -(2/len(x_data)) * sum(x_data * (error))
    # 오차함수를 a로 편미분
        # 편미분: 특정 변수를 제외한 나머지는 상수로 취급하는 미분
b_diff = -(2/len(x_data)) * sum(error)
    # 오차함수를 b로 편미분
a = a - lr * a_diff  # 학습률을 곱해 a값 업데이트
b = b - lr * b_diff  # 학습률을 곱해 b값 업데이트

print(a, b) # 업데이트된 a,b
print(y_hat, error)
print(a_diff, b_diff)
```

    27.84 5.43
    [0 0 0 0] [81 93 91 97]
    -928.0 -181.0

```python
# for문을 통해 a, b 업데이트를 반복
    # 최소의 오차값을 찾아감 (a,b의 변화가 없어질 때까지)
for i in range(epochs):
    y_hat = a * x_data + b
    error = y_data - y_hat
    a_diff = -(2/len(x_data))*sum(x_data*error)
    b_diff = -(2/len(x_data))*sum(error)
    a = a - lr * a_diff
    b = b - lr * b_diff
    if i % 100 == 0:
        print("epoch=%.f, 기울기=%.04f, 절편=%.04f" % (i, a, b))
```

    epoch=0, 기울기=3.9390, 절편=2.1822
    epoch=100, 기울기=7.0274, 절편=50.7888
    epoch=200, 기울기=4.0785, 절편=68.3865
    epoch=300, 기울기=2.9691, 절편=75.0070
    epoch=400, 기울기=2.5517, 절편=77.4978
    epoch=500, 기울기=2.3947, 절편=78.4348
    epoch=600, 기울기=2.3356, 절편=78.7874
    epoch=700, 기울기=2.3134, 절편=78.9200
    epoch=800, 기울기=2.3050, 절편=78.9699
    epoch=900, 기울기=2.3019, 절편=78.9887
    epoch=1000, 기울기=2.3007, 절편=78.9957
    epoch=1100, 기울기=2.3003, 절편=78.9984
    epoch=1200, 기울기=2.3001, 절편=78.9994
    epoch=1300, 기울기=2.3000, 절편=78.9998
    epoch=1400, 기울기=2.3000, 절편=78.9999
    epoch=1500, 기울기=2.3000, 절편=79.0000
    epoch=1600, 기울기=2.3000, 절편=79.0000
    epoch=1700, 기울기=2.3000, 절편=79.0000
    epoch=1800, 기울기=2.3000, 절편=79.0000
    epoch=1900, 기울기=2.3000, 절편=79.0000
    epoch=2000, 기울기=2.3000, 절편=79.0000

```python
# 앞서 구한 기울기와 절편을 이용해 그래프를 그려 봅니다.
y_pred = a * x_data + b
plt.scatter(x, y)
plt.plot([min(x_data), max(x_data)], [min(y_pred), max(y_pred)])
    # 2차원 리스트로 그리면 선형 그래프
plt.show()
```

![output_5_0](https://user-images.githubusercontent.com/67591105/149451596-1ef19bd2-76d4-4dae-a10c-023475204930.png)

```python
print(y_pred, x_data)
```

    [83.59999984 88.19999992 92.8        97.40000008] [2 4 6 8]

```python
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
%matplotlib inline
np.random.seed(0)
```

### y = 4X + 6

Y=4X+6 모델에 실제값을 시뮬레이션하는 데이터 값 생성
(w1=4, w0=6)

```python
np.random.seed(0) # 랜덤으로 형성된 값을 고정시켜줌
np.random.rand(100,1)[0:10]
    # 0~1 표준정규분포를 가진 100행 1열 np array
```

```python
np.random.seed(0)
x = 2*np.random.rand(100,1)
x[0:10] # 0 ~ 2 값 표준정규분포
```

```python
# y는 랜덤으로 생성된 X값에 매칭되는 값
y = 6 + 4*x + np.random.randn(100,1) # random 값은 Noise를 위해 만듦
```

```python
# x, y 데이터 셋 scatter plot으로 시각화
plt.scatter(x,y);
```

** 오차함수를 최소화 할 수 있도록 w0, w1 값을 업데이트하는 함수 생성 **

y_pred = w1\*x + w0

- 예측배열 y_pred는 np.dot(x, w1.T) + w0
  입력 데이터 X(1,2,...,100)이 있다면,
  예측값은 w0 + X(1)*w1 + X(2)*w1 +..+ X(100)\*w1이며, 이는 입력 배열 X와 w1 배열의 내적임.

- 새로운 w1과 w0를 update함

```python
# w1 과 w0 를 업데이트하는 함수
def get_weight_updates(w1, w0, x, y, lr = 0.01):
    N = len(y)
    w1_up = np.zeros_like(w1)
    w0_up = np.zeros_like(w0)

    # 예측 배열 계산하고 예측과 실제 값의 차이 계산
    y_pred = np.dot(x, w1.T)
    diff = y - y_pred

    # w0_update를 dot 행렬 연산으로 구하기 위해 모두 1값을 가진 행렬 생성
    w0_factors = np.ones((N, 1))

    # w1과 w0을 업데이트할 w1_update와 w0_update 계산
    w1_update = -(2/N)*lr*(np.dot(X.T,diff))
    w0_update = -(2/N)*lr*(np.dot(w0_factors.T,diff))

    return w1_update, w0_update
```

```python
w0 = np.zeros((1,1))
w1 = np.zeros((1,1))

w1, w0
```

```python
y_pred = np.dot(x, w1.T) + w0
diff = y - y_pred

print(diff.shape, '\n')
```

```python
w0_factors = np.ones((100,1))

w1_update = -(2/100)*0.01*(np.dot(x.T, diff))
w0_update = -(2/100)*0.01*(np.dot(w0_factors.T,diff))
print(w1_update.shape, w0_update.shape)
```

```python
w1_update
```

```python
w0_update
```

```python
# 반복적으로 w1과 w0를 업데이트 하는 함수
def gradient_descent_steps(x, y, iters=10000)
    # w0와 w1을 모두 0으로 초기화
    w0 = np.zeros((1,1))
    w1 = np.zeros((1,1))

    # iters만큼 반복적으로 get_weight_updates() 호출
    for ind in range(iters):
        # w0, w1 업데이트
        w1_update, w0_update = get_weight_updates(w1, w0, x, y, lr=0.01)
        w1 = w1 - w1_update
        w0 = w0 - w0_update


```
