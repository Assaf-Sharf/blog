---
layout: single
title:  "핸즈온 머신러닝 - 13"
categories : scikit-learn
tag : [scikit-learn, machine-learning, python]
toc: true
toc_sticky: true
---

![header](https://capsule-render.vercel.app/api?type=waving&color=a2dcec&height=300&section=header&text=핸즈온 머신러닝 - 13&fontSize=40&animation=fadeIn&fontAlignY=38&fontColor=FFFFFF)

- 참고 : [핸즈온 머신러닝 2판](http://www.kyobobook.co.kr/product/detailViewKor.laf?mallGb=KOR&ejkGb=KOR&barcode=9791162242964)

------------------------------------------------------

&nbsp;

## 텐서플로에서 데이터 적재와 전처리하기

- 메모리 용량에 맞지 않는 아주 큰 규모의 데이터셋으로 딥러닝 시스템을 훈련해야 하는 경우가 많음
- 다른 딥러닝 라이브러리를 사용해서는 대규모 데이터셋을 효율적으로 로드하고 전처리하도록 구현하기가 어려움
  - 텐서플로에서 멀티스레딩, 큐, 배치, 프리페치 같은 상세한 사항을 모두 처리 해줌

- 데이터 API는 텍스트 파일, 고정 길이의 레코드를 가진 이진 파일, 텐서플로의 TFRecord 포멧을 사용하는 이진 파일에서 데이터를 읽을 수 있음
  - 해당 포멧은 길이가 다른 레코드를 지원
  - 일반적으로 프로토콜 버퍼를 담은 유연하고 효율적인 이진 포멧
  - SQL 데이터베이스에서 읽는 기능도 지원
  - 구글 Big Query 와 같은 다양한 데이터 소스에서 읽을 수 있는 오픈소스 제공
  - **TF 변환**
    - 훈련 전에 전체 훈련 세트에 대해 실행하는 전처리 함수를 작성할 수 있음, 그다음 텐서플로 함수로 변환하고 상용 환경에 배포된 다음 훈련된 모델과 협업하여 새로운 샘플에 대해 동적으로 전처리를 수행
  - **TF 데이터셋**
    - 각종 데이터셋을 다운로드할 수 있는 편리한 함수를 제공, Imagenet과 같은 대용량 데이터셋도 포함

&nbsp;

### 데이터 API

- tf.data.Dataset.from_tensor_slices()를 사용하는 예제

  ```python
  X = tf.range(10) # 셈플 데이터 텐서
  dataset = tf.data.Dataset.from_tensor_slices(X)
  dataset
  >> <TensorSliceDataset shapes: (), types: tf.int32>
  
  for item in dataset:
      print(item)
  >>  tf.Tensor(0, shape=(), dtype=int64)
      tf.Tensor(1, shape=(), dtype=int64)
      tf.Tensor(2, shape=(), dtype=int64)
      tf.Tensor(3, shape=(), dtype=int64)
      tf.Tensor(4, shape=(), dtype=int64)
      tf.Tensor(5, shape=(), dtype=int64)
      tf.Tensor(6, shape=(), dtype=int64)
      tf.Tensor(7, shape=(), dtype=int64)
      tf.Tensor(8, shape=(), dtype=int64)
      tf.Tensor(9, shape=(), dtype=int64)
  ```

  - from_tensor_slices() 함수는 텐서를 받아 첫 번째 차원에 따라 X의 각 원소가 아이템으로 표현되는 tf.data.Dataset을 생성. 
  - 해당 텐서는 0~9 까지의 10개의 아이템을 가짐 (tf.data.Dataset.range(10)과 동일)

&nbsp;

### 연쇄 변환

- 데이터셋이 준비되면 변환 메서드를 호출하여 여러 종류의 변환을 수행할 수 있음

- 각 메서드는 새로운 데이터셋을 반환하므로 다음과 같이 변환 메서드를 연결할 수 있음

  ```python
  dataset = dataset.repeat(3).batch(7)
  for item in dataset:
      print(item)
      
  >>  tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int64)
      tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int64)
      tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int64)
      tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int64)
      tf.Tensor([8 9], shape=(2,), dtype=int64)
      
  dataset = dataset.map(lambda x: x * 2)
  >> 아이템 내부에 곱하기 2
  
  dataset = dataset.unbatch()
  >> batch 해제 하나의 정수 텐서로 반환
  
  dataset = dataset.filter(lambda x: x < 10) 
  >> 필터링도 적용 가능
  
  for item in dataset.take(3):
      print(item)
      
  >>  tf.Tensor(0, shape=(), dtype=int64)
      tf.Tensor(2, shape=(), dtype=int64)
      tf.Tensor(4, shape=(), dtype=int64)
  ```

  - repeat() 메서드를 호출하면 원본 데이터셋의 아이템을 세 차례 반복하는 데이터셋 생성

  - batch() 메서드를 호출하면 다시 새로운 데이터셋이 생성
    - batch 7 이여서 7개씩 묶음으로
    - drop_remainder=True를 호출하면 길이가 모자란 마지막 배치를 삭제
  - take() 메서드로 가져올 개수 지정 가능

&nbsp;

### 데이터 셔플링

- 경사 하강법은 훈련 세트에 있는 샘플이 독립적이고 동일한 분포일 때 최고의 성능을 발휘함

  - 간단한 방법은 shuffle() 메서드를 사용해 샘플을 섞으면 됨
  - 해당 메서드는 먼저 원본 데이터셋의 처음 아이템을 buffer_size 개수만큼 추출하여 버퍼에 채움
  - 그다음 새로운 아이템이 요청되면 이 버퍼에서 랜덤하게 하나를 반환
  - 그리고 원본 데이터셋에서 새로운 아이템을 추출하여 비워진 버퍼를 채움, 원본 데이터셋의 모든 아이템이 사용될 때 까지 반복 -> 버퍼가 비워질 때까지 계속하여 랜덤하게 아이템 반환

- 해당 메서드를 사용하기 위해서는 버퍼 크기를 지정해야 함, 버퍼 크기 충분히 크게 잡는게 중요

  ```python
  dataset = tf.data.Dataset.range(10).repeat(3)
  dataset = dataset.shuffle(buffer_size=3, seed=42).batch(7)
  for item in dataset:
      print(item)
      
  >>  tf.Tensor([1 3 0 4 2 5 6], shape=(7,), dtype=int64)
      tf.Tensor([8 7 1 0 3 2 5], shape=(7,), dtype=int64)
      tf.Tensor([4 6 9 8 9 7 0], shape=(7,), dtype=int64)
      tf.Tensor([3 1 4 5 2 8 7], shape=(7,), dtype=int64)
      tf.Tensor([6 9], shape=(2,), dtype=int64)
  ```

&nbsp;

#### 여러 파일에서 한 줄씩 번갈아 읽기

- 캘리포니아 주택 데이터셋을 적재하고 섞은 다음 훈련 세트, 검증 세트, 테스트 세트로 나누었다고 가정

- 각 세트를 다음과 같은 csv 파일 여러 개로 분할

  ```pyhton
  # 해당 CSV
  MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue
  3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442
  5.3275,5.0,6.490059642147117,0.9910536779324056,3464.0,3.4433399602385686,33.69,-117.39,1.687
  3.1,29.0,7.5423728813559325,1.5915254237288134,1328.0,2.2508474576271187,38.44,-122.98,1.621
  7.1736,12.0,6.289002557544757,0.9974424552429667,1054.0,2.6956521739130435,33.55,-117.7,2.621
  
  # train_filepaths
  >>  ['datasets/housing/my_train_00.csv',
       'datasets/housing/my_train_01.csv',
       'datasets/housing/my_train_02.csv',
       ....
       ]
  
  # 입력 파이프라인 만들기
  filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)
  ## 기본적으로 list_files() 함수는 파일 경로를 섞은 데이터셋을 반환
  ## 섞는 것을 방지하고 싶다면 shuffle = False 지정
  
  
  # iterleave() 메서드 사용해 한 번에 다섯 개의 파일을 한 줄씩 번갈아 읽음
  n_readers = 5
  dataset = filepath_dataset.interleave(
      lambda filepath: tf.data.TextLineDataset(filepath).skip(1),
      cycle_length=n_readers)
  ## 파일의 첫 줄은 header 임으로 skip으로 넘어감
  ```

  - interleave() 메서드는 filepath_dataset에 있는 다섯 개의 파일 경로에서 데이터를 읽는 데이터셋을 생성
    - 해당 메서드에 전달한 함수를 각 파일에 대해 호출하여 새로운 데이터셋을 생성
    - 총 7개의 데이터셋이 생성 (파일 경로 데이터셋, 인터리브 데이터셋, 인터리브 데이터셋에 의해 생성된 5개의 TextLineDataset)

&nbsp;

- 기본적으로 interleave() 메서드는 병렬화를 사용하지 않음

- 각 파일에서 한 번에 한 줄씩 순서대로 읽음, 여러 파일에서 병렬로 읽을때는 num_parallel_calls 매개변수에 원하는 스레드 수 지정

  - 해당 매개변수를 tf.data.experimental.AUTOTUNE으로 지정하면 텐서플로가 가용한 CPU를 기반으로 동적으로 적절한 스레드 개수를 선택할 수 있음 (아직 실험적인 기능)

  ```python
  for line in dataset.take(5):
      print(line.numpy())
      
  >>  b'4.6477,38.0,5.03728813559322,0.911864406779661,745.0,2.5254237288135593,32.64,-117.07,1.504'
      b'8.72,44.0,6.163179916317992,1.0460251046025104,668.0,2.794979079497908,34.2,-118.18,4.159'
      b'3.8456,35.0,5.461346633416459,0.9576059850374065,1154.0,2.8778054862842892,37.96,-122.05,1.598'
      b'3.3456,37.0,4.514084507042254,0.9084507042253521,458.0,3.2253521126760565,36.67,-121.7,2.526'
      b'3.6875,44.0,4.524475524475524,0.993006993006993,457.0,3.195804195804196,34.04,-118.15,1.625'
  ```

  - 위 내용은 CSV의 header 무시하고 첫 번째 행에 해당하며 순서는 랜덤

&nbsp;

#### 데이터 전처리

- 전처리 수행 예제

  ```python
  X_mean = scaler.mean_
  X_std = scaler.scale_
  
  n_inputs = 8 # X_train.shape[-1]
  
  @tf.function
  def preprocess(line):
      defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]
      fields = tf.io.decode_csv(line, record_defaults=defs)
      x = tf.stack(fields[:-1])
      y = tf.stack(fields[-1:])
      return (x - X_mean) / X_std, y
  ```

  - mean, std는 미리 개산 
  - preprocess() 함수는 CSV 한 라인을 받아 파싱, 이를 위해 tf.io.decode_csv() 함수를 사용
    - tf.io.decode_csv()
      - 두 개의 매개변수를 입력 받음
      - 첫 번째는 파싱할 라인과 두 번째는 CSV파일의 각 열에 대한 기본값을 담은 배열
      - 이 배열은 텐서플로에게 각 열의 기본값뿐만 아니라 열 개수와 데이터 타입도 알려줌
      - 스칼라 텐서의 리스트를 반환
      - 1D 텐서 배열을 반환해야 하므로 마지막 열을 제외하고 모든 텐서에 대해 tf.stack() 함수를 호출
      - 해당 함수는 모든 텐서를 쌓아 1D 배열을 만듬, 그 다음 타깃값에도 동일하게 적용
  - 마지막으로 입력 특성에서 평균을 빼고 표준편차로 나누어 스케일을 조정. 그다음 스케일로 조정된 특성과 타깃을 담은 튜플을 반환

&nbsp;

#### 데이터 적재와 전처리를 합치기

- 재사용 가능한 코드를 만들기 위해 지금까지 수행한 모든 것을 하나의 헬퍼 함수로 생성
  - 해당 함수는 CSV 파일에서 데이터를 효율적으로 적재하고 전처리, 셔플링, 반복, 배치를 적용한 데이터셋을 만들어 반환

```python
def csv_reader_dataset(filepaths, repeat=1, n_readers=5,
                       n_read_threads=None, shuffle_buffer_size=10000,
                       n_parse_threads=5, batch_size=32):
    dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat)
    dataset = dataset.interleave(
        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),
        cycle_length=n_readers, num_parallel_calls=n_read_threads)
    dataset = dataset.shuffle(shuffle_buffer_size)
    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)
    dataset = dataset.batch(batch_size)
    return dataset.prefetch(1)
```

&nbsp;

#### 프리배치

- 마지막 prefetch(1)를 호출하면 데이터셋은 항상 한 배치가 미리 준비되도록 함
  - 훈련 알고리즘이 한 배치로 작업을 하는 동안 이 데이터셋이 동시에 다음 배치를 준비
    - 해당 기능을 통해 성능을 크게 향상시킴

&nbsp;

#### tf.keras와 데이터셋 사용하기

- csv_reader_dataset() 함수로 훈련 세트로 사용할 데이터셋을 생성할 수 있음
- tf.keras에서 반복을 처리하므로 반복을 지정할 필요가 없음

```python
train_set = csv_reader_dataset(train_filepaths, repeat=None)
valid_set = csv_reader_dataset(valid_filepaths)
test_set = csv_reader_dataset(test_filepaths)

model = keras.models.Sequential([
    keras.layers.Dense(30, activation="relu", input_shape=X_train.shape[1:]),
    keras.layers.Dense(1),
])
model.compile(loss="mse", optimizer=keras.optimizers.SGD(learning_rate=1e-3))
batch_size = 32
model.fit(train_set, steps_per_epoch=len(X_train) // batch_size, epochs=10, validation_data=valid_set)

model.evaluate(test_set, steps=len(X_test) // batch_size)

new_set = test_set.map(lambda X, y: X) # we could instead just pass test_set, Keras would ignore the labels
X_new = X_test
model.predict(new_set, steps=len(X_new) // batch_size)
```

- keras의 fit() 메서드에 X_train, y_train, X_valid, y_valid 대신 훈련 데이터셋과 검증 데이터셋을 전달하면 됨

- 하지만 대규모의 복잡한 데이터 구조를 지원하지 못함

&nbsp;

### TFRecord 포맷

- 대용량 데이터를 저장하고 효율적으로 읽기 위해 텐서플로가 사용하는 포멧인 TFRecord가 정의
- TFRecord는 크기가 다른 연속된 이진 레코드를 저장하는 단순한 이진 포맷
- tf.io.TFRecordWriter 클래스를 사용해 TFRecord를 손 쉽게 생성 가능

```python
with tf.io.TFRecordWriter("my_data.tfrecord") as f:
    f.write(b"This is the first record")
    f.write(b"And this is the second record")
```

- tf.data.TFRecordDataset을 사용해 하나 이상의 TFRecord를 읽을 수 있음

```python
filepaths = ["my_data.tfrecord"]
dataset = tf.data.TFRecordDataset(filepaths)
for item in dataset:
    print(item)
    
>> tf.Tensor(b'This is the first record', shape=(), dtype=string)
   tf.Tensor(b'And this is the second record', shape=(), dtype=string)
```

&nbsp;

#### 압축된 TFRecord 파일

- optopms 매개변수를 사용하여 압축된 TFRecord 파일 생성 가능

```python
options = tf.io.TFRecordOptions(compression_type="GZIP")
with tf.io.TFRecordWriter("my_compressed.tfrecord", options) as f:
    f.write(b"This is the first record")
    f.write(b"And this is the second record")
```

- 압축된 TFRecord 파일을 읽으려면 압축 형식을 지정해야 함

```python
dataset = tf.data.TFRecordDataset(["my_compressed.tfrecord"],
                                  compression_type="GZIP")
```

&nbsp;

