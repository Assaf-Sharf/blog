---
layout: single
title:  "U-net paper review"
categories : paper
tag : [논문리뷰, U-net, 딥러닝]
toc: true
toc_sticky: true
sidebar:
  nav: "docs"

---





# Introduction

- 기존 AlexNet의 등장으로 분류의 영역이 커짐
- 하지만 기존의 한 이미지를 분류하기보다 픽셀별로 클래스 분류를 해야하는 Localization이 포함된 Classification도 필요성이 늘어남
- 이를 해결하기 위해 픽셀 주변의 영역을 포함한 정보를 통해 해당 객체가 무엇을 나타내는지 판단하는 방법을 사용함
    - 결과적으로 학습 데이터가 이미지 단위가 아닌 이미지 속 일부가 학습 단위가 됨 (풍부한 데이터셋 구현 가능, 속도가 느림, 많은 max-pooling layer 필요, localization accuracy 감소)
        - localization : bbox에서 원하는 객체를 찾는 회귀
        - 전체이미지가 아닌 이미지속 일부 : patch
- 이런 문제점 때문에 U-net이 고안됨



## U-net

- 적은 데이터셋에 학습시켜도 더 좋은 성능이 나오는 모델

- 특성을 추출하여 점저 해상도가 줄어드는 feature map 생성하다가 up-sampling -> CNN -> 축소하는 과정에서 얻은 같은 해상도의 feature map과 합체하는 과정을 거쳐 최종 output 생성

    

- U-net 구조의 특징
    - upsampling과정에서 channel의 숫자가 더 많기 때문에 고해상도의 layer 에 context information을 전달
    - 해상도가 점점 줄어드는 왼쪽 경로와 해상도가 점점 늘어나는 오른쪽 경로의 layer 개수가 거의 대칭되기 때문에 U자 형태를 가짐
    - FC layer를 사용하지 않기에 patch에서 얻은 정보만 가지고 해당 patch에서 classification을 수행할수 있음
        - 이 과정을 전체에 대해 수행하기 때문에 원할하게 segmentation 가능
        - 큰 이미지에 대해서 수행할때 훨신 효율성 높음
    - Data augmentation에도 사용가능, elastic deformation을 적용해 학습용 데이터셋이 기존의 Data augmentation을 사용할 때보다 성능이 늘어남
        - elastic deformation(탄성 변환) : 연속체에서 어떤 힘이나 시간 흐름으로 인해 변화가 발생하는 경우. 이 힘이 제거된 후 변형이 원래처럼 돌아오게 되면 이 변형을 탄성이라 함, 탄성이 있는 경우는 같은 물체라 해도 촬영 방법이나 각도 등에 의해서 다른 결과를 가져올 수 있음
        - Biomedical segmentation에 주로 사용, deformation이 조직(근육 조직 등)의 공통적으로 가장 많이 쓰이는 방식
    - 근접해서 붙어있는 객체를 구분하는데 효과적인 Weighted Loss을 사용
    - 기존의 Semantic Segmentation의 다른 모델보다 높은 성능



## Network Architecture

![Untitled 1](/paper/images/2022-06-21-U-net/Untitled 1.png)

- U-net의 모델구조는 크게 3가지로 구분
    - 수축 경로(Contracting Path)
        - 점진적으로 넓은 범위의 이미지 픽셀을 보며 의미정보(Context Information)을 추출
        - conv의 Downsampling 과정을 반복하여 특징맵을 생성
            1. 3×3 Convolution Layer + ReLu + BatchNorm (No Padding, Stride 1) 
            2. 3×3 Convolution Layer + ReLu + BatchNorm (No Padding, Stride 1)
            3. 2×2 Max-polling Layer (Stride 2)
            
            - padding 사용X, 점차 크기 감소
            - Downsampling 할 때 마다 채널 수를 2배 증가시키면서 진행 (Downsampling 진행 할때마다)
        - 수축경로에서는 주변 픽셀들을 참조하는 범위를 넓혀 가면서 이미지로 부터 Contextual 정보를 추출
        - contracting path는 3x3 사이즈의 kernel을 가진 CNN(3x3 CNN)을 2번 적용 ← 이 과정에서 활성화 함수로 ReLU 사용
        - stride = 2의 2x2 max pooling을 적용해 너비와 높이를 반으로 줄임
    - 전환 구간(Bottle Neck)
        - 수축 경로에서 확장 경로로 전환되는 구간
            1. 3×3 Convolution Layer + ReLu + BatchNorm (No Padding, Stride 1)
            2. 3×3 Convolution Layer + ReLu + BatchNorm (No Padding, Stride 1)
            3. Dropout Layer
            
            > Dropout Layer는 모델을 일반화하고 노이즈에 견고하게 하는 역할 수행
            > 
    - 확장 경로(Expanding Path)
        - 의미정보를 픽셀 위치정보와 결합(Localization)하여 각 픽셀마다 어떤 객체에 속하는지를 구분
        - Upsampling 과정을 반복하여 특징맵(Feature Map)을 생성
            1. 2×2 Deconvolution layer (Stride 2)
            2. 수축 경로에서 동일한 Level의 특징맵을 추출하고 크기를 맞추기 위하여 crop, 이전 Layer에서 생성된 특징맵과 연결
            3. 3×3 Convolution Layer + ReLu + BatchNorm (No Padding, Stride 1)
            4. 3×3 Convolution Layer + ReLu + BatchNorm (No Padding, Stride 1)
            - expansive path는 contracting path와 좌우로 대칭되는 구조
            - 3x3 CNN을 2번 적용한 뒤 feature map의 너비와 높이를 2배로 늘리는 과정을 여러번 반복, upsampling을 할 때마다 적용하는 CNN에 있는 커널의 개수를 반으로 줄임
                - 커널 개수를 반으로 줄인 CNN을 적용하기 전에 반대쪽 contracting path에서 같은 층에 있는 feature map 결합 → 이 과정이 논문에서의 ‘copy and crop’
            - CNN -> upsampling -> 맞은편 contracting path의 feature map을 copy and crop -> CNN ->… 하는 순서
                - crop을 하는 이유는 contracting path에 있는 feature map과 expansive path에 있는 feature map의 해상도가 같지 않아서 수
            - 확장 경로 마지막에는 Class의 수만큼 필터를 가지고 있는 1×1 Convolution Layer가 존재 →1x1 convolution layer를 통과한 후 각 픽셀이 어떤 class에 해당하는지에 대한 정보를 나타내는 3차원(width*height*class) 벡터 생성
    
- foreground, Bottle Neck, background 을 통해서 전체 이미지에 있는 픽셀들을 분류

- U-Net은 23개의 CNN 레이어들로 구성

- segmentation map에 max pooling을 할 때 너비(x-size)와 높이(y-size)에 모두 적용하는 것이 중요
    - segmentation map : 각 픽셀이 예측된 클래스
    
- 모델의 Input은 이미지의 픽셀별 RGB 데이터, 모델의 Output은 이미지의 각 픽셀별 객체 구분 정보(Class)
    - Convolution 연산과정에서 패딩을 사용하지 않으므로 모델의 Output Size는 Input Size보다 작음
        - 572×572×3 input 하면 output은 388x388xclass 이미지 생성
          
            
            $$
            input(width*heught*RGB) -> output(width*height*class)
            $$
            
        





## Training

- U-net에서 다양한 학습 장치를 통해 모델 성능 향상
    - Overlap-tile strategy : 큰 이미지를 겹치는 부분이 있도록 일정크기로 나누고 모델의 Input으로 활용합니다.
    
    - Mirroring Extrapolate : 이미지의 경계 부분을 거울이 반사된 것처럼 확장하여 Input으로 활용합니다.
    
    - Weight Loss : 모델이 객체간 경계를 구분할 수 있도록 Weight Loss를 구성하고 학습합니다.
    
    - Data Augmentation : 적은 데이터로 모델을 잘 학습할 수 있도록 데이터 증강 방법을 활용
    
    - Overlap-tile strategy
      
        ![Untitled 1](/paper/images/2022-06-21-U-net/Untitled 2.png)
        
        - 이미지의 크기가 큰 경우 이미지를 자른 후 각 이미지에 해당하는 Segmentation을 진행
        - U-Net은 Input과 Output의 이미지 크기가 다르기 때문에 위 그림에서 처럼 파란색 영역을 Input으로 넣으면 노란색 영역이 Output으로 추출, 그후 다음 input은 기존 파란색 영역과 겹치게
        - 이렇게 이미지를 자르고 Segmentation 수행하기 때문에 overlap-tile 전략이라고 함
    
    - Mirroring Extrapolate
      
        ![Untitled 1](/paper/images/2022-06-21-U-net/Untitled 3.png)
        
        - 이미지의 경계부분 예측을 수행할때 Padding을 넣어 활용하는게 일반적, U-net에서는 경계에 위치한 이미지를 복사하고 좌우 반전을 통해 Mirror 이미지를 생성한 후 원본 이미지의 주변에 붙여 Input으로 사용
    
    - Weight Loss
      
        ![Untitled 1](/paper/images/2022-06-21-U-net/Untitled 4.png)
        
        - a가 원본이미지, b가 원하는 분할 목표, 3이 분할된 이미지, 4번이 wegiht map 시각화 이미지
        
        - a와 b를 확인했을때 작은 경계까지 분리가 가능하도록 학습이 수행되어야 함, 논문에서는 각 픽셀이 경계와 얼마나 가까운지에 따라서 weight-map을 생성하고 학습할 때 경계에 가까운 픽셀의 loss를 weight-map에 비례하게 증가시켜 경계를 보다 잘 학습함
        
        - weight loss 설명
          
            ![Untitled 1](/paper/images/2022-06-21-U-net/Untitled 5.png)
            
            - a_k(x) : 픽셀 x가 Class k일 값(픽셀 별 모델의 Output)
            - p_k(x) : 픽셀 x가 Class k일 확률(0~1)
            - l(x) : 픽셀 x의 실제 Label
            - w_0 : 논문의 Weight hyper-parameter, 논문에서 10으로 설정
            - σ : 논문의 Weight hyper-parameter, 논문에서 5로 설정
            - d_1(x) : 픽셀 x의 위치로부터 가장 가까운 경계와 거리
            - d_2(x) : 픽셀 x의 위치로부터 두번째로 가까운 경계와 거리
            
            > w(x)는 픽셀 x와 경계의 거리가 가까우면 큰 값을 갖게 되므로 해당 픽셀의 Loss 비중이 커지게 됨
          > 
          
      - weight map c,d 이미지를 확인하면 각 경계부분에서 큰 값을 가지는 것을 확인할수 있음(경계 = 빨간색으로 보임), 객체간 경계가 전체 픽셀에서 차지하는 비중은 매우 작지만, Weight Loss를 사용하면 경계가 구분됨
      
    - Data Augmentation
        - 데이터 양이 적기 때문에 Data Augmentation를 수행해서 모델이 noise에 robust하게함
        - shift, rotation, random-elastic deformation 기법을 통한 Data Augmentation을 구현
            - random-elastic deformation 사용하면 작은 데이터셋에서 segmentation network를 학습할때 좋다고 설명함
        - U-net에서 수행한 Data Augmentation 구현 순서
            1. 3x3 grid에 random displacement vectors을 이용해 smooth deformation을 수행
                - displacement는 10개 픽셀이 가지는 값들의 표준편차를 따르는 가우시안 분포에서 임의로 뽑은 값으로 수행
                - displacement(변위) : 두 지점사이 최단거리
            2. bicubic interpolation을 이용해 픽셀 단위로 displacement를 계산
                - bicubic interpolation :  삼차보간법을 2차원으로 확장 시킨것
                    - 삼차보간법 관련 자료
                    
                    [https://bskyvision.com/789](https://bskyvision.com/789)
                    
                
    
- 정규분포를 이용하여 모델의 파라미터를 초기화하고 학습

- 이미지를 최대한 크게 사용하고 Optimizer에 Momentum = 0.99를 부여하여 일관적이게 학습
  
    
  
  

## 결론

- Layer를 깊게 쌓을 수 있게 하여 복잡한 Task를 잘 수행
- Bottle Neck에서 생기는 정보의 손실을 줄이는 역할 수행
- Weighted Loss는 근거리에 있는 객체를 효과적으로 분리하여 학습
- 모델의 구조가 간단하여 구현이 쉽고, 좋은 성능 기록