---
title : '[DL/CV] 소규모 데이터로 컨브넷 훈련하기 : 데이터 증식 활용하기 🤹'
layout : single
---

# 2. 소규모 데이터로 컨브넷 훈련하기

### 2.0 작은 데이터셋으로 딥러닝 훈련하기
모델을 훈련하기에 '충분한 샘플'이라는 정의는 상대적이다. 우선 훈련하려는 모델의 크기와 깊이에 대해 상대적이다. 예를 들어, 복잡한 문제를 푸는 컨브넷을 수십개의 데이터셋으로만 훈련한다는 것은 불가능하지만 , 모델이 작고 규제가 잘되어 있는 모델로 간단한 문제를 푼다고 하면 수백개의 샘플로도 충분할 수 있다. 

컨브넷은 지역적이고 평행이동으로 변하지 않는 특성을 학습하기 때문에 지각에 관한 문제에서 데이터를 효율적으로 사용한다. 따라서 작은 이미지 데이터셋에서 특성공학을 사용하지 않고 처음부터 컨브넷을 훈련해도 어느정도의 결과를 만들 수 있다. 거기에 더해 딥러닝 모델은 태생적으로 다목적이기 때문에 대규모 이미지 데이터셋인 ImageNet 데이터셋에서 사전 훈련된 모델들을 내려받아서 매우 적은 데이터로 강력한 컴퓨터 비전 모델을 만드는데 사용할 수 있다. (*이것이 딥러닝의 가장 큰 장점 중 하나인 특성 재사용이다.*)

### 2.0.1 데이터 내려받기
이번 글에서 사용할 강아지 vs 고양이 데이터셋은 캐글에서 컴퓨터 비전 대회를 개최할 당시 제공했던 데이터셋이다. 원본 데이터셋을 캐글에서 다운 받을수 있지만 다음 코드로 간단히 데이터를 내려받겠다.


```python
import gdown
gdown.download(id='18uC7WTuEXKJDDxbj-Jq6EjzpFrgE7IAd', output='dogs-vs-cats.zip')
```

    Downloading...
    From: https://drive.google.com/uc?id=18uC7WTuEXKJDDxbj-Jq6EjzpFrgE7IAd
    To: /content/dogs-vs-cats.zip
    100%|██████████| 852M/852M [00:12<00:00, 66.9MB/s]





    'dogs-vs-cats.zip'




```python
!unzip -qq dogs-vs-cats.zip
!unzip -qq train.zip
```

내려받은 데이터셋은 25,000개의 강아지와 고양이 이미지 (각각 12,500개씩)를 담고 있고, 3개의 서브셋이 들어있는 새로운 데이터셋을 만들것이다. 클래스마다 1,000개의 샘플로 이루어진 훈련세트, 클래스마다 500개의 샘플로 이루어진 검증세트, 클래스마다 1,000개의 샘플로 이루어진 테스트세트이다. 

(*소규모 데이터셋으로 컨브넷을 훈련하는 것을 연습하는 것이기 때문에 실제 데이터셋의 일부만 사용한다.*)

shutil 패키지를 사용해서 3개의 서브셋이 들어있는 새로운 데이터셋을 만든다.


```python
import os, shutil, pathlib

original_dir = pathlib.Path("train")  # 원본 데이터셋이 압축 해제되어 있는 디렉터리 경로
new_base_dir = pathlib.Path("cats_vs_dogs_small")  # 서브셋 데이터를 저장할 디렉터리

def make_subset(subset_name, start_index, end_index):
    for category in ("cat", "dog"):
        dir = new_base_dir / subset_name / category
        os.makedirs(dir)
        fnames = [f"{category}.{i}.jpg" for i in range(start_index, end_index)]
        for fname in fnames:
            shutil.copyfile(src=original_dir / fname,
                            dst=dir / fname)

make_subset("train", start_index=0, end_index=1000)  # 카테고리마다 처음 1,000개의 이미지를 훈련 서브셋으로 저장
make_subset("validation", start_index=1000, end_index=1500) # 카테고리마다 그 다음 500개의 이미지를 검정 서브셋으로 저장
make_subset("test", start_index=1500, end_index=2500)  # 카테고리마다 그 다음 1,000개의 이미지를 테스트 서브셋으로 저장
```

이제 훈련 이미지 2,000개, 검증 이미지 1,000개, 테스트 이미지 2,000개가 준비되었다. 

### 2.1 모델 구축하기

[**합성곱 신경망 : 컴퓨터 비전의 기본**](https://hamin-chang.github.io/conv/) 이 글에서 사용했던 Conv2D층과 MaxPooling2D층을 번갈아 쌓은 컨브넷을 사용해서 모델을 구축하겠다. 임의로 선택한 입력 크기 180x180의 입력으로 시작해서 Flatten 층 이전에 7x7 크기의 특성맵으로 줄어들것이다. Dogs vs Cats 문제는 이진분류 문제이므로 합성곱 층 다음 Dense 층은 크기가 1이고 sigmoid 활성화 함수를 사용해서 모델이 보고 있는 샘플이 한 클래스에 속할 확률을 인코딩할 것이다. 또한 모델의 첫 층을 Rescaling 층으로 시작하는데, 이 층은 [0,255] 범위인 이미지 입력을 [0,1] 범위로 스케일링하는 층이다.


```python
from tensorflow import keras
from tensorflow.keras import layers

inputs = keras.Input(shape=(180, 180, 3))  # 임의로 입력 이미지 크기를 180x180으로 설정
x = layers.Rescaling(1./255)(inputs)  # 입력을 255로 나눠서 [0,1]범위로 스케일링
x = layers.Conv2D(filters=32, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=64, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=128, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.Flatten()(x)
outputs = layers.Dense(1, activation="sigmoid")(x)
model = keras.Model(inputs=inputs, outputs=outputs)
```


```python
model.summary()
```

    Model: "model"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     input_1 (InputLayer)        [(None, 180, 180, 3)]     0         
                                                                     
     rescaling (Rescaling)       (None, 180, 180, 3)       0         
                                                                     
     conv2d (Conv2D)             (None, 178, 178, 32)      896       
                                                                     
     max_pooling2d (MaxPooling2D  (None, 89, 89, 32)       0         
     )                                                               
                                                                     
     conv2d_1 (Conv2D)           (None, 87, 87, 64)        18496     
                                                                     
     max_pooling2d_1 (MaxPooling  (None, 43, 43, 64)       0         
     2D)                                                             
                                                                     
     conv2d_2 (Conv2D)           (None, 41, 41, 128)       73856     
                                                                     
     max_pooling2d_2 (MaxPooling  (None, 20, 20, 128)      0         
     2D)                                                             
                                                                     
     conv2d_3 (Conv2D)           (None, 18, 18, 256)       295168    
                                                                     
     max_pooling2d_3 (MaxPooling  (None, 9, 9, 256)        0         
     2D)                                                             
                                                                     
     conv2d_4 (Conv2D)           (None, 7, 7, 256)         590080    
                                                                     
     flatten (Flatten)           (None, 12544)             0         
                                                                     
     dense (Dense)               (None, 1)                 12545     
                                                                     
    =================================================================
    Total params: 991,041
    Trainable params: 991,041
    Non-trainable params: 0
    _________________________________________________________________


컴파일 단계에서는 RMS 옵티마이저, 손실함수로 이진 크로스엔트로피(binary crossentropy) (*모델의 마지막이 하나의 시그모이드 유닛이기 때문에*)를 사용한다.


```python
model.compile(loss="binary_crossentropy",
              optimizer="rmsprop",
              metrics=["accuracy"])
```

### 2.2 데이터 전처리
현재 데이터가 JPEG 파일로 되어 있기 때문에 모델에 데이터를 주입하려면 다음의 과정을 거쳐야한다.


1.   사진 파일을 읽는다.
2.   JPEG 콘텐츠를 RGB 픽셀값으로 디코딩한다.
3.   RGB 픽셀값을 부동 소수점 타입의 텐서로 변환한다.
4.   동일한 크기 (여기서 임의로 정한 180x180)로 변환한다.
5.   배치로 묶는다.(하나의 배치당 32개의 이미지로 구성)

위의 과정을 자동으로 처리하는 케라스의 유틸리티를 사용한다. 케라스가 제공하는 image_dataset_from_directory( ) 함수를 사용하면 디스크에 있는 이미지 파일을 자동으로 전처리된 텐서의 배치로 변환할 수 있다. 

다음 코드와 함께 image_dataset_from_directory( ) 함수를 알아보자



```python
from tensorflow.keras.utils import image_dataset_from_directory

train_dataset = image_dataset_from_directory(
    new_base_dir / 'train', # 먼저 directory의 서브 디렉터리를 찾고 , 이미지 파일을 인덱싱
    image_size = (180,180), # 각 서브 디렉터리에 있는 이미지 파일을 동일한 크기로 변환
    batch_size = 32) # 한 배치당 32개의 이미지가 들어있게 배치로 묶음

validation_dataset = image_dataset_from_directory( # 검증 데이터도 똑같이 전처리
    new_base_dir / "validation",
    image_size=(180, 180),
    batch_size=32)

test_dataset = image_dataset_from_directory(   # 테스트 데이터도 똑같이 전처리
    new_base_dir / "test",
    image_size=(180, 180),
    batch_size=32)

''' 텐서플로 Dataset 객체의 유용한 메서드
.shuffle(buffer_size) : 버퍼 안의 원소를 섞는다.
.prefetch(buffer_size) : 장치 활용도를 높이기 위해 GPU 메모리에 로드할 데이터를 미리 준비한다.
.map(callable) : 임의의 변환을 데이터셋의 원소에 적용한다. (ex.원소 크기를 (16,)->(4,)로 변환)
(callable 함수는 데이터셋이 반환하는 1개의 원소를 입력으로 기대한다.)'''
```

    Found 2000 files belonging to 2 classes.
    Found 1000 files belonging to 2 classes.
    Found 2000 files belonging to 2 classes.





    ' 텐서플로 Dataset 객체의 유용한 메서드\n.shuffle(buffer_size) : 버퍼 안의 원소를 섞는다.\n.prefetch(buffer_size) : 장치 활용도를 높이기 위해 GPU 메모리에 로드할 데이터를 미리 준비한다.\n.map(callable) : 임의의 변환을 데이터셋의 원소에 적용한다. (ex.원소 크기를 (16,)->(4,)로 변환)\n(callable 함수는 데이터셋이 반환하는 1개의 원소를 입력으로 기대한다.)'



위의 코드에서 준비한 Dataset 객체의 출력 하나를 살펴보자.


```python

for data_batch, labels_batch in train_dataset:
    print("데이터 배치 크기:", data_batch.shape)
    print("레이블 배치 크기:", labels_batch.shape)
    break
```

    데이터 배치 크기: (32, 180, 180, 3)
    레이블 배치 크기: (32,)


위의 출력은 180x180 RGB 이미지의 배치 ((32,180,180,3) 크기)와 정수 레이블 배치((32,) 크기)이다. 각 배치에는 32개의 샘플이 있다.

이제 준비한 데이터셋으로 모델을 훈련해보자.


```python
callbacks = [ 
    keras.callbacks.ModelCheckpoint(           # 콜백 사용
        filepath="convnet_from_scratch.keras", # 모델 저장 경로
        save_best_only=True,                   # val_loss 값이 이전보다 낮을 때만 저장
        monitor="val_loss")
]
history = model.fit(
    train_dataset,
    epochs=30,
    validation_data=validation_dataset,        # 검증 데이터
    callbacks=callbacks)
```

    Epoch 1/30
    63/63 [==============================] - 17s 97ms/step - loss: 0.7200 - accuracy: 0.5315 - val_loss: 0.6768 - val_accuracy: 0.5750
    Epoch 2/30
    63/63 [==============================] - 6s 90ms/step - loss: 0.7213 - accuracy: 0.5785 - val_loss: 0.9444 - val_accuracy: 0.5100
    Epoch 3/30
    63/63 [==============================] - 5s 80ms/step - loss: 0.6674 - accuracy: 0.6230 - val_loss: 0.6560 - val_accuracy: 0.5880
    Epoch 4/30
    63/63 [==============================] - 5s 71ms/step - loss: 0.6304 - accuracy: 0.6725 - val_loss: 0.6412 - val_accuracy: 0.6320
    Epoch 5/30
    63/63 [==============================] - 6s 98ms/step - loss: 0.5930 - accuracy: 0.6930 - val_loss: 0.5871 - val_accuracy: 0.6910
    Epoch 6/30
    63/63 [==============================] - 5s 71ms/step - loss: 0.5564 - accuracy: 0.7180 - val_loss: 0.6310 - val_accuracy: 0.6670
    Epoch 7/30
    63/63 [==============================] - 5s 70ms/step - loss: 0.4986 - accuracy: 0.7665 - val_loss: 0.5895 - val_accuracy: 0.7030
    Epoch 8/30
    63/63 [==============================] - 5s 70ms/step - loss: 0.4653 - accuracy: 0.7885 - val_loss: 0.6056 - val_accuracy: 0.7140
    Epoch 9/30
    63/63 [==============================] - 5s 75ms/step - loss: 0.3962 - accuracy: 0.8130 - val_loss: 0.7551 - val_accuracy: 0.6860
    Epoch 10/30
    63/63 [==============================] - 6s 91ms/step - loss: 0.3414 - accuracy: 0.8520 - val_loss: 0.6346 - val_accuracy: 0.7260
    Epoch 11/30
    63/63 [==============================] - 6s 85ms/step - loss: 0.2960 - accuracy: 0.8765 - val_loss: 0.7377 - val_accuracy: 0.7080
    Epoch 12/30
    63/63 [==============================] - 6s 84ms/step - loss: 0.2524 - accuracy: 0.8910 - val_loss: 0.8512 - val_accuracy: 0.6840
    Epoch 13/30
    63/63 [==============================] - 7s 104ms/step - loss: 0.1845 - accuracy: 0.9235 - val_loss: 1.1699 - val_accuracy: 0.7030
    Epoch 14/30
    63/63 [==============================] - 6s 85ms/step - loss: 0.1530 - accuracy: 0.9415 - val_loss: 1.7814 - val_accuracy: 0.6510
    Epoch 15/30
    63/63 [==============================] - 7s 107ms/step - loss: 0.1198 - accuracy: 0.9575 - val_loss: 1.0960 - val_accuracy: 0.7090
    Epoch 16/30
    63/63 [==============================] - 7s 105ms/step - loss: 0.1165 - accuracy: 0.9610 - val_loss: 1.2967 - val_accuracy: 0.7060
    Epoch 17/30
    63/63 [==============================] - 5s 71ms/step - loss: 0.0862 - accuracy: 0.9660 - val_loss: 1.4664 - val_accuracy: 0.7130
    Epoch 18/30
    63/63 [==============================] - 5s 70ms/step - loss: 0.0621 - accuracy: 0.9815 - val_loss: 1.7707 - val_accuracy: 0.7280
    Epoch 19/30
    63/63 [==============================] - 5s 70ms/step - loss: 0.0672 - accuracy: 0.9740 - val_loss: 1.5680 - val_accuracy: 0.7260
    Epoch 20/30
    63/63 [==============================] - 5s 70ms/step - loss: 0.0789 - accuracy: 0.9750 - val_loss: 2.3996 - val_accuracy: 0.6900
    Epoch 21/30
    63/63 [==============================] - 5s 71ms/step - loss: 0.0744 - accuracy: 0.9770 - val_loss: 1.6955 - val_accuracy: 0.7160
    Epoch 22/30
    63/63 [==============================] - 5s 69ms/step - loss: 0.0644 - accuracy: 0.9755 - val_loss: 1.7621 - val_accuracy: 0.7070
    Epoch 23/30
    63/63 [==============================] - 5s 69ms/step - loss: 0.0466 - accuracy: 0.9850 - val_loss: 3.0233 - val_accuracy: 0.6730
    Epoch 24/30
    63/63 [==============================] - 5s 69ms/step - loss: 0.0583 - accuracy: 0.9835 - val_loss: 1.7579 - val_accuracy: 0.7320
    Epoch 25/30
    63/63 [==============================] - 5s 70ms/step - loss: 0.0476 - accuracy: 0.9850 - val_loss: 2.2815 - val_accuracy: 0.7080
    Epoch 26/30
    63/63 [==============================] - 5s 69ms/step - loss: 0.0287 - accuracy: 0.9925 - val_loss: 2.3078 - val_accuracy: 0.6940
    Epoch 27/30
    63/63 [==============================] - 5s 70ms/step - loss: 0.0668 - accuracy: 0.9840 - val_loss: 2.2340 - val_accuracy: 0.7220
    Epoch 28/30
    63/63 [==============================] - 5s 76ms/step - loss: 0.0588 - accuracy: 0.9810 - val_loss: 2.6606 - val_accuracy: 0.7110
    Epoch 29/30
    63/63 [==============================] - 5s 73ms/step - loss: 0.0361 - accuracy: 0.9875 - val_loss: 2.9502 - val_accuracy: 0.7080
    Epoch 30/30
    63/63 [==============================] - 5s 70ms/step - loss: 0.0475 - accuracy: 0.9875 - val_loss: 2.5123 - val_accuracy: 0.7250


훈련한 모델의 손실과 정확도를 그래프로 나타내보자.


```python
import matplotlib.pyplot as plt
accuracy = history.history["accuracy"]
val_accuracy = history.history["val_accuracy"]
loss = history.history["loss"]
val_loss = history.history["val_loss"]
epochs = range(1, len(accuracy) + 1)
plt.plot(epochs, accuracy, "bo", label="Training accuracy")
plt.plot(epochs, val_accuracy, "b", label="Validation accuracy")
plt.title("Training and validation accuracy")
plt.legend()
plt.figure()
plt.plot(epochs, loss, "bo", label="Training loss")
plt.plot(epochs, val_loss, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.legend()
plt.show()
```


    
![png](20221018_files/20221018_18_0.png)
    



    
![png](20221018_files/20221018_18_1.png)
    


위의 그래프에서 볼 수 있듯이 모델이 과대적합된다. 훈련 정확도는 시간이 지남에 따라 100%에 가까이 도달합니다. 하지만 검증 정확도는 13번째 에포크만에 거의 최고점에 다다르고 더이상 진전되지 않는다. 그럼 테스트 정확도는 어떨까? 과대적합 되기 전의 상태를 평가하기 위해 콜백으로 저장한 모델을 로드하자.


```python

test_model = keras.models.load_model("convnet_from_scratch.keras")
test_loss, test_acc = test_model.evaluate(test_dataset)
print(f"테스트 정확도: {test_acc:.3f}")
```

    63/63 [==============================] - 3s 36ms/step - loss: 0.5975 - accuracy: 0.6900
    테스트 정확도: 0.690


테스트 정확도는 69%를 얻었다.

이번장에서 다루는 데이터셋의 크기가 작기 때문에 과대적합을 막는 것이 가장 중요한 요소이다. 이전 글에서 언급한 드롭아웃이나 L2 규제 등의 방법들도 있지만 , 컴퓨터 비전에 특화된 과대적합을 막는 방법인 **데이터 증식**을 시도하겠다.

### 2.3 데이터 증식
데이터 증식은 기존 훈련 샘플을 이용해서 더 많은 훈련 데이터를 생성하는 방법이다. 여러가지 랜덤한 변환을 적용해서 샘플을 늘리는 방법이 있다. 데이터 증식의 궁극적인 목표는 모델이 훈련할 때 정확히 같은 데이터를 두번 만나지 않도록 하는 것이다. 그러면 모델이 데이터의 여러 측면을 학습하므로 더 잘 일반화할 수 있다.

케라스에서 모델의 시작부분에 **데이터 증식층**을 추가할 수 있다. 다음 코드는 데이터 증식 층의 예시 코드다.


```python
data_augmentation = keras.Sequential(
    [
        layers.RandomFlip("horizontal"),  # 랜덤하게 50% 이미지를 수평을 뒤집는다.
        layers.RandomRotation(0.1),       # [-10%,+10%] 범위 안에서 랜덤한 값만큼 이미지 회전
        layers.RandomZoom(0.2),  ])         # [-20%,+20%] 범위 안에서 랜덤한 비율만큼 이미지 확대 or 축소
```

증식된 이미지를 출력해보자.


```python
plt.figure(figsize=(10, 10))
for images, _ in train_dataset.take(1):
    for i in range(9):
        augmented_images = data_augmentation(images)
        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(augmented_images[0].numpy().astype("uint8"))
        plt.axis("off")
```


    
![png](20221018_files/20221018_24_0.png)
    


출력된 이미지들을 보면 알겠지만 여전히 입력 데이터들 사이에 상호 연관성이 크다. 즉, 기존 정보의 재조합만 가능하기 때문에 완전히 과대적합을 제거할 수는 없다. 과대적합을 더욱 확실히 억제하기 위해 Dense 층 직전에 Dropout층을 추가한다.

또한 이미지 증식층은 Dropout 층처럼 predict나 evaluate같은 테스트 데이터를 사용하는 단계에서는 동작하는 않는다. 즉, 모델을 평가할 때는 데이터 증식과 드롭아웃이 없는 모델처럼 동작한다.


```python
inputs = keras.Input(shape=(180, 180, 3))
x = data_augmentation(inputs)           # 이미지 증식층 추가
x = layers.Rescaling(1./255)(x)
x = layers.Conv2D(filters=32, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=64, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=128, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.Flatten()(x)
x = layers.Dropout(0.5)(x)              # Dropout 층 추가
outputs = layers.Dense(1, activation="sigmoid")(x)
model = keras.Model(inputs=inputs, outputs=outputs)

model.compile(loss="binary_crossentropy",
              optimizer="rmsprop",
              metrics=["accuracy"])

```


```python
callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath="convnet_from_scratch_with_augmentation.keras",
        save_best_only=True,
        monitor="val_loss")
]
history = model.fit(
    train_dataset,
    epochs=100,
    validation_data=validation_dataset,
    callbacks=callbacks)
```

    Epoch 1/100
    63/63 [==============================] - 8s 101ms/step - loss: 0.7366 - accuracy: 0.5020 - val_loss: 0.6907 - val_accuracy: 0.5000
    Epoch 2/100
    63/63 [==============================] - 6s 93ms/step - loss: 0.6942 - accuracy: 0.5330 - val_loss: 0.6770 - val_accuracy: 0.5590
    Epoch 3/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.6927 - accuracy: 0.5510 - val_loss: 0.8064 - val_accuracy: 0.5150
    Epoch 4/100
    63/63 [==============================] - 6s 98ms/step - loss: 0.6721 - accuracy: 0.5970 - val_loss: 0.6623 - val_accuracy: 0.5900
    Epoch 5/100
    63/63 [==============================] - 6s 92ms/step - loss: 0.6472 - accuracy: 0.6290 - val_loss: 0.7294 - val_accuracy: 0.5530
    Epoch 6/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.6408 - accuracy: 0.6365 - val_loss: 0.6342 - val_accuracy: 0.6630
    Epoch 7/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.6093 - accuracy: 0.6890 - val_loss: 0.5593 - val_accuracy: 0.6920
    Epoch 8/100
    63/63 [==============================] - 6s 92ms/step - loss: 0.6034 - accuracy: 0.6910 - val_loss: 0.5635 - val_accuracy: 0.6970
    Epoch 9/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.5731 - accuracy: 0.7045 - val_loss: 0.5722 - val_accuracy: 0.6940
    Epoch 10/100
    63/63 [==============================] - 7s 106ms/step - loss: 0.5628 - accuracy: 0.7080 - val_loss: 0.5701 - val_accuracy: 0.7080
    Epoch 11/100
    63/63 [==============================] - 6s 93ms/step - loss: 0.5564 - accuracy: 0.7220 - val_loss: 0.7103 - val_accuracy: 0.6690
    Epoch 12/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.5399 - accuracy: 0.7330 - val_loss: 0.5260 - val_accuracy: 0.7270
    Epoch 13/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.5245 - accuracy: 0.7415 - val_loss: 0.5854 - val_accuracy: 0.7070
    Epoch 14/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.5260 - accuracy: 0.7375 - val_loss: 0.4788 - val_accuracy: 0.7720
    Epoch 15/100
    63/63 [==============================] - 6s 93ms/step - loss: 0.5038 - accuracy: 0.7630 - val_loss: 0.5134 - val_accuracy: 0.7420
    Epoch 16/100
    63/63 [==============================] - 6s 93ms/step - loss: 0.5038 - accuracy: 0.7570 - val_loss: 0.4751 - val_accuracy: 0.7680
    Epoch 17/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.4806 - accuracy: 0.7685 - val_loss: 0.4781 - val_accuracy: 0.7740
    Epoch 18/100
    63/63 [==============================] - 6s 92ms/step - loss: 0.4832 - accuracy: 0.7705 - val_loss: 0.4908 - val_accuracy: 0.7490
    Epoch 19/100
    63/63 [==============================] - 6s 92ms/step - loss: 0.4620 - accuracy: 0.7850 - val_loss: 0.7823 - val_accuracy: 0.6990
    Epoch 20/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.4638 - accuracy: 0.7890 - val_loss: 0.4866 - val_accuracy: 0.7720
    Epoch 21/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.4385 - accuracy: 0.7945 - val_loss: 0.4414 - val_accuracy: 0.8110
    Epoch 22/100
    63/63 [==============================] - 6s 93ms/step - loss: 0.4320 - accuracy: 0.8055 - val_loss: 0.4776 - val_accuracy: 0.7870
    Epoch 23/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.4391 - accuracy: 0.7990 - val_loss: 0.4527 - val_accuracy: 0.7970
    Epoch 24/100
    63/63 [==============================] - 6s 98ms/step - loss: 0.4046 - accuracy: 0.8210 - val_loss: 0.4723 - val_accuracy: 0.7880
    Epoch 25/100
    63/63 [==============================] - 6s 97ms/step - loss: 0.4148 - accuracy: 0.8060 - val_loss: 0.4603 - val_accuracy: 0.7960
    Epoch 26/100
    63/63 [==============================] - 6s 97ms/step - loss: 0.4099 - accuracy: 0.8240 - val_loss: 0.4889 - val_accuracy: 0.7940
    Epoch 27/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.3970 - accuracy: 0.8110 - val_loss: 0.7713 - val_accuracy: 0.7430
    Epoch 28/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.3954 - accuracy: 0.8310 - val_loss: 0.4524 - val_accuracy: 0.8110
    Epoch 29/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.3820 - accuracy: 0.8245 - val_loss: 0.5146 - val_accuracy: 0.8030
    Epoch 30/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.3792 - accuracy: 0.8380 - val_loss: 0.4337 - val_accuracy: 0.8120
    Epoch 31/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.3564 - accuracy: 0.8540 - val_loss: 0.5081 - val_accuracy: 0.7770
    Epoch 32/100
    63/63 [==============================] - 6s 98ms/step - loss: 0.3727 - accuracy: 0.8335 - val_loss: 0.4726 - val_accuracy: 0.8090
    Epoch 33/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.3614 - accuracy: 0.8415 - val_loss: 0.5023 - val_accuracy: 0.8080
    Epoch 34/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.3529 - accuracy: 0.8485 - val_loss: 0.4630 - val_accuracy: 0.8110
    Epoch 35/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.3374 - accuracy: 0.8620 - val_loss: 0.8080 - val_accuracy: 0.7570
    Epoch 36/100
    63/63 [==============================] - 6s 93ms/step - loss: 0.3543 - accuracy: 0.8580 - val_loss: 0.4616 - val_accuracy: 0.8230
    Epoch 37/100
    63/63 [==============================] - 6s 97ms/step - loss: 0.3373 - accuracy: 0.8525 - val_loss: 0.4217 - val_accuracy: 0.8340
    Epoch 38/100
    63/63 [==============================] - 7s 106ms/step - loss: 0.3332 - accuracy: 0.8550 - val_loss: 0.4735 - val_accuracy: 0.8190
    Epoch 39/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.3032 - accuracy: 0.8675 - val_loss: 0.4315 - val_accuracy: 0.8300
    Epoch 40/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.2977 - accuracy: 0.8730 - val_loss: 0.4437 - val_accuracy: 0.8470
    Epoch 41/100
    63/63 [==============================] - 6s 99ms/step - loss: 0.3005 - accuracy: 0.8645 - val_loss: 0.3975 - val_accuracy: 0.8230
    Epoch 42/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.2899 - accuracy: 0.8790 - val_loss: 0.5765 - val_accuracy: 0.8180
    Epoch 43/100
    63/63 [==============================] - 6s 91ms/step - loss: 0.3003 - accuracy: 0.8725 - val_loss: 0.5932 - val_accuracy: 0.7840
    Epoch 44/100
    63/63 [==============================] - 6s 93ms/step - loss: 0.3084 - accuracy: 0.8860 - val_loss: 0.4764 - val_accuracy: 0.7960
    Epoch 45/100
    63/63 [==============================] - 6s 93ms/step - loss: 0.2841 - accuracy: 0.8820 - val_loss: 0.4053 - val_accuracy: 0.8260
    Epoch 46/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.2676 - accuracy: 0.8890 - val_loss: 0.4277 - val_accuracy: 0.8430
    Epoch 47/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.2758 - accuracy: 0.8860 - val_loss: 0.4477 - val_accuracy: 0.8250
    Epoch 48/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.2686 - accuracy: 0.8930 - val_loss: 0.5004 - val_accuracy: 0.8220
    Epoch 49/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.2779 - accuracy: 0.8915 - val_loss: 0.5253 - val_accuracy: 0.8040
    Epoch 50/100
    63/63 [==============================] - 6s 99ms/step - loss: 0.2503 - accuracy: 0.8955 - val_loss: 0.9680 - val_accuracy: 0.7690
    Epoch 51/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.2727 - accuracy: 0.8940 - val_loss: 0.4147 - val_accuracy: 0.8210
    Epoch 52/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.2417 - accuracy: 0.8985 - val_loss: 0.6074 - val_accuracy: 0.8240
    Epoch 53/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.2429 - accuracy: 0.8970 - val_loss: 0.4815 - val_accuracy: 0.8230
    Epoch 54/100
    63/63 [==============================] - 6s 98ms/step - loss: 0.2299 - accuracy: 0.9120 - val_loss: 0.5419 - val_accuracy: 0.8100
    Epoch 55/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.2559 - accuracy: 0.8905 - val_loss: 0.6649 - val_accuracy: 0.8030
    Epoch 56/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.2423 - accuracy: 0.9125 - val_loss: 0.5062 - val_accuracy: 0.8290
    Epoch 57/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.2625 - accuracy: 0.9000 - val_loss: 0.4762 - val_accuracy: 0.8160
    Epoch 58/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.2184 - accuracy: 0.9180 - val_loss: 0.5613 - val_accuracy: 0.8210
    Epoch 59/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.2191 - accuracy: 0.9145 - val_loss: 0.4793 - val_accuracy: 0.8480
    Epoch 60/100
    63/63 [==============================] - 6s 93ms/step - loss: 0.2253 - accuracy: 0.9115 - val_loss: 0.4898 - val_accuracy: 0.8340
    Epoch 61/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.2497 - accuracy: 0.9035 - val_loss: 0.5486 - val_accuracy: 0.8380
    Epoch 62/100
    63/63 [==============================] - 6s 93ms/step - loss: 0.2251 - accuracy: 0.9120 - val_loss: 0.6480 - val_accuracy: 0.8170
    Epoch 63/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.2057 - accuracy: 0.9180 - val_loss: 0.6012 - val_accuracy: 0.8090
    Epoch 64/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.2392 - accuracy: 0.9070 - val_loss: 0.7982 - val_accuracy: 0.7870
    Epoch 65/100
    63/63 [==============================] - 7s 106ms/step - loss: 0.2344 - accuracy: 0.9095 - val_loss: 0.5322 - val_accuracy: 0.8200
    Epoch 66/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.2084 - accuracy: 0.9190 - val_loss: 1.4610 - val_accuracy: 0.7520
    Epoch 67/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.1963 - accuracy: 0.9225 - val_loss: 0.8049 - val_accuracy: 0.8030
    Epoch 68/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.2132 - accuracy: 0.9170 - val_loss: 0.6219 - val_accuracy: 0.8380
    Epoch 69/100
    63/63 [==============================] - 7s 105ms/step - loss: 0.1866 - accuracy: 0.9315 - val_loss: 0.5037 - val_accuracy: 0.8490
    Epoch 70/100
    63/63 [==============================] - 7s 109ms/step - loss: 0.2079 - accuracy: 0.9195 - val_loss: 0.6616 - val_accuracy: 0.8230
    Epoch 71/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.2044 - accuracy: 0.9230 - val_loss: 0.6571 - val_accuracy: 0.8300
    Epoch 72/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.2269 - accuracy: 0.9175 - val_loss: 0.7447 - val_accuracy: 0.8040
    Epoch 73/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.1957 - accuracy: 0.9210 - val_loss: 0.5015 - val_accuracy: 0.8460
    Epoch 74/100
    63/63 [==============================] - 6s 93ms/step - loss: 0.2087 - accuracy: 0.9250 - val_loss: 0.5579 - val_accuracy: 0.8340
    Epoch 75/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.2140 - accuracy: 0.9200 - val_loss: 0.5589 - val_accuracy: 0.8360
    Epoch 76/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.2199 - accuracy: 0.9290 - val_loss: 0.6654 - val_accuracy: 0.8480
    Epoch 77/100
    63/63 [==============================] - 6s 93ms/step - loss: 0.1672 - accuracy: 0.9405 - val_loss: 0.6647 - val_accuracy: 0.8360
    Epoch 78/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.1770 - accuracy: 0.9350 - val_loss: 0.6143 - val_accuracy: 0.8170
    Epoch 79/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.1752 - accuracy: 0.9295 - val_loss: 0.6483 - val_accuracy: 0.8400
    Epoch 80/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.2064 - accuracy: 0.9220 - val_loss: 0.9454 - val_accuracy: 0.7900
    Epoch 81/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.2104 - accuracy: 0.9195 - val_loss: 0.5437 - val_accuracy: 0.8540
    Epoch 82/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.1769 - accuracy: 0.9340 - val_loss: 0.5951 - val_accuracy: 0.8390
    Epoch 83/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.1903 - accuracy: 0.9280 - val_loss: 0.7472 - val_accuracy: 0.8410
    Epoch 84/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.1801 - accuracy: 0.9300 - val_loss: 0.5435 - val_accuracy: 0.8600
    Epoch 85/100
    63/63 [==============================] - 6s 98ms/step - loss: 0.1798 - accuracy: 0.9360 - val_loss: 0.7137 - val_accuracy: 0.8530
    Epoch 86/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.1973 - accuracy: 0.9305 - val_loss: 0.6568 - val_accuracy: 0.8450
    Epoch 87/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.1689 - accuracy: 0.9345 - val_loss: 0.6006 - val_accuracy: 0.8640
    Epoch 88/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.1599 - accuracy: 0.9440 - val_loss: 1.3120 - val_accuracy: 0.8050
    Epoch 89/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.1991 - accuracy: 0.9270 - val_loss: 0.6726 - val_accuracy: 0.8410
    Epoch 90/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.1820 - accuracy: 0.9330 - val_loss: 0.7732 - val_accuracy: 0.8550
    Epoch 91/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.1948 - accuracy: 0.9270 - val_loss: 0.6773 - val_accuracy: 0.8330
    Epoch 92/100
    63/63 [==============================] - 7s 103ms/step - loss: 0.1979 - accuracy: 0.9275 - val_loss: 0.5139 - val_accuracy: 0.8420
    Epoch 93/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.1698 - accuracy: 0.9445 - val_loss: 0.8142 - val_accuracy: 0.8570
    Epoch 94/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.1746 - accuracy: 0.9385 - val_loss: 0.6159 - val_accuracy: 0.8470
    Epoch 95/100
    63/63 [==============================] - 6s 98ms/step - loss: 0.1748 - accuracy: 0.9400 - val_loss: 0.8730 - val_accuracy: 0.7350
    Epoch 96/100
    63/63 [==============================] - 7s 97ms/step - loss: 0.2152 - accuracy: 0.9265 - val_loss: 0.7178 - val_accuracy: 0.8530
    Epoch 97/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.1700 - accuracy: 0.9385 - val_loss: 0.5947 - val_accuracy: 0.8420
    Epoch 98/100
    63/63 [==============================] - 6s 97ms/step - loss: 0.1956 - accuracy: 0.9255 - val_loss: 0.6206 - val_accuracy: 0.8550
    Epoch 99/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.1689 - accuracy: 0.9455 - val_loss: 0.9251 - val_accuracy: 0.8260
    Epoch 100/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.2022 - accuracy: 0.9360 - val_loss: 0.7826 - val_accuracy: 0.8310


결과를 그래프로 나타내보면 , 데이터 증식과 드롭아웃 덕분에과대적합이 이전의 모델 보다 훨씬 늦은 60,70번째 에포크 근처에서 시작된다. 즉, 모델의 성능이 월등히 좋아졌다.

마지막으로 테스트 세트의 정확도를 확인해보자.


```python
test_model = keras.models.load_model(
    "convnet_from_scratch_with_augmentation.keras")
test_loss, test_acc = test_model.evaluate(test_dataset)
print(f"테스트 정확도: {test_acc:.3f}")
```

    63/63 [==============================] - 3s 37ms/step - loss: 0.4403 - accuracy: 0.8290
    테스트 정확도: 0.829


테스트 정확도를 82.9% 정도 얻었다. 이전에 얻었던 69% 보다 성능이 더 좋아진 것을 알 수 있다. 모델의 파라미터들을 튜닝하면 더 좋은 성능의 모델을 얻을 수 있지만 데이터의 개수가 적기 때문에 한계가 있다. 이런 상황에서 더 좋은 모델을 얻을 수 있는 방법은 **사전 훈련된 모델을 활용**하는 것이다. 다음 글에서 다뤄보도록 하겠다.

[<케라스 창시자에게 배우는 딥러닝 개정 2판>(길벗, 2022)을 학습하고 개인 학습용으로 정리한 내용입니다.]

출처: 프랑소와 숄레 지음, ⌜케라스 창시자에게 배우는 딥러닝 개정2판⌟, 박해선 옮김, 길벗, 2022 , 318-336쪽

도서보기: https://www.gilbut.co.kr/book/view?bookcode=BN003496
