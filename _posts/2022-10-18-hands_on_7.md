---
layout: single
title:  "핸즈온 머신러닝 - 7"
categories : hands-on
tag : [hands-on, study, machine-learning, python]
toc: true
toc_sticky: true
---

![header](https://capsule-render.vercel.app/api?type=waving&color=a2dcec&height=300&section=header&text=핸즈온 머신러닝 - 7&fontSize=40&animation=fadeIn&fontAlignY=38&fontColor=FFFFFF)

- 참고 : [핸즈온 머신러닝 2판](http://www.kyobobook.co.kr/product/detailViewKor.laf?mallGb=KOR&ejkGb=KOR&barcode=9791162242964)

------------------------------------------------------

&nbsp;

## 앙상블 학습과 랜덤 포레스트

- 일련의 예측기로부터 예측을 수집하면 가장 좋은 모델하나보다 더 좋은 예측을 얻을 수 있음
- 훈련 세트로부터 무작위로 각기 다른 서브셋을 만들어 일련의 결정 트리 분류기를 훈련

&nbsp;

### 투표 기반 분류기

- 좋은 분류기를 만드는 매우 간단한 방법은 각 분류기의 예측을 모아서 가장 많이 선택된 클래스를 예측하는 것
- 다수결 투표로 정해지는 분류기를 **직접 투표 분류기** 라고 함

> 앙상블 방법은 예측기가 가능한 한 서로 독립적일 때 최고의 성능을 보임
>
> 다양한 분류기를 얻는 한 가지 방법은 각기 다른 알고리즘으로 학습시키는 것, 매우 다른 종류의 오차를 만들 가능성이 높기 때문에 앙상블 모델의 정확도를 향상시킴

&nbsp;

**투표 기반 분류기 훈련 예시**

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

log_clf = LogisticRegression(solver="lbfgs", random_state=42)
rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)
svm_clf = SVC(gamma="scale", random_state=42)

voting_clf = VotingClassifier(
    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],
    voting='hard')
voting_clf.fit(X_train, y_train)


from sklearn.metrics import accuracy_score

for clf in (log_clf, rnd_clf, svm_clf, voting_clf):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))
    
>>  LogisticRegression 0.864
    RandomForestClassifier 0.896
    SVC 0.896
    VotingClassifier 0.912
```

- 모든 분류기가 클래스의 확률을 예측할 수 있다면 개별 분류기의 예측을 평균 내어 확률이 가장 높은 클래스를 예측할 수 있음 이를 **간접 투표** 라고 함
  - 위 방식은 확률이 높은 투표에 비중을 더 두기 때문에 직접 투표 방식보다 성능이 높음
  - voting= 'hard' -> voting='soft'로 바꾸고 모든 분류기가 클래스의 확률을 추정할수 있다면 가능
  - SVC 기본값에서는 클래스 확률을 제공 X, probability 매개변수 True 설정해야 확률 추정 가능

&nbsp;

### 배깅과 페이스팅

- 같은 알고리즘을 사용하고 훈련 세트의 서브셋을 무작위로 구성하여 분류기를 각기 다르게 학습시키는 것
- 훈련 세트에서 중복을 허용하여 샘플링하는 방식을 **배깅**
- 중복을 허용하지 않고 샘플링 하는 방식을 **페이스팅**

- 배깅과 페이스팅에서는 같은 훈련 샘플을 여러 개의 예측기에 걸쳐 사용 가능 하지만 배깅만이 한 예측기를 위해 같은 훈련 샘플을 여러 번 샘플링할 수 있음

- 모든 예측기가 훈련을 마치면 앙상블은 모든 예측기의 예측을 모아서 새로운 샘플에 대한 예측을 만듬
  - 분류일경우 통계적 최빈값, 회귀는 평균으로 계산
- 배깅과 페이스팅은 병렬로 학습이 가능 

&nbsp;

**사이킷런의 배깅과 페이스팅**

- sklearn API를 통한 결정 트리 분류기 500개의 앙상블 훈련시키는 코드

  - 중복 허용해 무작위로 선택된 100개 샘플 학습 (배깅), bootstrap=False (페이스팅)

    ```python
    from sklearn.ensemble import BaggingClassifier
    from sklearn.tree import DecisionTreeClassifier
    
    bag_clf = BaggingClassifier(
        DecisionTreeClassifier(), n_estimators=500,
        max_samples=100, bootstrap=True, random_state=42)
    bag_clf.fit(X_train, y_train)
    y_pred = bag_clf.predict(X_test)
    
    from sklearn.metrics import accuracy_score
    print(accuracy_score(y_test, y_pred))
    >> 0.904
    ```

    - BaggingClassifier도 확률 추정가능 -> 간접투표 사용가능 
    - predict_proba() 있으면 간접투표방식 사용


- ![image-20221018113544333](/images/2022-10-18-hands_on_7/image-20221018113544333.png)
  - 위 그림에서 처럼 앙상블 예측이 결정 트리 하나의 예측보다 일반화가 더 잘됨
  - 앙상블은 비슷한 편향에서 더 작은 분산을 만듬 (훈련 세트의 오차 수가 거의 비슷하지만 결정 경계는 덜 불규칙)
- bootstrap은 각 예측기가 학습하는 서브셋에 다양성을 증가시키므로 배깅이 페이스팅보다 편향이 조금 더 높음
  - 다양성을 추가한다는 것은 예측기들의 상관관계를 줄이므로 앙상블의 분산 감소
  - 전반적으로 배깅이 더 나은 모델을 만들기 때문에 더 선호

&nbsp;

**oob 평가**

- 배깅을 사용하면 어떤 샘플은 한 예측기를 위해 여러 번 샘플링되고 어떤 것은 전혀 선택이 안될수도 있음

- BaggingClassifier는 기본값으로 중복을 허용(bootstrap=True) 훈련 세트의 크기만큼인 m개 샘플을 선택

  - 평균적으로 각 예측기에 훈련 샘플의 63%만 샘플링
  - 선택되지 않은 훈련 샘플의 나머지 37%를 **oob 샘플**이라고 함

- 예측기가 훈련되는 동안에도 oob 샘플을 사용하지 않으므로 별도의 검증 세트를 사용하지 않고 oob 샘플을 사용해 평가 가능

  - 앙상블의 평가는 각 예측기의 oob 평가를 평균하여 얻음
  - oob_score = True 로 수행

  ```python
  bag_clf = BaggingClassifier(
      DecisionTreeClassifier(), n_estimators=500,
      bootstrap=True, oob_score=True, random_state=40)
  bag_clf.fit(X_train, y_train)
  bag_clf.oob_score_
  >> 0.9013
  ```

  - oob score 결과를 보면 약 90.1% 정확도를 얻을수 있음

  ```python
  from sklearn.metrics import accuracy_score
  y_pred = bag_clf.predict(X_test)
  accuracy_score(y_test, y_pred)
  
  >> 0.912
  ```

  - oob 결정 함수의 값은 oob_decision_function_ 변수에서 확인 가능
  - 결정 함수는 각 훈련 샘플의 클래스 확률을 반환 (predict_proba() 있기 때문)

&nbsp;

### 랜덤 패치와 랜덤 서브스페이스

- BaggingClassifier는 특성 샘플링도 지원, 샘플링은 max_features, bootstrap_features 두 매개변수로 조절
- 작동은 max_samples, bootstrap과 동일하지만 샘플이 아닌 특성에 대한 샘플링
  - 각 예측기는 무작위로 선택한 입력 특성의 일부분으로 훈련
- 매우 고차원의 데이터셋을 다룰 때 유용함
- 훈련 특성과 샘플을 모두 샘플링 하는 것을 **랜덤 패치 방식**
- 훈련 샘플을 모두 사용하고 (bootstrap=False, max_samples=1.0) 특성은 샘플링하는 (bootstrap_features=True, max_features < 1.0) 것을 **랜덤 서브스페이스 방식**
- 특성 샘플링은 더 다양한 예측기를 만들며 편향을 늘리는 대신 분산을 낮춤

&nbsp;

### 랜덤 포레스트

- 랜덤 포레스트는 일반적으로 배깅 or 페이스팅을 적용한 결정 트리의 앙상블 방법

- max_samples를 훈련 세트의 크기로 지정

- Bagging Classifier에 DecisionTreeClassifier를 넣어 만드는 대신에 결정 트리에 최적화되어 사용하기 편리한 RandomForestClassifier를 사용할수 있음

- 500개 트리로 이루어진 랜덤포레스트 훈련 예제

  ```python
  from sklearn.ensemble import RandomForestClassifier
  
  rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, random_state=42)
  rnd_clf.fit(X_train, y_train)
  
  y_pred_rf = rnd_clf.predict(X_test)
  ```

  - RandomForestClassifer는 몇가지 예외를 제외하고 DecisionTreeClassifier의 매개변수와 앙상블 자체를 제어하는 데 필요한 BaggingClassifier의 매개변수 모두 존재

- 랜덤포레스트는 트리의 노드 분할시 전체 특성 중에서 최선의 특성을 찾는 대신에 선택한 특성 후보 중에서 최적의 특성을 찾는 방식으로 무작위성을 더 추가함

  - 트리를 더욱 다양하게 만들고 편향은 커지지만 분산을 낮추어 결과적으로 더 좋은 모델이 생성됨

  - BaggingClassifier을 RandomForestClassifier와 유사하게 만드는 예제
    ```python
    bag_clf = BaggingClassifier(
        DecisionTreeClassifier(max_features="sqrt", max_leaf_nodes=16),
        n_estimators=500, random_state=42)
    ```

&nbsp;

**엑스트라 트리**

- 트리를 생성시 각각의 노드는 무작위로 특성의 서브셋을 만들어 분할에 사용
- 트리를 더욱 무작위하게 만들기 위해 최적의 임곗값을 찾는 대신 후보 특성을 사용해 무작위로 분할한 다음 그중에서 최상의 분할을 선택
- 극단적으로 무작위한 트리의 랜덤 포레스트를 **엑스트라 트리 앙상블**이라고 함
  - 편향증가, 분산감소
- 랜덤포레스트 보다 엑스트라 트리가 빠름

&nbsp;

**특성 중요도**

- 랜덤 포레스트의 주요한 장점중 하나는 특성의 상대적 중요도를 측정하기 쉬움

- sklearn은 어떤 특성을 사용한 노드가 평균적으로 분순도를 얼마나 감소시키는지 확인하여 특성의 중요도 측정

  - 가중치 평균을 의미, 각 노드의 가중치는 연관된 훈련 샘플 수와 같음

- sklearn은 훈련이 끝나면 각각의 특성마다 자동으로 점수를 계산하고 중요도의 전체 합이 1이 되도록 결과값을 정규화 함

  - 해당 값은 feature_importance_ 변수에 저장됨

- RandomForestClassifier 중요도 출력 예시

  ```python
  from sklearn.datasets import load_iris
  iris = load_iris()
  rnd_clf = RandomForestClassifier(n_estimators=500, random_state=42)
  rnd_clf.fit(iris["data"], iris["target"])
  for name, score in zip(iris["feature_names"], rnd_clf.feature_importances_):
      print(name, score)
      
  >>  sepal length (cm) 0.11249225099876375
      sepal width (cm) 0.02311928828251033
      petal length (cm) 0.4410304643639577
      petal width (cm) 0.4233579963547682
  ```

&nbsp;

### 부스팅

- 약한 학습기를 여러 개 연결하여 강한 학습기를 만드는 앙상블 방법을 의미
- 앞의 모델을 보완해나가면서 일련의 예측기를 학습시키는 것
- 가장 인기가 많은 부스팅은 AdaBoost, gradient boosting

&nbsp;

**에이다부스트**

- 이전 모델이 과소적합했던 훈련 샘플의 가중치를 높임, 새로운 예측기는 학습하기 어려운 샘플에 초점을 두는 방식
- 첫 번째 분류기를 훈련 세트에서 훈련시키고 예측 생성, 알고리즘이 잘못 분류된 훈련 샘플의 가중치를 상대적으로 높임
- 두 번째 분류기는 업데이트된 가중치를 사용해 훈련 세트에서 훈련하고 다시 예측을 생성, 그 후에 다시 가중치를 업데이트하는 식으로 계속 진행

- ![image-20221020105254021](/images/2022-10-18-hands_on_7/image-20221020105254021.png)

  - 위 그림을 확인하면 첫 번째 분류기가 많은 샘플을 잘못 분류해서 잘못된 샘플에 대한 가중치가 높아짐
  - 이후에 두 번째 분류기는 이 샘플들을 더 정확히 예측하도록 훈련
  - 학습률을 제외한 모든 설정이 같은 예측기로 훈련시킨 이미지

  > 경사 하강법은 비용 함수를 최소화하기 위해 한 예측기의 모델 파라미터를 조정해가는 반면 에이다부스트는 점차 더 좋아지도록 앙상블에 예측기를 추가함

- 모든 예측기가 훈련을 마치면 이 앙상블은 배깅이나 페이스팅과 비슷한 방식으로 예측을 생성 하지만 가중치가 적용된 훈련 세트의 전반적인 정확도에 따라 예측기마다 다른 가중치 적용
  - 연속된 학습 기법에서는 각 예측기는 이전 예측기가 훈련되고 평가된 후에 학습될 수 있기 때문에 **병렬or분할을 할수 없음**, 배깅이나 페이스팅만큼 확장성이 높지 않음

- sklaern의 AdaBoostClassifier을 사용하여 200개의 아주 얕은 **결정 트리를 기반**으로 하는 AdaBoost 분류기를 학습, 결정트리는 max_depth=1, 결정 노드와 리프 노드 두 개로 이루어진 트리

  ```python
  from sklearn.ensemble import AdaBoostClassifier
  
  ada_clf = AdaBoostClassifier(
      DecisionTreeClassifier(max_depth=1), n_estimators=200,
      algorithm="SAMME.R", learning_rate=0.5, random_state=42)
  ada_clf.fit(X_train, y_train)
  ```

  - AdaBoost 앙상블이 훈련 세트에 과대적합되면 추정기 수를 줄이거나 추정기의 규제를 추가해야함

&nbsp;

**그레이디언트 부스팅**

- 에이다부스트처럼 그레이디언트 부스팅은 앙상블에 이전까지의 오차를 보정하도록 예측기를 순차적으로 추가
  - 하지만 에이다 부스트처럼 반복마다 샘플의 가중치를 수정하는 대신 이전 예측기가 만든 **잔여 오차**에 새로운 예측기를 학습

- 그레이디언트 트리 부스팅(GBRT) 예제

  ```python
  from sklearn.tree import DecisionTreeRegressor
  
  tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)
  tree_reg1.fit(X, y)
  
  # 생성된 잔여 오차에 두 번째 DecisionTreeRegressor 훈련
  y2 = y - tree_reg1.predict(X)
  tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=42)
  tree_reg2.fit(X, y2)
  
  # 세 번째 잔여 오차 학습
  y3 = y2 - tree_reg2.predict(X)
  tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42)
  tree_reg3.fit(X, y3)
  
  # 세 개의 트리를 포함하는 앙상블 모델 생성
  y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))
  ```

  - ![image-20221020112907595](/images/2022-10-18-hands_on_7/image-20221020112907595.png)

    - 왼쪽 이미지가 위의 과정, 오른쪽은 앙상블의 예측

    > 결과적으로 트리가 앙상블에 추가될수록 앙상블의 예측이 점차 좋아지는 것을 확인할수 있음

- sklearn GradientBoostingRegressor 예제 (위코드와 같은 연산)

  ```python
  from sklearn.ensemble import GradientBoostingRegressor
  
  gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=42)
  gbrt.fit(X, y)
  ```

- learning_rate  매개변수가 각 트리의 기여 정도를 조절함
  - 0.1처럼 낮게 설정하면 앙상블을 훈련 세트에 학습시키기 위해 많은 트리가 필요하지만 일반적으로 예측의 성능은 좋아짐 (이를 **축소**라고 함)
  - ![image-20221020113325966](/images/2022-10-18-hands_on_7/image-20221020113325966.png)
    - 위 이미지는 작은 학습률로 훈련시킨 GBRT 앙상블
      - 왼쪽은 훈련 세트를 학습하기에 트리가 충분하지 않는 모습
      - 오른쪽은 트리가 너무 많아 과대적합 되는 모습
      - 이를 해결하는 방법은 최적의 트리수를 찾을때 종료하는 **조기 종료** 기법 사용

- 조기 종료 GBRT 예시

  ```python
  import numpy as np
  from sklearn.model_selection import train_test_split
  from sklearn.metrics import mean_squared_error
  
  X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=49)
  
  gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120, random_state=42)
  gbrt.fit(X_train, y_train)
  
  errors = [mean_squared_error(y_val, y_pred)
            for y_pred in gbrt.staged_predict(X_val)]
  bst_n_estimators = np.argmin(errors) + 1
  
  gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators, random_state=42)
  gbrt_best.fit(X_train, y_train)
  ```

  - ![image-20221020113556987](/images/2022-10-18-hands_on_7/image-20221020113556987.png)

  - 5 epoch 동안 향상 되지 않을때 조기 종료 예시

    ```python
    gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True, random_state=42)
    
    min_val_error = float("inf")
    error_going_up = 0
    for n_estimators in range(1, 120):
        gbrt.n_estimators = n_estimators
        gbrt.fit(X_train, y_train)
        y_pred = gbrt.predict(X_val)
        val_error = mean_squared_error(y_val, y_pred)
        if val_error < min_val_error:
            min_val_error = val_error
            error_going_up = 0
        else:
            error_going_up += 1
            if error_going_up == 5:
                break  # early stopping
                
                
    print(gbrt.n_estimators)
    >> 61
    print("Minimum validation MSE:", min_val_error)
    >> Minimum validation MSE: 0.002712853325235463
    ```

    - warm_staart = True 설정시 fit() 호출될 때 기존 트리를 유지하고 훈련을 추가할 수 있도록 해줌

- GradientBoostingRegressor은 각 트리가 훈련할 때 사용할 훈련 샘플의 비율을 지정할수 있음, subsample

  - subsample=0.25 라고 하면 각 트리는 무작위로 선택된 25%의 훈련 샘플로 학습
  - 평향이 높아지는 대신, 분산이 낮아짐
  - 훈련 속도를 상당히 높임

  > 이런 방식을 **확률적 그레이디언트 부스팅** 이라고 함
  >
  > 최적화된 그레이디언트 부스팅 구현으로 XGBoost가 존재

  ```python
  xgb_reg = xgboost.XGBRegressor(random_state=42)
  xgb_reg.fit(X_train, y_train)
  y_pred = xgb_reg.predict(X_val)
  ```

&nbsp;

### 스태킹

- 앙상블에 속한 모든 예측기의 예측을 취합하는 간단한 함수를 사용하는 대신 취합하는 모델을 훈련시키는 방식

- ![image-20221020131056805](/images/2022-10-18-hands_on_7/image-20221020131056805.png)

  - 마지막 예측기를 **블렌더** 라고함

  - 블렌더를 학습시키는 일반적인 방법은 홀드 아웃세트를 사용하는 것

    - ![image-20221020131130120](/images/2022-10-18-hands_on_7/image-20221020131130120.png)
      - 먼저 훈련 세트를 두 개의 서브셋으로 분할, 첫 번째 서브셋은 첫 번째 레이어의 예측을 훈련시키기 위해 사용
    - ![image-20221020131317018](/images/2022-10-18-hands_on_7/image-20221020131317018.png)
      - 그후에 첫 번째 레이어의 예측기를 사용해 두 번째 홀드 아웃 세트에 대한 예측 생성
        - 예측기들은 훈련하는 동안 해당 샘플은 보지 않았기 때문에 이때 생성된 예측은 완전히 새로운 예측
      - 홀드 아웃 세트의 각 샘플에 대해 세 개의 예측값이 생성됨, 타깃값은 그대로 사용하고 예측한 값을 입력 특성으로 사용하는 새로운 훈련 세트를 생성
        - 블렌더가 새 훈련 세트로 훈련됨

    > 첫 번째 레이어의 예측을 가지고 타깃값을 예측하는 형태로 학습됨

- 이런 방식의 블렌더를 여러 개 훈련시키는 것도 가능, 이런식으로 훈련되면 해당 블렌더만의 레이어가 생성됨

&nbsp;

## 연습문제

### 1. 정확히 같은 훈련 데이터로 다섯 개의 다른 모델을 훈련시켜서 모두 95%의 정확도를 얻었다면 이 모델들을 연결하여 더 좋은 결과를 얻을 수 있을까요? 가능하다면 어떻게 해야 할 까요? 그렇지 않다면 왜일까요?

> 앙상블을 만들어 더 나은 결과를 기대할 수 있다. 모델이 서로 다르다면 더 좋다. 그리고 다른 훈련 샘플에서 훈련되었다면 더더욱 좋다. ( 배깅 / 페이스팅의 핵심 ). 그렇지 않아도 모델이 서로 많이 다르면 여전히 좋은 결과를 얻음

&nbsp;

### 2. 직접 투표와 간접 투표 분류기 사이의 차이점은 무엇일까요?

> 직접 투표 분류기는 횟수를 중요시하여, 가장 많은 투표를 얻은 클래스를 선택. (다수결)간접 투표 분류기는 각 클래스의 평균적인 확률 추정값을 계산해서 가장 높은 확률을 가진 클래스를 고름. 이 방법은 신뢰가 높은 투표에 더 가중치를 주고 종종 더 나은 성능을 낸다. 하지만 앙상블에 있는 모든 분류기가 클래스 확률을 추정할 수 있어야 사용 가능

&nbsp;

### 3. 배깅 앙상블의 훈련을 여러 대의 서버에 분산시켜 속도를 높일 수 있을까요? 페이스팅 앙상블, 부스팅 앙상블, 랜덤 포레스트, 스태킹 앙상블의 경우는 어떨까요?

> 배깅 앙상블의 각 예측기는 독립적이므로 여러 대의 서버에 분산하여 앙상블의 훈련 속도를 높일 수 있다. 페이스팅, 랜덤 포레스트도 마찬가지.하지만 부스팅 앙상블의 예측기는 이전 예측기를 기반으로 만들어지기 때문에 훈련이 순차적이어야하고, 여러 대의 서버에 분산해서 얻을 수 있는 이득이 없다.스태킹 앙상블의 경우 한 층의 모든 예측기가 각각 독립적이므로 여러대의 서버에서 병렬로 훈련될 수 있다. 하지만 한 층에 있는 예측기들은 이전 층의 예측기들이 훈련된 후에 훈련될 수 있다.

&nbsp;

### 4. oob 평가의 장점은 무엇인가요?

> 배깅 앙상블의 각 예측기가 훈련에 포함되지 않은 샘플을 사용해 평가된다. 이는 추가적인 검증 세트가 없어도 편향되지 않게 앙상블을 평가하도록 도와준다. 그러므로 훈련에 더 많은 샘플을 사용할 수 있어서 앙상블의 성능은 조금 더 향상 될 것.

&nbsp;

### 5. 무엇이 엑스트라 트리를 일반 랜덤 포레스트보다 더 무작위하게 만드나요? 추가적인 무작위성이 어떻게 도움이 될까요? 엑스트라 트리는 일반 랜덤 포레스트보다 느릴까요, 빠를까요?

> 랜덤 포레스트에서 트리가 성장할 때 각 노드에서 특성의 일부를 무작위로 선택해 분할에 사용한다. 가능한 최선의 임계점을 찾는게 아니라, 각 특성에 대해 랜덤한 임계점을 사용한다, 이 추가적인 무작위성은 규제처럼 작동한다. 즉 랜포가 과대적합이라면 엑스트라는 X. 가능한 최선의 임계점을 찾는게 아니라 랜포보다 훈련이 더 빠르다. 그러나 예측을 할 때는 랜덤 포레스트 보다 더 빠르지도 느리지도 않다.

&nbsp;

### 6. 아다부스트 앙상블이 훈련 데이터에 과소적합되었다면 어떤 매개변수를 어떻게 바꾸어야 할까요?

> 예측기의 수를 증가시키거나, 기반 예측기의 규제 하이퍼파라미터를 감소시켜 볼 수 있다. 또한 학습률을 약간 증가시켜 볼 수 있다.

&nbsp;

### 7. 그래디언트 부스팅 앙상블이 훈련 데이터에 과대적합되었다면 학습률을 높여야 할까요, 낮춰야 할까요?

> 학습률을 감소시켜야한다 ( 예측기 수가 너무 많으면 ) 알맞은 개수를 찾기 위해 조기 종료 기법을 사용할 수 있다.

&nbsp;

### 8. 투표 기반 분류기

> - 데이터 로드
>
> ```python
> from sklearn.model_selection import train_test_split
> X_train_val, X_test, y_train_val, y_test = train_test_split(
>     mnist.data, mnist.target, test_size=10000, random_state=42)
> X_train, X_val, y_train, y_val = train_test_split(
>     X_train_val, y_train_val, test_size=10000, random_state=42)
> ```
>
> - 랜덤 포레스트 분류기, 엑스트라 트리 분류기, SVM 같은 여러 종류의 분류기를 훈련
>
> ```python
> from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
> from sklearn.svm import LinearSVC
> from sklearn.neural_network import MLPClassifier
> 
> random_forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)
> extra_trees_clf = ExtraTreesClassifier(n_estimators=100, random_state=42)
> svm_clf = LinearSVC(max_iter=100, tol=20, random_state=42)
> mlp_clf = MLPClassifier(random_state=42)
> 
> estimators = [random_forest_clf, extra_trees_clf, svm_clf, mlp_clf]
> for estimator in estimators:
>     print("Training the", estimator)
>     estimator.fit(X_train, y_train)
>     
>     
> [estimator.score(X_val, y_val) for estimator in estimators]
> >> [0.9692, 0.9715, 0.859, 0.9666]
> ```
>
> - 간접 또는 직접 투표 분류기를 사용하는 앙상블로 연결
>
> ```python
> from sklearn.ensemble import VotingClassifier
> 
> named_estimators = [
>     ("random_forest_clf", random_forest_clf),
>     ("extra_trees_clf", extra_trees_clf),
>     ("svm_clf", svm_clf),
>     ("mlp_clf", mlp_clf),
> ]
> 
> voting_clf = VotingClassifier(named_estimators)
> voting_clf.fit(X_train, y_train)
> 
> voting_clf.score(X_val, y_val)
> >> 0.9708
> ```
>
> - SVM 모델 제외
>
> ```python
> voting_clf.set_params(svm_clf=None)
> 
> >> VotingClassifier(estimators=[('random_forest_clf',
>                               RandomForestClassifier(random_state=42)),
>                              ('extra_trees_clf',
>                               ExtraTreesClassifier(random_state=42)),
>                              ('svm_clf', None),
>                              ('mlp_clf', MLPClassifier(random_state=42))])
> 
> 
> del voting_clf.estimators_[2] # svm 모델 제거
> 
> voting_clf.score(X_val, y_val)
> >> 0.9741
> ```
>
> - voting 방법 비교 (soft, hard)
>
> ```python
> voting_clf.voting = "soft"
> voting_clf.score(X_val, y_val)
> >> 0.972
> 
> voting_clf.voting = "hard"
> voting_clf.score(X_test, y_test)
> >> 0.971
> 
> [estimator.score(X_test, y_test) for estimator in voting_clf.estimators_]
> >> [0.9645, 0.9691, 0.9604]
> ```

&nbsp;

### 9. 스태킹 앙상블

> ```python
> X_val_predictions = np.empty((len(X_val), len(estimators)), dtype=np.float32)
> 
> for index, estimator in enumerate(estimators):
>     X_val_predictions[:, index] = estimator.predict(X_val)
>     
> X_val_predictions
> >>  array([[5., 5., 5., 5.],
>            [8., 8., 8., 8.],
>            [2., 2., 3., 2.],
>            ...,
>            [7., 7., 7., 7.],
>            [6., 6., 6., 6.],
>            [7., 7., 7., 7.]], dtype=float32)
> 
> rnd_forest_blender = RandomForestClassifier(n_estimators=200, oob_score=True, random_state=42)
> rnd_forest_blender.fit(X_val_predictions, y_val)
> 
> rnd_forest_blender.oob_score_
> >> 0.9684
> ```
>
> - 블렌더에 예측을 모아서 최종 예측
>
> ```python
> X_test_predictions = np.empty((len(X_test), len(estimators)), dtype=np.float32)
> 
> for index, estimator in enumerate(estimators):
>     X_test_predictions[:, index] = estimator.predict(X_test)
>     
>     
> y_pred = rnd_forest_blender.predict(X_test_predictions)
> 
> from sklearn.metrics import accuracy_score
> accuracy_score(y_test, y_pred)
> >> 0.9695
> ```

