---
title: "Review : The Practical Guides for Large Language Models"
escerpt: "LLM(Large Language Models) 논문 리뷰해보기"

categories:
  - AI
tags:
  - [AI, LLM, Review]

toc: true
toc_sticky: true

breadcrumbs: true

date: 2023-09-21
last_modified_at: 2023-09-21

comments: true


---

## Abstraction

[LLMsPracticalGuide](https://github.com/Mooler0410/LLMsPracticalGuide?fbclid=IwAR2jeFAXvKfGh6FGVovC7M5ww2PJgaWpawGAsHf61ISJ8owoBKRcOU5an5M)

LLM은 대량의 데이터셋에 대해 사전 학습된 거대 언어모델
fine-tuned models은 소규모 언어모델시 적용


GPT 및 BERT 스타일의 LLM에 대한 소개와 요약
사전학습데이터, 학습데이터, 테스트 데이터의 영향
다양한 자연어 처리 작업에 대한 대규모 언어 모델의 사용 및 비사용 사례
LLM의 실제 적용과 한계
데이터의 중요성
NLP작업과 관련된 구체적인 과제
허위 편향이 LLM에 미치는 영향

목표 : 모델크기, 계사요구사항, 도메인별 사전 학습된 모델의 가용성 등의 요소를 고려하여 LLM을 선택하는 방법
      특정작업에 LLM을 선택해야 하는 이유 or 선택하지 말아야 하는 이유

## 1. Introduction : GPT 및 BERT 스타일의 LLM에 대한 소개와 요약
LLM은 인코더 전용 언어모델과 디코더 전용 언어모델이라는 두 가지 유형으로 분류가능.

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/8cda724a-b9d5-4280-8ca6-07de6dddd97e)

  - 디코더 전용모델이 점차 LLM 개발주도함 : GPU-3 이후 크게 활성화중

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/2853f299-99df-4ecd-9ccc-ddc2c5613740)

  - BERT 스타일 언어모델 : 인코더-디코더 or 인코더 전용
    1) 자연어 데이터를 쉽게 사용가능
    2) 비지도 학습 패러다임
    3) 주변 문맥을 고려하면서 문장에서 가려진 단어를 예측 = masked words
    4) 이러한 훈련을 통해 모델은 단어와 단어가 사용되는 문맥 간의 관계를 더 깊이 이해할수 있음.
    5) Transformer 아케틱처와 같은 기술사용하여 대규모 텍스트 포러스를 학습하며 감정 분석 및 명명된 개체 인식과 같은 많은 NLP작업에서 좋은 결과를 달성.

  - GPT 스타일 언어모델 : 디코더 전용
    1) downstream작업의 데이터 세트에 대한 fine-tuning이 필요 : 제로샷 성능 향상.
    2) 제로샷 성능 향상을 위한 가장 좋은 모델은 자동 회귀언어모델로, 앞의 단어가 주어진 시퀀스에서 다음 단어를 생성하여 학습.
    3) 텍스트 생성 및 질문 답변과 같은 다운스트림 작업에 널리 사용.


## 2. 데이터에 대한 실용적인 가이드
- Adversarial examples and domain shifts와 같이 out-of-distribution data에 직면한 downstream 작업에서는 LLM이 미세 조정모델보다 일반화가 더 잘이루어짐
- 제한된 주석달린 데이터로 작업 시, LLM이 fine-tuning model보다 선호됨.
- 풍부한 주석이 달린 데이터 사용시 두 모델 모두 합리적 선택임.
- 다운스트림 작업과 유사한 데이터 필드에 대해 사전 학습된 모델을 선택하는것이 좋음

### 2-1. 사전학습데이터
  - 사전학습데이터는 단어지식, 문법, 구문, 의미에 대한 풍부한 이해를 바탕으로 언어모델에 정보제공 + 문맥 인식하고 일관된 응답생성 능력
  - PaLM 과 BLOOM은 풍부한 다국어 사전 학습 데이터를 통해 다국어 작업과 기계 번역에서 탁월한 성능을 발휘
  - PaLM은 소셜미디어 대화와 도서 말뭉치를 통합함으로써 질문 답변 작업에서 성능이 향상.
  - GPT-3.5의 코드 실행 및 코드 완성 기능은 사전학습데이터 set + 코드데이터를 통합함으로써 증폭됨. 즉, 다운스트림 작업을 위해 LLM선택시 유사한 데이터 분야에 대해 사전 학습된 모델 선택하는게 좋음.

### 2-2. 데이터 미세조정
  : 데이터 가용성 측면에서 LLM이 좋고, 풍부한 주석이 달린 데이터의 경우 fine-tuning models 까지 고려가능.
  : 댜운스트림 작업을 위해 모델 배포시 주석이 달린 데이터의 가용성에 따른 고려사항
    1) 주석이 없는 데이터 : 제로샷 설정에서 LLM을 활용하는 방식이 좋음. 또한, 매개변수 업데이트 프로세스가 없기 떄문에 치명적인 망각을 방지할수 있음.
    2) 주석이 별로 없는 데이터 : 상황 내 학습(ICL)이라고 명명된 LLM의 입력 프롬프트에 직접 통합되며, 이러한 예제는 LLM이 작업에 일반화하도록 효과적으로 안내 할 수 있음. 또한, LLM의 제로샷 능력은 스케일링을 통해 더욱 향상가능.다만, 미세 조정된 모델의 규모가 작고, 과적합으로 인해 LLM을 사용하는것보다 성능이 떨어질 수 있음. 
    3) 주석이 많은 데이터 : 미세조정모델과 LLM 모두 고려가능. 

### 2-3. 테스트 데이터/사용자 데이터
  : ODD(Out of distribution Data,학습데이터의 분포와는 다른 분포를 갖는 데이터)로 인해 real data적용시 문제가 되지만, LLM은 명시적인 피팅 프로세스가 없기 때문에 우수한 성능발휘.
    1) InstructGPT는 다양한 작업에 대한 다양한 지침을 따르는 데 능숙하며 때로는 지침이 부족하더라도 다른 언어로 된 지침을 준수하는 데 능숙함을 보여줌.
    2) ChatGPT는 대부분의 적대적 및 배포외(ODD) 분류 및 번역작업에서 일관된 성능을 보여줌.


## 3. 다양한 자연어 처리 작업에 대한 대규모 언어 모델의 사용 및 비사용 사례
전통적인 NLU(Natural Language Understanding, 자연어이해) 작업은 텍스트 분류, 명명된 엔티티 인식(NER, named entity recognition), 수반조건예측 등을 포함한 NLP의 기본작업이다.

- 의사결정흐름

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/14a4325d-80c1-4cd7-912f-13c45068dd72)

  - 일반적으로 NLU작업에서는 fine-tuning model이 좋지만, 일반화능력을 구현할때는 LLM도 좋다.

### 3-1. 비교

1) GLUE, SuperGLUE의 작업과 같은 대부분의 자연어 이해작업에서 주석이 달린 데이터는 분포에서 벗어난 예가 거의 없기에 미세조정모델이 좋다.
2) 텍스트 분류의 경우 미세조정된 모델이 좋다.
3) 감정분석의 경우 IMDB, SST와 같이 LLM, 미세조정모델 둘다 좋다.
4) PerspectiveAPI는 독성감지 하는데 최고 모델. 독성데이터와 이 모델에서 추출된 여러 개의 작은 단일 언어 CNN을 기반으로 조정된 다국어 BERT기반 모델에 의해 구동됨. 이는 독성이 언어 표현의 미묘한 뉘앙스에 의해 정의되기 때문에 대규모 언어 모델은 제공된 입력만으로는 이 작업을 정확하게 이해할 수 없기 떄문.
5) NLI(자연어추론)은 RTE,SNLI와 같은 대부분의 데이터세트에서 미세조정모델이 좋음.
6) CB와 같은 일부데이터에서는 LLM이 더 좋음.
7) 질문답변(QA) 에서는 SQuAD(Stanford Question Answering Dataset)v2, QuAC 및 여러 데이터세트에서 미세조정모델이 우수.
8) CoAQ에서는 LLM과 미세조정모델이 동등한 성능보임.
9) 정보검색(IR, Information retrieval)은 수천개의 후보 텍스트를 LLM이 요구하는 몇개 또는 제로 샷 형태로 변환하는 자연스러운 방법이 없다.
10) 즉, NLU(Natural Language Understanding, 자연어이해)는 데이터세트에서의 성능과 계산 비용측면에서 fine-tuning model이 더 좋다.

### 3-2. 사용사례

1) 기타 텍스트 분류는 서로 명확하거나 강한 관계가 없을 수 있는 다양한 주제와 범주를 다룬다. 이떄는 LLM이 적합하다.
2) 적대적 NLI(ANLI, Adversarial Natural language inference)는 적대적으로 채굴된 자연어 추론 문제로 구성된 세가지 라운드로 구성된 어려운 데이터 세트이다. 특히, LLM은 ANLI, 특히 R3,R2에서 우수한 성능을 보여줌. 기존 NLP작업에서 분산되지 않고 주석이 희박하게 달린 데이터에 대해 일반화할 수 있는 LLM의 탁월한 능력을 보여줌.

현재 STR(Scene Text Recognition)에 대한 모델이 많이 등장하였지만, 전체적으로 공정하게 비교하기 쉽지 않다. 
-> 일치하는 training과 evaluation dataset을 선택하지 않기 때문.

이 논문에서는 다음과 같은 3가지에 대해 기여를 하였다.

- training dataset과 evaluation dataset의 불일치성과 이로 인한 performance 차이에 대하여 조사한다.

- 통합된 four-stage STR framework를 소개한다. 이전에 제안된 STR module의 광범위한 평가할 수 있고, 새로운 조합의 module을 발견할 수 있다.

- 성능(정확성, 속도, 메모리, 하나로 일관된 training & evaluation dataset)에 대한 모듈별 기여도를 분석한다.

## 1. Introduction

발전한 OCR(Optical Character Recognition, 광학 문자 인식)은 깨끗한 문서에 대하여 성공적인 성능을 보여주었지만, 이전의 OCR은 그렇지 못하였다.  

STR은 현실 세계에서 발생하는 다양한 형태의 텍스트와 캡쳐되는 장면의 불완전한 조건장면이 포착된다.

![image](https://user-images.githubusercontent.com/46878973/165026188-896c4b70-3e0f-4fff-b6ca-6000d1adc1ba.png)

이러한 문제를 다루기 위해 여러가지 model 등장하였다.  
하지만 1) training datasets 2) evaluation datasets 로 인해 서로 비교하기가 어려웠다.  

같은 IC13을 사용하더라도 서로 다른 subset을 사용하는 경우, 성능 차이가 15% 넘게 발생한다.

이 논문에서는 다음과 같은 이슈에 대해 다룬다.

- STR 논문에 자주 사용되는 모든 training과 evaluation datasets를 분석한다.
- 통합하는 STR framework를 소개한다.
- module 별 기여도를 분석한다. (accuracy, speed, memory demand, under a unified experimental setting)
- 추가적으로 실패사례 또한 분석한다.

## 2. Dataset Matters in STR

|**dataset**|examples|
|:---:|:---:|
|Training Dataset|MJ, ST|
|Evaluation Dataset|IIIT, SVT, IC03, IC13, IC15, SP, CT|



### 2.1. Synthetic datasets for training

대부분의 STR model은 synthetic datasets를 training datasets로 사용하였다.

![image](https://user-images.githubusercontent.com/46878973/165031641-5b17bf7a-5bd9-4bb1-a13b-ae64870da899.png)

|dataset|images|
|:---:|:---:|
|MJSynth (MJ)|8.9M word box images|  
|SynthText (ST)|5.5M word box images|

- MJ : 합성데이터세트
  - 단어 상자 생성 프로세스
    1) 글꼴 렌더링
    2) 테두리 및 그림자 렌더링
    3) 배경색 지정
    4) 글꼴, 테두리 및 배경 구성
    5) 투영 왜곡 적용
    6) 실제 이미지와 혼합
    7) 노이즈 추가

- SJ : 합성적으로 생성된 또 다른 데이터 세트이며 장면 텍스트 감지를 위해 설계됨


이전에는 MJ와 ST의 다양한 조합을 사용하였는데 이는 불일치성이 발생하도록 한다. 제안된 module로 인해 성능이 향상된건지 더 크고 나은 training data를 사용해서 향상된건지 알 수가 없다.  
따라서, 앞으로의 STR research에서는 같은 training set을 사용하여 모델을 페어링 할것이다.

### 2.2. Real-world datasets for evaluation

7개의 실제real-world STR datasets가 학습된 STR model에 대하여 evaluation을 위해 사용된다. 몇몇은 다른 subset을 사용하는데 이로인해 비교의 불일치가 발생하게 된다.

텍스트의 난이도와 기하학적 레이아웃에 따라 regular와 irregular dataset으로 나눈다.

![image](https://user-images.githubusercontent.com/46878973/165034247-707258e6-84a5-49a8-822f-7c75a58f3bd7.png)

#### 1) Regular Datasets
regular dataset은 수평으로 배치된 텍스트로, 간격이 있는 텍스트도 포함하고 있다. STR에서 쉬운 data에 속한다.

|dataset|for training|for evaluation|content|collect|특징|
|:---:|:---:|:---:|:---:|:---:|:---:|
|IIIT5K-Words (IIIT)|2000 images|3000 images|텍스트 이미지로 반환되는 검색어|Google image searchs|크롤링된 데이터 세트|
|Street View Text (SVT)|256 images|647 images|noisy, blurry, 저해상도|Google Street View|스트리트뷰에서 수집한 이미지|
|ICDAR2003 (IC03)|1156 images|1110 images(867, 860)|-|ICDAR 2003 Robust Reading competition|-|
|ICDAR2013 (IC13)|848 images|1095 images(1015, 857)|-|ICDAR 2013 Robust Reading competition|-|

-IC03 : evaluation datasets로 사용하는 subset이 867 images와 860 images 두 개가 존재한다. 867 images는 3 문자보다 작거나 영숫자가 아닌 문자를 포함하는 경우를 제외한 set이고, 860 images는 867에서 7개의 word box를 제외한 set이다.

-IC13 : evaluation datasets로 사용하는 subset이 1015 images와 857 images 두 개가 존재한다. 1015 images는 영숫자가 아닌 문자를 포함하는 경우를 제외한 set이고, 857 images는 1015 images에서 3개의 문자보다 작은 경우를 뺀 set이다.

#### 2) Irregular Datasets

irregular dataset은 어려운 케이스를 포함하고 있다. 구부러지고 회전되거나 왜곡된 텍스트가 일반적이다.

|dataset|for training|for evaluation|content|collect|
|:---:|:---:|:---:|:---:|:---:|
|ICDAR2015 (IC15)|4468 images|2077 images(1811, 2077)|noisy, blurry, rotated, 저해상도|ICDAR 2015 Robust Reading competition, Google Glasses|
|SVT Perspective (SP)|-|645 images|perpective projections|Google Street View|
|CUTE80 (CT)|-|	288 cropped images|curved text images|natural scenes|

- IC15 : evaluation datasets로 사용하는 subset이 1811 images와 2077 images 두 개가 존재한다. 이전의 논문에서는 1811 images만 사용했다. 1811 images는 영숫자가 아닌 문자를 포함하는 경우를 빼고, 심하게 회전되거나 원근감이 있거나 구부러진 경우 또한 제외하였다.
이전의 model은 서로 다른 기준의 dataset을 가지고 평가했다. IC03의 경우에는 data 7개로 인해 0.8%의 큰 성능차를 보인다.

## 3. STR Framework Analysis

![image](https://user-images.githubusercontent.com/46878973/165050025-2ed80e10-350a-473f-b6cf-ffba4a261c9a.png)

 CRNN(Convolutional-Recurrent Neural Network) = CNN(Convolutional Neural Networks) + RNN(Recurrent Neural Networks) : 첫번째 CNN과 RNN의 STR을 위한 combination 

![image](https://user-images.githubusercontent.com/46878973/165034983-7f24fa22-a33d-4b0d-8a16-635aaa702513.png)

|Trans(변환)|Extraction(추출)|Seq(시퀀스)|Predict(예측)|
|:---:|:---:|:---:|:---:|
|None|VGG|None|CTC|
TPS|ResNet|BiLSTM|Attn|

### 3.1. Transformation Stage (Tran.)

input text image를 normalize한다.(정규화하기)

- None

- TPS(Thin-Plate Spline) : STN(Spatial Transformer Network, 공간 변환 네트워크)의 변형, smooth spline interpolation 사용  
  - 문자 영역을 미리 정의된 사각형으로 변환해준다.  
  - 여러기준점(위쪽 및 아래쪽 둘러싸는점들)을 정규화한다.

### 3.2. Feature Extraction Stage (Feat.)

input image를 문자 인식 관련 속성에 초점을 둔 표현과 연결하고 관련없는 특징(font, color, size, background)은 억제한다.

다음은 모두 CNN의 변형으로, STR의 feature extrators로 쓰인다.

- VGG : convolution layer 여러개 + fully connected layer 조금

- RCNN : 문자 모양에 따른 receptive field 조정을 위해 재귀적으로 적용 가능

- ResNet : 더 deep한 CNN의 training을 쉽게 하는 residual connections

### 3.3. Sequence Modeling Stage (Seq.)

다음 stage를 위해 문맥 정보를 캡쳐한다. (독립적으로 하는 것보다 강력해짐)

- None : 계산 복잡도와 메모리 소비 때문에 사용하지 않는 경우도 있음

- BiLSTM : 더 나은 시퀀스를 만들기 위한 modul

### 3.4. Prediction Stage (Pred.)

이미지의 식별된 특징에서 출력 문자 시퀀스를 추정한다.

- CTC(Connectionist Temporal Classification) : 고정된 개수의 feature가 주어지더라도 고정되지 않은 수의 sequence를 예측할 수 있음 
  - 각 H의 열에 있는 문자를 예측하고 반복되는 문자와 공백을 삭제하여 full character sequence를 고정되지 않은 문자 스트림으로 수정함

- Attn(Attention-based sequence prediction) : output sequence를 예측하기 위해 자동으로 input sequence 내의 information flow를 캡쳐함


## 4. Experiment and Analysis

모든 가능한 STR module combinations(2*3*2*2 = 24)를 four-stage framework로부터 평가하고 분석하였다.

![image](https://user-images.githubusercontent.com/46878973/165036190-9426c1ac-a9b1-4542-b92e-966fdf1ea1b9.png)

### 4.1. Implement Detail
- training batch size : 192
- num of iteration(반복횟수) : 300K
- decay rate(감쇠율) : 0.95 (AdaDelta optimizer)
- gradient clapping value : 5
  - 모든 parameter는 He’s method의 초기화를 따른다.

- training data : MJSynth 8.9M + SynthText 5.5M (14.4M, 1440만)

- validation data : IC13 IC15 IIIT SVT
  - IC03 train data 사용X : IC13와 겹친다 (34 scene images = 215 word boxes)
  - 2000 training steps마다 validate한다. (set에서 가장 높은 정확도를 가지는 model을 택함)  

- evaluation data : IIIT 3000 + SVT 647 + IC03 867 + IC13 1015 + IC15 2077 + SP 645 + CT 288 (8539 images)
  - only 알파벳 + 숫자
  - 5trials(매번 random seeds를 다르게 초기화)를 통해 accuracy의 평균 구함  

- same environment for fair speed comparison!

### 4.2. Analysis on Training Datasets

MJSynth와 SynthText를 각각 사용하는 것보다 둘의 combination으로 train하였을 때, 더 높은 accuracy를 보인다는 것을 확인하였다. (자신들의 best model로 실험한 결과, 약 4.1% 더 높았다)

MJSynth 20% (1.8M) + SynthText 20% (1.1M)의 조합(2.9M으로 SynthText의 반 크기)으로 하였을 때 각각 사용한 것보다 더 좋은 accuracy를 보였다. 다시말해, training images의 개수보다는 **training data의 다양성**이 더 중요하다는 것을 알 수 있다.

### 4.3. Analysis on Trade-offs for Module Combinations

![image](https://user-images.githubusercontent.com/46878973/165209745-fbbb1d07-6faf-4f51-957f-ba1be52663ba.png)

- accuracy-time trade-offs  
: T1 ~ T5까지 ResNet, BiLSTM, TPS, Attn 순서대로 추가
  - ResNet, BiLSTM, TPS : 속도가 늦어지는 반면 정확도가 빠르게 높아짐(+13.4%)
  - Attn : 속도가 많이 늦어진 만큼 정확도가 높아지지 않음(+1.1%)

- accuracy-memory trade-offs  
: P1 ~ P5까지 Attn, TPS, BiLSTM, ResNet 순서대로 추가
  - Attn, TPS, BiLSTM : 메모리 사용을 크게 하지 않음 -> 가벼우면서도 정확성 높여줌
  - ResNet : 메모리를 많이 사용하지만(7.2M -> 49.6M) 그 만큼 정확도가 높아지지 않음(+1.7%)

- accuracy-speed는 prediction(CTC/Attn), accuracy-memory는 feature extrator(ResNet)에서 영향을 크게 미치므로 필요에 따라 선택해야 한다.

### 4.4. Module Analysis

정확도,속도, 메모리 요구량 측면에서 모듈별 성능 분석해보자.

- accuracy-time ResNet, BiLSTM, TPS, Attn 순서대로 upgrade하였을 때 가장 효율적이었다. (T1 -> T5)

- accuracy-memory RCNN, Attn, TPS, BiLSTM, ResNet 순서대로 upgrade하였을 때 가장 효율적이었다. (P1 -> P5)

서로 반대 순서로 upgrade하였을 때 효율적이지만, 결과적으로 가장 효율적이 **combination(TPS-ResNet-BiLSTM-Attn)**은 동일하다.

![image](https://user-images.githubusercontent.com/46878973/165038837-a2bbed46-d3bd-4b7d-ab1e-494118f003c3.png)

- TPS transformation : curved and perspective texts를 normalize하여 standardized view로 변환함

- ResNet feaure extrator : 표현력 향상 (심한 배경 혼란, 처음 보는 font의 경우 개선됨)

- BiLSTM sequence modeling : 관련없는데 잘라진 문자를 무시함

- Attn prediction : 사라지거나 누락된 문자를 찾음

### 4.5. Failure Case Anlysis

![image](https://user-images.githubusercontent.com/46878973/165038935-a917fc78-cd06-4d15-b1e5-699d04a56a70.png)

- Calligraphic Fonts : 브랜드 font, 가게 이름
  - 정규화하는 feature extrator 사용하자!
  - font가 overfitting될 수 있으므로 regularization 하자!

- Vertical Texts : 현재 대부분 horizontal text image 제공

- Special Characters : 현재 이러한 case train안함 -> alphanumeric characters로 취급하여 fail
  - 그냥 sepcial character train하자! (IIIT에서 87.9% -> 90.3%로 정확도 오름)

- Heavy Occlusions : 현재 문맥정보를 광범위하게 사용하지 않음

- Low Resolution : 현재 다루지 않음
  - image pyramids
  - super-resolution modules

- Label Noise : incorrect labels 찾음
  - mislabeling (special characters 포함x) 1.3%
  - mislabeling (special characters 포함) 6.1%
  - mislabeling (case-sensitive) 24.1%

## 5. Conclusion

이 논문은 일관되지 않은 실험 설정 때문에 문제가 있던 STR model의 기여도를 분석하였다. 주요 STR method 중 공통 framework와 일관된 dataset(7 banchmark evaluation dataset + 2 training dataset)을 소개하여, 공정한 비교를 제공하였다.



---


[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}