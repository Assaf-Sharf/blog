---
layout: single
title:  "Pytorch-8 Multi-GPU"
categories : Pytorch
tag : Multi_GPU
toc : true
---

# Multi Gpu 학습

오늘날 딥러닝은 엄청난 데이터와의 싸움
어떻게 gpu를 다룰 것인가?

## 개념 정리
+ Single / Multi
+ Gpu / Node(or system) : node는 한대의 컴퓨터를 쓴다는 뜻
+ Single Node Singe GPU
+ Single Node Multi GPU (주로 이걸로 학습)
+ Multi Node Multi GPU


## Model Parallel
다중 gpu에 학습을 분산하는 두가지 방법
(모델을 나누기 / 데이터를 나누기)
모델을 나누는 것은 생각보다 예전부터 썼음(alexnet)
모델의 병목, 파이프라인의 어려움 등으로 인해 모델 병렬화는 고난이도 과제

좋은 파이프라인을 위해서 두 GPU가 동시에 처리하는 구간이 있어야 함

### 실습 코드
```python
class ModelParallelResNet50(ResNet):
  def __init__(self, *args, **kwargs):
    super(ModelParallelResNet50, self).__init__(
      Bottleneck, [3, 4, 6, 3], num_classes=num_classes, *args, **kwargs)
    
    #첫번째 모델을 cuda에 할당
    self.seq1 = nn.Sequential(
    self.conv1, self.bn1, self.relu, self.maxpool, self.layer1, self.layer2).to('cuda:0")

    #두버째 모델 cuda에 할당
    self.seg2 = nn.Sequential(
      self.layer3, self.layer4, self.avgpool,
      ). to('cuda:1')
    self.fc.to('cuda:1')
  
  #모델 연결
  def forward(self, x):
    x= self.seq2(self.seq1(x).to('cuda: 1'))
    return self.fclx.view(x.size(0),-1))

```

## Data parallel
데이터를 나눠 GPU에 할당 후 결과의 평균을 취하는 방법
minibatch의 수식과 유사한데 한번에 여러 GPU에서 수행

pytorch에서는 두가지 방식을 제공
	DataParallel, DistributedDataParallel=0

- DataParallel : 단순히 데이터를 분배한 후 평균을 취함
- DistributedDataParallel : 각 cpu마다 process생성하여 개별 cpu에 할당

### 실습코드
```pytorch
#DataParallel 은 한줄로 ㅆ가능

parallel_model = torch.nn.DataParallel(model)

```


```pytorch
#DistributedDataParallel

복잡하니 찾아보셈
1)

```