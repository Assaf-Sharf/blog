---
title:  "[ML] 로그 변환 시 np.log()가 아닌 np.log1p()를 사용하는 이유"
layout: single

categories: "ML"
tags: 
    - "머신러닝"

toc: true
toc_sticky: true
toc_label : "목차"
---

***

# 로그(Logarithm)
로그 함수는 x가 (0, 1) 사이의 값을 가질 때, y는 (-∞, 0) 값을 가진다.


<div style="text-align : center;">
<img src="https://w.namu.la/s/d6acfbc3de1edf2cdb37867f48bfeb687549fba33195a7448d84defb13b41ba458e90f57e6808be2fa4faf7c2ee25568e8af44ff1e272af1e39daac6cac97e37a6c5c78ee35213564c67d18665f1e5166abd19f780915b5f04846d7625ecbf1b" width="400">
</div>
<center><small>Fig 3. 로그변환 전(왼쪽), 후(오른쪽) 그래프</small></center>

# Reference
- [위키백과, 우리 모두의 백과사전 - 정규분포](https://ko.wikipedia.org/wiki/%EC%A0%95%EA%B7%9C_%EB%B6%84%ED%8F%AC)
- [티스토리 by 퐁스 - 왜 machine learning에서는 Normal Distributions를 사용할까?](https://da-nika.tistory.com/83)

<br>