---
title:'[DL/CV] 합성곱 신경망 : 컴퓨터 비전의 기본 👓'
layout: single
---

# 6. 합성곱 신경망

## 6.1 합성곱 신경망 알아보기
이 글에서 소개할 합성곱 신경망은 **컨브넷(convnet)**이라고도 부른다. 컨브넷 모델은 거의 대부분의 컴퓨터 비전 (computer vision) 분야에 사용된다. 이 글에서는 작은 데이터셋을 사용한 이미지 분류 문제를 컨브넷을 사용해서 푸는 법을 다룬다.

### 6.1.1 합성곱 연산
합성곱 연산은 **특성맵(feature map)**이라는 랭크-3 텐서에 적용된다. 이 텐서는 2개의 **공간** 축 (**높이**와 **너비**)과 **깊이** 축(**채널** 축)으로 구성된다.RGB 이미지는 3개의 컬러 채널을 갖기 때문에 깊이 축의 차원이 3이다. 예시로 다룰 MNIST 데이터는 흑백이미지이기 때문에 깊이 축의 차원은 1이다. 합성곱 연산은 입력 특성맵에서 작은 **패치**들을 추출하고 이런 모든 패치에 변환을 적용해서 **출력 특성맵**을 만든다.

출력 특성맵도 랭크-3 텐서다. 출력 텐서의 깊이는 층의 매개변수로 결정되기 때문에 RGB 같은 특정 컬러를 의미하지 않고 일종의 **필터(filter)**를 의미한다. 필터는 입력 데이터의 어떤 특성을 인코딩한다.

이제 MNIST 데이터를 활용한 간단한 합성곱 신경망을 구현한 코드를 살펴보자.


```python
from tensorflow import keras
from tensorflow.keras import layers
inputs = keras.Input(shape=(28, 28, 1))  # (28,28,1) 크기의 특성맵을 입력
x = layers.Conv2D(filters=32, kernel_size=3, activation="relu")(inputs)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=64, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=128, kernel_size=3, activation="relu")(x)
x = layers.Flatten()(x)
outputs = layers.Dense(10, activation="softmax")(x)
model = keras.Model(inputs=inputs, outputs=outputs)
```


```python
model.summary()
```

    Model: "model"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     input_1 (InputLayer)        [(None, 28, 28, 1)]       0         
                                                                     
     conv2d (Conv2D)             (None, 26, 26, 32)        320       
                                                                     
     max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         
     )                                                               
                                                                     
     conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     
                                                                     
     max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         
     2D)                                                             
                                                                     
     conv2d_2 (Conv2D)           (None, 3, 3, 128)         73856     
                                                                     
     flatten (Flatten)           (None, 1152)              0         
                                                                     
     dense (Dense)               (None, 10)                11530     
                                                                     
    =================================================================
    Total params: 104,202
    Trainable params: 104,202
    Non-trainable params: 0
    _________________________________________________________________


model.summary( )에서 알 수 있듯이 첫번째 합성곱 층이 (28,28,1) 크기의 특성맵을 입력으로 받아서 (26,26,32) 크기의 특성맵을 출력한다. 즉, 입력에 대해 32개의 필터를 적용한다는 뜻이다. 32개의 출력 채널은 26x26 크기의 배열을 갖는다. 이 출력은 입력의 각 위치에서의 필터 패턴에 대한 응답을 나타내는 **응답 맵(response map)**이다. 

합성곱은 핵심적인 2개의 파라미터로 정의된다.


*   입력으로부터 뽑아낼 패치크기 : 3x3 또는 5x5 크기를 일반적으로 사용한다.
*   특성 맵의 출력 깊이 : 합성곱으로 계산할 필터 개수다. MNIST 예제에서는 32로 시작해서 128로 끝났다.

케라스의 Conv2D 층에서 위의 파라미터는 Conv2D(output_depth,(window_height,window_width))와 같이 매개변수로 전달된다.

입력 특성맵 위를 3x3 또는 5x5 크기의 윈도우가 슬라이딩(sliding)하면서 모든 위치에서 특성 패치를 추출하는 방식으로 합성곱이 작동한다. 이런 패치는 합성곱 커널(convolution kernel)이라고 불리는 학습된 가중치 행렬과의 텐서 곱셉을 통해 (output_depth,) 크기의 1D 벡터로 변환된다. 동일한 커널이 모든 패치에 걸쳐서 재사용된다. 변환된 모든 벡터는 출력 특성맵을 만든다.

아래 그림은 합성곱 작동을 간단히 나타낸 그림이다.

그림1

다음 두가지 이유로 출력 높이와 너비는 입력의 높이,너비와 다를 수 있다.


1.   경계문제 , 패딩 추가 여부
2.   스트라이드의 사용 여부

#### 경계 문제와 패딩 사용
MNIST 예시에서 첫번째 합성곱 층에서 28x28 크기의 입력이 26x26이 되었듯이 다음 사진과 같이 합성곱을 한번 진행하면 높이와 너비 차워을 따라서 정확히 2개의 타일이 줄어든다.

그림2

입력과 동일한 높이와 너비를 가진 출력 특성맵을 얻고 싶으면 **패딩(padding)**을 사용할 수 있다. 패딩은 입력 특성맵의 가장 자리에 0으로 채워진 제로패딩(zero padding)을 추가하면 된다. Conv2D 층에서 padding 매개변수로 설정할 수 있다. 2가지 값을 매개변수로 전달할 수 있는데,

*   valid : 패딩을 사용하지 않는다. (기본값)
*   same  : 입력과 동일한 높이와 너비를 가진 출력을 만들기 위해 패딩한다.

밑의 이미지는 padding : 'same'으로 설정했을 때 패딩이 설정되는 상황이다.

그림3

#### 합성곱 스트라이드 사용하기
출력 크기에 영향을 미치는 다른 요소인 스트라이드(stride)는 두번의 연속적인 윈도우 사이의 거리다. 쉽게 말해서 필터가 슬라이딩하는 걸음의 크기다. 스트라이드의 기본값은 1로, 스트라이드가 1보다 큰 스트라이드 합성곱도 가능하다. 패딩을 설정하지 않았다는 가정하에 스트라이드가 커질수록 출력 특성맵의 크기가 작아진다. (필터의 걸음걸이가 커질수록 합성곱을 하는 횟수가 줄어들기 때문에)

예를 들어 밑의 이미지처럼 스트라이드 2를 사용했다는 것은 특성 맵의 너비와 높이가 2의 배수로 다운샘플링 되었다는 뜻이다. 분류 모델에서는 스트라이드가 드물게 사용된다. 분류 모델에서는 특성맵을 다운샘플링하기 위해서 스트라이드 대신 이어서 다룰 최대 풀링 연산을 사용한다. 

그림4

### 6.1.2 최대 풀링 연산

위의 MNIST 컨브넷 예제에서 MaxPooling2D 층마다 특성맵의 크기가 절반으로 줄어들었다. 크기가 26x26인 특성맵이 첫번째 MaxPooling 층을 지나고 13x13으로 줄어들었다. 최대 풀링연산은 스트라이드 합성곱처럼 강제적으로 특성맵을 다운샘플링하는 것이다.

최대 풀링은 입력 특성 맵에서 윈도우에 맞는 패치를 추출하고 각 채널별로 최대값을 추출한다. 합성곱과 다른 점은 최대 풀링은 보통 2x2 윈도우와 스트라이드 2를 사용해서 특성 맵을 절반 크기로 다운샘플링한다는 것이다. (*합성곱은 일반적으로 3x3 윈도우와 스트라이드 1을 사용한다.*)

다운 샘플링을 하는 이유가 뭘까? 먼저 최대 풀링층이 빠진 컨브넷을 살펴보자.


```python
inputs = keras.Input(shape=(28, 28, 1))
x = layers.Conv2D(filters=32, kernel_size=3, activation="relu")(inputs)
x = layers.Conv2D(filters=64, kernel_size=3, activation="relu")(x)
x = layers.Conv2D(filters=128, kernel_size=3, activation="relu")(x)
x = layers.Flatten()(x)
outputs = layers.Dense(10, activation="softmax")(x)
model_no_max_pool = keras.Model(inputs=inputs, outputs=outputs)
```


```python
model_no_max_pool.summary()
```

    Model: "model_1"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     input_2 (InputLayer)        [(None, 28, 28, 1)]       0         
                                                                     
     conv2d_3 (Conv2D)           (None, 26, 26, 32)        320       
                                                                     
     conv2d_4 (Conv2D)           (None, 24, 24, 64)        18496     
                                                                     
     conv2d_5 (Conv2D)           (None, 22, 22, 128)       73856     
                                                                     
     flatten_1 (Flatten)         (None, 61952)             0         
                                                                     
     dense_1 (Dense)             (None, 10)                619530    
                                                                     
    =================================================================
    Total params: 712,202
    Trainable params: 712,202
    Non-trainable params: 0
    _________________________________________________________________


위의 모델 요약표를 보면 , 최종 특성맵은 22 x 22 x 128 = 61952개의 파라미터를 가진다. 이를 마지막 10개의 유닛을 가진 Dense층과 연결하면 총 50만개가 넘는 가중치 파라미터를 갖게 된다. 이는 작은 모델치고는 너무 많은 가중치고, 그 결과 심각한 과대적합이 발생한다.

간단히 말해서 다운샘플링을 하는 이유는 처리할 특성맵의 가중치 개수를 줄이기 위해서다. 다운샘플링을 하는 방법은 최대풀링 뿐만 아니라 앞에서 언급한 특성맵의 특성들의 평균값보다 여러 특성 중 최대값을 사용하는 것이 더 유용하기 때문에 최대 풀링층을 사용하는 것이 최선이다.


```python

```
