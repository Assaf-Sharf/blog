---
layout: single        # 문서 형식
title: Multi Layer Perceptron (MLP)         # 제목
categories: DL    # 카테고리
toc: true             # 글 목차
author_profiel: false # 홈페이지 프로필이 다른 페이지에도 뜨는지 여부
sidebar:              # 페이지 왼쪽에 카테고리 지정
    nav: "docs"       # sidebar의 주소 지정
#search: false # 블로그 내 검색 비활성화
---

# 1. Neuron
## In Biology
전기적 및 화학적 신호를 통해 정보를 처리하고 전송하는 전기적으로 흥분시키는 세포. 뉴런 사이의 신호는 다른 세포와의 특별하게 연결된 시냅스에서 발생. 뉴런은 신경망을 형성할 수 있도록 상호 연결 가능.
![11](https://github.com/SeokHwanHong/SeokHwanHong.github.io/assets/126441224/644bf9a8-fac8-4f39-91e6-f56a5191797d)

# 2. Perceptron
## In 



# 1. Intorduction


# 2. Background


# 3. Model Architecture
## 3.1. Encoder & Decoder Stacks
### Encoder
### Decoder

## 3.2. Attention
### Scaled Dot-Product Attention
### Multi-Head Attention
### Applications of Attention in our Model

## 3.3. Position-wise Feed-Forward Networks
## 3.4. Embeddings and Softmax
## 3.5. Positional Encoding

# 4. Why Self-Attention

# 참고
https://velog.io/@grovy52/Fully-Connected-Layer-FCL-%EC%99%84%EC%A0%84-%EC%97%B0%EA%B2%B0-%EA%B3%84%EC%B8%B5
