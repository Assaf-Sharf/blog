---
layout: single        # 문서 형식
title: Multi Layer Perceptron (MLP)         # 제목
categories: DL    # 카테고리
toc: true             # 글 목차
author_profiel: false # 홈페이지 프로필이 다른 페이지에도 뜨는지 여부
sidebar:              # 페이지 왼쪽에 카테고리 지정
    nav: "docs"       # sidebar의 주소 지정
#search: false # 블로그 내 검색 비활성화
---

# 1. Neuron
### In Biology
전기적 및 화학적 신호를 통해 정보를 처리하고 전송하는 전기적으로 흥분시키는 세포입니다. 뉴런 사이의 신호는 다른 세포와의 특별하게 연결된 시냅스에서 발생합니다. 뉴런은 신경망을 형성할 수 있도록 끝과 끝이 연결 가능합니다.

### In Deep Learning
생물학적인 뉴런의 개념에 기조한 수학적인 함수를 의미합니다. 뉴런의 활성화 유무에 따라 활성함수가 결정됩니다. 이 때, 해당 뉴런의 결과가 0이라면, 신호를 주고받지 않는 비활성화 상태임을 알 수 있습니다.

# 2. Perceptron




# 1. Intorduction


# 2. Background


# 3. Model Architecture
## 3.1. Encoder & Decoder Stacks
### Encoder
### Decoder

## 3.2. Attention
### Scaled Dot-Product Attention
### Multi-Head Attention
### Applications of Attention in our Model

## 3.3. Position-wise Feed-Forward Networks
## 3.4. Embeddings and Softmax
## 3.5. Positional Encoding

# 4. Why Self-Attention

# 참고
http://kbrain.co.kr/board_FXki69/890
https://davinci-ai.tistory.com/20

https://velog.io/@grovy52/Fully-Connected-Layer-FCL-%EC%99%84%EC%A0%84-%EC%97%B0%EA%B2%B0-%EA%B3%84%EC%B8%B5
