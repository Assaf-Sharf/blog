---
layout: single
title:  "핸즈온 머신러닝 - 14"
categories : scikit-learn
tag : [scikit-learn, machine-learning, python]
toc: true
toc_sticky: true
use_math : true
---

![header](https://capsule-render.vercel.app/api?type=waving&color=a2dcec&height=300&section=header&text=핸즈온 머신러닝 - 14&fontSize=40&animation=fadeIn&fontAlignY=38&fontColor=FFFFFF)

- 참고 : [핸즈온 머신러닝 2판](http://www.kyobobook.co.kr/product/detailViewKor.laf?mallGb=KOR&ejkGb=KOR&barcode=9791162242964)

------------------------------------------------------

&nbsp;

## 합성곱 신경망을 사용한 컴퓨터 비전

- 지각은 간단한 문제가 아니고 이를 이해하기 위해서는 감각기관의 작동 원리를 이해해야 함
- 함성곱 신경망(CNN)은 1980년대부터 이미지 인식 분야에 사용
- CNN 시각분야 뿐만 아니라 **음성 인식**, **자연어 처리**와 같은 다른 작업에도 많이 사용됨

&nbsp;

### 시각 피질 구조

- 대뇌의 뉴런들이 시야의 일부 범위 안에 있는 시각 자극에만 반응
- 고수준의 뉴런이 저수준 뉴런의 출력에 기반한다는 의미를 얻음
- 위의 구조가 결과적으로 전체 시야 영역에 포함된 모든 종류의 복잡한 패턴을 감지할 수 있게 함

&nbsp;

### 합성곱 층

- CNN의 가장 중요한 구성 요소는 **합성곱 층**

  - ![image-20230222133107473](/images/2023-02-22-hands_on_14/image-20230222133107473.png)

  - 첫 번째 합성곱 층의 뉴런은 입력 이미지의 모든 픽셀에 연결되는 것이 아니라 합성곱 층 뉴런의 수용장 안에 있는 픽셀에만 연결

  - 두 번째 합성곱 층에 있는 각 뉴런은 첫 번째 층의 작은 사각 영역 안에 위치한 뉴런에 연결

  

  > 위와 같은 구조는 네트워크가 첫 번째 은닉층에서는 작은 저수준 특성에 집중하고, 그다음 은닉층에서는 더 큰 고수준 특성으로 조합해나가도록 도와줌. 
  >
  > 해당 하는 계층적 구조는 실제 이미지에서 흔히 볼 수 있으며, 이는 CNN이 이미지 인식에 잘 작동되는 이유 중 하나

&nbsp;

- **패딩**

  ![image-20230222133401775](/images/2023-02-22-hands_on_14/image-20230222133401775.png)

  - 위 이미지와 같이 높이와 너비를 이전 층과 같게 하기 위해 입력 주위에 0을 추가하는 것을 **제로 패딩**이라고 함

  ![image-20230222133454928](/images/2023-02-22-hands_on_14/image-20230222133454928.png)

  - 수용장 사이에 간격을 두어 큰 입력층을 훨씬 작은 층에 연결하는 것도 가능
    - 해당 기능은 모델의 계산 복잡도를 낮춰줌
  - 한 수용장과 다음 수용장 사이 간격을 **스트라이드**라고 함
  - 위 그림에서 5x7 입력층(제로 패딩 적용)이 3x3 수용장과 스트라이드 2를 사용해 3x4층에 연결
  - 상위층의 i행, j열에 있는 뉴런이 이전 층의 $i * S_h$에서 $ i * S_h + f_h -1$ 까지의 행과 $ j * S_w $ 에서 $ j * S_w + f_w -1 $ 까지의 열에 위치한 뉴런과 연결
  - 여기서 $ S_h, S_w $는 스트라이드의 수직 값과 수평 값  

&nbsp;

#### 필터

![image-20230222140006007](/images/2023-02-22-hands_on_14/image-20230222140006007.png)

- 뉴런의 가중치는 수용장 크기의 작은 이미지로 표현될 수 있음
- **필터**라 부르는 두 개의 가중치 세트를 위 그림에서 보면 첫 번째것은 가운데 흰 수직선이 있는 검은 사각형
  - 이런 가중치를 사용한 뉴런은 가운데 수직선 부분을 제외하고는 수용장에 있는 모든 것을 무시할 것

- 두 번째 필터는 가운데 흰 수평선이 있는 검은 사각형
  - 해당 가중치를 사용한 뉴런은 가운데 수평선 부분을 제외하고는 수용장 안의 모든 것을 무시

- 층의 전체 뉴런에 적용된 하나의 필터는 하나의 **특성 맵**을 생성
  - 특성맵은 필터를 가장 크게 활성화시키는 이미지의 영역을 강조
  - 수동으로 필터를 정의할 필요가 없음
  - 훈련하는 동안 합성곱 층이 자동으로 해당 문제에 가장 유용한 필터를 찾고 상위층은 이들을 연결하여 더 복잡한 패턴을 학습


&nbsp;

#### 여러 가지 특성 맵 쌓기

- 실제 합성곱층은 여러 가지 필터를 가지고 필터마다 하나의 특성 맵을 출력하므로 3D로 표현

- 각 특성 맵의 픽셀은 하나의 뉴런에 해당하고 하나의 특성 맵 안에서는 모든 뉴런이 같은 파라미터를 공유하지만, 다른 특성 맵에 있는 뉴런은 다른 파라미터를 사용

  - 한 수용장은 이전 층에 있는 모든 특성 맵에 걸쳐 확장 됨
  - 하나의 합성곱 층이 입력에 여러 필터를 동시에 적용하여 입력에 있는 여러 특성을 감지할 수 있음

  > CNN이 한 지점에서 패턴을 인식하도록 학습되었다면 다른 어느위치에서도 해당 패턴 인지 가능
  >
  > DNN은 한 지점에 있는 패턴을 인식하도록 학습되었다면 오직 패턴이 그 위치에 있을 때만 감지가 가능

- 입력 이미지는 **컬러 채널**마다 하나씩 여러 서브 층으로 구성되기도 함, 컬러 채널은 전형적으로 빨강, 파랑, 초록의 RGB 3개 
  - 흑백 이미지는 하나의 채널만 가짐, 하지만 어떤 이미지는 매우 많은 채널을 가질 수 있음
    - 위성 이미지는 가시광선외에도 다른 빛의 파장이 기록됨

![image-20230222180115602](/images/2023-02-22-hands_on_14/image-20230222180115602.png)

- 구체적으로 보면, 합성곱 층 $l$에 있는 $k$특성 맵의 $i$행,$j$열에 위치한 뉴런은 이전 $l-1$층에 있는 모든 특성 맵의 $i * S_h$에서 $i * S_h + f_h - 1$까지의 행과 $i * S_w$에서 $i * S_w + f_w -1$까지의 열에 있는 뉴런의 출력에 연결
  - 다른 특성 맵이더라도 같은 $i$행과 $j$열에 있는 뉴런이라면 정확히 이전층에 있는 동일한 뉴런들의 출력에 연결됨
- **합성곱 층에 있는 뉴런의 출력 계산**\
  - $z_{i,j,k} = b_k + \sum_{u=0}^{f_h-1} \sum_{v=0}^{f_w-1} \sum_{k'=0}^{f_n'-1} \chi_{i',j',k'} * W_{u,v,k',k}$
  - $i' = i * S_h + u, j' = j * S_w + v$
  - 해당 수식은 합성곱 층에서 한 뉴런의 출력을 계산하는 법에 해당, 입력에 대한 가중치 합을 계산하고 편향을 더하는 것


&nbsp;

#### 텐서플로 구현

- 텐서플로에서 각 입력 이미지는 보통 [높이, 너비, 채널] 형태의 3D 텐서로 표현

- 하나의 미니배치는 [미니배치 크기, 높이, 너비, 채널] 형태의 4D 텐서로 표현

- 합성곱 층의 가중치는 [$f_h, f_w, f_n', f_n$] 형태의 4D 텐서로 표현, 합성곱 층의 편향은 간단하게 [$f_n$] 형태의 1D 텐서로 나타냄

-  예제

  ```python
  import numpy as np
  from sklearn.datasets import load_sample_image
  
  # 샘플 이미지를 로드합니다.
  china = load_sample_image("china.jpg") / 255
  flower = load_sample_image("flower.jpg") / 255
  images = np.array([china, flower])
  batch_size, height, width, channels = images.shape
  
  # 2개의 필터를 만듭니다.
  filters = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)
  filters[:, 3, :, 0] = 1  # 수직선
  filters[3, :, :, 1] = 1  # 수평선
  
  outputs = tf.nn.conv2d(images, filters, strides=1, padding="SAME")
  
  plt.imshow(outputs[0, :, :, 1], cmap="gray") # 첫 번째 이미지의 두 번째 특성맵을 그립니다.
  plt.axis("off") # 책에는 없습니다.
  plt.show()
  ```

  ![image-20230222191200636](/images/2023-02-22-hands_on_14/image-20230222191200636.png)

  - tf.nn.conv2d()
    - image는 입력의 미니배치(4D)
    - filters는 적용될 일련의 필터(4D)
    - strides는 1이나 4개의 원소를 갖는 1D 배열로 지정할 수 있음, 1D 배열의 가운데 두 개의 원소는 수직, 수평, 스트라이드이고 현재는 첫 번째와 마지막 원소가 1이어야 함
    - padding은 'valid'와 'same' 중 하나를 지정
      - 'valid'로 지정하면 합성곱 층에 제로 패딩을 사용하지 않음
      - ![image-20230222192817569](/images/2023-02-22-hands_on_14/image-20230222192817569.png)
      - ![image-20230222193122146](/images/2023-02-22-hands_on_14/image-20230222193122146.png)

  - 해당 예제에서는 필터를 직접 지정, 하지만 CNN을 사용할때는 보통 훈련 가능한 변수로 필터를 정의 하므로 앞서 설명한 것처럼 신경망이 가장 잘 맞는 필터를 학습할 수 있음

    ```python
    conv = keras.layers.Conv2D(filters=32, kernel_size=3, strides=1, 
                               padding='same', activation='relu')
    ```

    - 해당 코드는 3x3 크기의 32개의 필터, 수평 수직 스트라이드 1, 'same' 패딩을 사용하는 conv2d 층을 만들고 출력을 위해 ReLU 활성화 함수를 적용
    - 합성곱 층을 위해 꽤 많은 하이퍼파마리터가 필요 <- 필터의 수, 필터의 높이와 너비, 스트라이드, 패딩 종류

&nbsp;

#### 메모리 요구 사항

- CNN에 관련된 또 하나의 문제는 합성곱 층이 많은 양의 RAM을 필요로 함
  - 특히 훈련하는 동안에 역전파 알고리즘이 역방향 계산을 할 때 정방향에서 계산했던 모든 중간 값을 필요로 하기 때문
  - 예시로 5 x 5 필터로 스트라이드 1과 'same' 패딩을 상요해 150 x 100 크기의 특성 맵 200개를 만드는 합성곱을 예시로 한다면
    - 입력이 150 x 100 RGB 이미지,  파라미터 수는 ((5x5x3+1)x200=15,200) (+1은 편향) 해당수는 완전연결 층보다는 적지만 200개의 특성 맵마다 150x100개의 뉴런을 포함하고, 각 뉴런이 5x5x3개의 입력에 대한 가중치 합을 계산
    - 추론을 수행할 때는 하나의 층이 점유하고 있는 RAM은 다음 층의 계산이 완료되자마자 해제가 가능
      - 연속된 두 개의 층에서 필요로 하는 만큼의 RAM만 가지고 있으면 가능함

&nbsp;

### 풀링 층

- 풀링 층의 목적은 계산량과 메모리 사용량, 결과적으로 과대적합을 줄여주는 파라미터 수를 줄이기 위해 입력 이미지의 **부표본**을 만드는 것

- 풀링 층의 각 뉴런은 이전 층의 작은 사각 영역의 수용장 안에 있는 뉴런의 출력과 연결되어 있음

  - 이전과 동일하게 크기, 스트라이드, 패딩 유형을 지정해야 함

- 풀링층은 가중치 없고, 최대나 평균 같은 합산 합수를 사용해 입력값을 더하는 연산

  - **최대 풀링** 층 예시

    ![image-20230224123918108](/images/2023-02-22-hands_on_14/image-20230224123918108.png)

    - 2 x 2 **풀링 커널** 과 스트라이드 2를 사용하며 패딩은 없음
    - 1,5,3,2 중 최댓값인 5가 다음 층으로 전달, 스트라이드가 2이므로 높이/2, 너비/2

- 계산량, 메모리, 파라미터 수를 감소하는 것 외에도 최대 풀링은 작은 변화에도 일정 수준의 **불변성**을 생성

- ![image-20230224124130123](/images/2023-02-22-hands_on_14/image-20230224124130123.png)
  - 위와 같은 a,b,c 2x2 커널, 스트라이드 2인 최대 풀링을 통과
    - 이미지 b와 c는 이미지 a와 동일하지만 각각 오른쪽으로 한 픽셀과 두 픽셀 이동한 것
      - 이미지 a,b의 최대 풀링층의 출력은 동일함, 이를 **이동 불변성**
    - 이미지 c의 출력은 조금 다른데, 오른쪽으로 한픽셀 이동함
      - CNN에서 몇 개 층마다 최대 풀링 층을 추가하면 전체적으로 일정 수준의 이동 불변성을 얻을 수 있음 또한 최대 풀링은 회전과 확대, 축소에 대해 약간의 불변성을 제공함
      - 이와 같은 불변성은 분류 작업처럼 예측을 수행할때 작은 부분에서 영향을 받지 않는 경우 유용

- 최대 풀링의 단점
  - 최대 풀링은 매우 파괴적
    - 작은 2x2 커널과 스트라이드 2를 사용하더라도 출력은 양방향으로 절반이 줄어들어 입력값의 75%  손실
  - **동변성**
    - 입력 이미지가 오른쪽으로 한 픽셀 이동했다면 출력도 오른쪽으로 한 픽셀 이동해야 함

&nbsp;

#### 텐서플로 구현

- 텐서플로에서 최대 풀링 층을 구현하는 것은 아주 쉬움

- 2x2 커널을 사용해 최대 풀링 예제

  ```python
  max_pool = keras.layers.MaxPool2D(pool_size=2)
  ```

- **평균 풀링 층**을 만들때는 MaxPool2D 대신 AvgPool2D를 사용

  - 해당 층은 최댓값이 아닌 평균을 계산하는 것만 빼고 최대 풀링 층과 동일하게 작동
  - 일반적으로 평균 풀링 < 최대 풀링 

- 깊이방향 풀링 층은 케라스에서 지원 X, 텐서플로 지원 tf.nn.max_pool()

  - 커널 크기와 스트라이드를 4개의 원소를 가진 튜플로 지정
  - 첫 번째 세 값은 1, 이는 배치 ,높이 ,너비 차원을 따라 커널 크기와 스트라이드가 1이라는 의미
  - 예를 들어 3과 같이 깊이 차원을 따라 원하는 커널 사이즈와 스트라이드를 마지막 값에 지정

  ```python
  output =  tf.nn.max_pool(inputs,
                           ksize=(1, 1, 1, 3),
                           strides=(1, 1, 1, 3),
                           padding="valid")
  
  # keras 층으로 사용할 경우 lambda
  depth_pool = keras.layers.Lambda(lambda X: tf.nn.max_pool(
      X, ksize=(1, 1, 1, 3), strides=(1, 1, 1, 3), padding="VALID"))
  ```

- 최근 신경망 구조에서는 마지막 출링 층의 종류는 **전역 평균 풀링 층** 사용

  - 해당 층의 작동방법은 각 특성 맵의 평균을 계산 하는 방식
  - 각 샘플의 특성 맵마다 하나의 숫자를 출력한다는 의미, 물론 매우 파괴적인 연산 이미지만 출력층에는 유용할 수 있음

  ```python
  global_avg_pool = keras.layers.GlobalAvgPool2D()
  
  # 높이 너비를 따라 평균을 계산하는 Lambda 층과 동일
  output_global_avg2 = keras.layers.Lambda(lambda X: tf.reduce_mean(X, axis=[1, 2]))
  ```

&nbsp;

### CNN 구조

- 전형적인 CNN 구조는 합성곱 층을 몇 개 쌓고 그다음에 풀링층을 쌓고, 그 다음에 또 합성곱을 몇 개 더 쌓고 그 다음에 다시 풀링 층을 쌓는 방식으로 수행

- 네트워크를 통과하여 진행할수록 이미지는 점점 작아지지만 합성곱 때문에 일반적으로 점점 더 깊어짐

- 맨 위층에는 몇 개의 완전 연결 층으로 구성된 일반적인 피드포워드 신경망이 추가되고 마지막 층에서 예측을 출력

  ![image-20230224130406245](/images/2023-02-22-hands_on_14/image-20230224130406245.png)

- 패션 MNIST 데이터셋 CNN 예제

  ```python
  from functools import partial
  
  DefaultConv2D = partial(keras.layers.Conv2D,
                          kernel_size=3, activation='relu', padding="SAME")
  
  model = keras.models.Sequential([
      DefaultConv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]),
      keras.layers.MaxPooling2D(pool_size=2),
      DefaultConv2D(filters=128),
      DefaultConv2D(filters=128),
      keras.layers.MaxPooling2D(pool_size=2),
      DefaultConv2D(filters=256),
      DefaultConv2D(filters=256),
      keras.layers.MaxPooling2D(pool_size=2),
      keras.layers.Flatten(),
      keras.layers.Dense(units=128, activation='relu'),
      keras.layers.Dropout(0.5),
      keras.layers.Dense(units=64, activation='relu'),
      keras.layers.Dropout(0.5),
      keras.layers.Dense(units=10, activation='softmax'),
  ])
  ```

  - 첫 Layer 64 필터 7x7, 스트라이드 1
  - 풀링 크기 2인 최대 풀링 추가, 공간 방향 차원을 절반으로 축소
  - CNN이 출력층에 갈수록 필터 개수가 늘어남, 저수준 특성의 개수는 적지만 이를 연결하여 고수준 특성을 만들 수 있는 방법이 많기 때문에 해당 구조 사용
  - 풀링층 다음으로는 필터 개수를 두 배로 늘리는 것이 일반적인 방법, 풀링 층이 공간 방향 차원을 절반으로 줄이기 때문에 이어지는 층에서 파라미터 개수, 메모리 사용량, 계산 비용을 크게 늘리지 않고 특성 맵 개수를 두 배로 늘림
  - 두 개의 은닉층과 하나의 출력층으로 구성된 완전 연결 네트워크, 1d 연결을 위해 Flatten 사용, 밀집 층 사이에 과대적합을 줄이기 위해 0.5 드랍아웃 비율을 가진 드롭아웃 층 추가

&nbsp;

### 발전한 CNN 구조 설명

&nbsp;

#### LeNet-5

- LeNet-5 구조는 가장 널리 알려진 CNN구조

- 구조

  ![image-20230224134502739](/images/2023-02-22-hands_on_14/image-20230224134502739.png)

  - 28 x 28 인 input을 Layer 적용전에 zero-padding 수행해 32 x 32 변경 및 정규화
  - 네트워크 다른 부분에서는 zero-padding 사용하지 않음, 그래서 이미지 네트워크를 따라 진행하면서 크기 축소
  - 평균 풀링층이 복잡, 각 뉴런의 입력의 평균 계산한 다음 해당 값에 학습되는 계숫값을 곱함 그 후 학습되는 값인 편향을 더하는 방식으로 적용
  - 출력층에서 입력과 가중치 벡터를 곱셈하는 방식 대신 각 뉴런에서 입력 벡터와 가중치 벡터 사이의 유클리드 거리를 출력, 각 출력은 이미지가 얼마나 특정 숫자 클래스에 속하는지 측정함
    - 최근에는 Cross_Entropy 사용

&nbsp;

#### AlexNet

- 크기가 더 깊어졌을 뿐 LeNet-5와 비슷하고 처음으로 합성곱 층 위에 풀링 층을 쌓지 않고 바로 합성곱 층끼리 적용하는 구조

- 구조

  ![image-20230224135047296](/images/2023-02-22-hands_on_14/image-20230224135047296.png)

  - 과대적합을 줄이기 위해 2가지 규제 기법을 사용
    1. 훈련하는 동안 F9과 F10의 출력에 드롭아웃을 50% 비율로 적용
    2. 훈련 이미지를 랜덤하게 여러 간격으로 이동하거나 수평으로 뒤집고 조명을 바꾸는 식으로 **데이터 증식** 수행
  - ReLU 단계 이후 LRN이라 부르는 경쟁적인 정규화 단계를 사용, 가장 강하게 활성화된 뉴런이 다른 특성 맵에 있는 같은 위치의 뉴런을 억제
    - 해당 수행은 특성 맵을 각기 특별하게 다른 것과 구분되게 하고, 더 넓은 시각에서 특징을 탐색하도록 만들어 결국 일반화 성능을 향상
  - AlexNet의 하이퍼파라미터는 $r=2, a=0.00002, B = 0.75, k=1$
    - $r : 깊이 반경, a : 뉴런 활성화 값, B : 뉴런의 정규화된 출력, k : 편향$
    - tf.nn.local_response_normalization() 연산을 사용해 구현 가능

&nbsp;

#### GoogLeNet

- 네트워크가 이전 CNN보다 훨씬 더 깊어짐, **인셉션 모듈**이라는 서브 네트워크를 가지고 있어서 이전의 구조보다 훨씬 효과적으로 파라미터를 사용

- AlexNet보다 10배나 적은 파라미터를 가짐

- 설명

  ![image-20230224140707809](/images/2023-02-22-hands_on_14/image-20230224140707809.png)

  - '3x3+1(S)' : 3x3커널, 스트라이드 1, padding='same' 사용한다는 의미
  - 처음에 입력 신호가 복사되어 네 개의 다른 층에 전달, 모든 합성곱 층은 ReLU 활성화 함수를 사용
  - 두 번째 합성곱 층은 각기 다른 커널크기(1x1, 3x3, 5x5)를 사용해 다른 크기의 패턴을 잡음, 모든 층은 스트라이드 1과 'same' 패딩을 사용하므로 출력의 높이와 너비가 모두 입력과 같음
    - 모든 출력을 **깊이 연결 층**에 깊이방향으로 연결할 수 있음
    - 해당 연결 층은 텐서플로의 axis=3 (3은 깊이방향 축), 매개변수로 tf.concat() 연산을 사용해 구현

- 인셉션 모듈이 1x1 커널의 합성곱 층을 가지는 이유

  1. 공간상의 패턴을 잡을 수는 없지만 깊이 차원을 따라 놓인 패턴을 잡음
  2. 입력보다 더 작은 특성 맵을 출력하므로 차원을 줄인다는 의미인 **병목 층**의 역할을 수행, 연산 비용과 파라미터 개수를 줄여 훈련 속도를 높이고 일반화 성능을 향상
  3. 합성곱 층의 쌍 ([1x1, 3x3]과 [1x1, 5x5])이 더 복잡한 패턴을 감지할 수 있는 한 개의 강력한 합성곱 층처럼 작동, 실제로 합성곱 층의 쌍은 단순한 선형 분류기 하나로 이미지를 훑는 것이 아닌 두 개의 층을 가진 신경망으로 이미지를 훑음

> 전체 인셉션 모듈을 여러 크기의 복잡한 패턴이 담긴 특성 맵을 출력하는 합성곱 층

&nbsp;

- 구조

  ![image-20230224141839194](/images/2023-02-22-hands_on_14/image-20230224141839194.png)

  - 합성곱 층과 풀링 층에서 출력되는 특성 맵의 수는 커널 크기 앞에 표시 되어 있음
  - 해당 네트워크는 매우 깊어서 세 개의 열로 나타냄
  - GoogLeNet은 실제로 네트워크를 하나로 길게 쌓은 구조이고 9개의 인셉션 모듈을 포함하고 있음
    - 인셉션 모듈에 있는 6개의 숫자는 모듈 안에 있는 합성곱 층에서 출력하는 특성 맵의 수를 나타냄
    - 모든 합성곱 층은 ReLU 활성화 함수를 사용

- 네트워크

  - 처음의 두 층은 계산양을 줄이기 위해 이미지의 높이와 너비를 4배로 줄임
  - 많은 정보를 유지하기 위해 첫 번째 층은 큰 크기의 커널을 사용
  - LRN 층은 이전 층이 다양한 특성을 학습하도록 만듬, 이어지는 두 개의 합성곱 층 중에서 첫 번째 층이 앞서 설명했듯이 병목 층처럼 작동, 이 합성곱 쌍을 하나의 합성곱 층으로 생각 할수 있음
  - 다시 LRN층이 이전 층의 다양한 패턴을 학습하도록 만듬
  - 최대 풀링 층이 계산 속도를 높이기 위해 이미지의 높이와 너비를 2배로 줄임
  - 9개의 인셉션 모듈이 이어지면서 차원 감소와 속도 향상을 위해 몇 개의 최대 풀링 층을 넣음
  - 전역 평균 풀링 층이 각 특성 맵의 평균을 출력, 여기서 공간 방향 정보를 모두 잃음 이 지점에서는 남아 있는 공간 정보가 많지 않기 때문에 괜찮음
    - 실제 GoogleNet는 입력이 224x224, 5번의 최대 풀링 층에서 매번 높이와 너비가 절반으로 줄어들면 특성 맵의 크기는 7x7
    - 위치 추정이 아니라 분류 작업이므로 물체가 어디 있는지는 중요하지 않음. 
    - 해당 층에서 수행된 차원 축소에 의해 CNN 위에 몇 개의 완전 연결 층을 둘 필요가 없음, 파라미터의 수를 크게 감소시키고 과대적합의 위험도 줄어줌
  - 마지막 층은 규제를 위한 드롭아웃 층 다음에 1000개 유닛과 소프트맥스 활성화 함수를 적용한 완전 연결 층으로 클래스 확률 추정 값을 출력

&nbsp;

#### VGGNet

- 매우 단순하고 고전적인 구조, 2개 또는 3개의 합성곱층 뒤에 풀링 층이 나오고 다시 2개 또는 3개의 합성곱 층과 풀링 층이 등장하는 방식(VGGNet 종류에 따라 총 16개 또는 19개의 합성곱 층이 존재)
- 마지막 밀집 네트워크는 2개의 은닉층과 출력층으로 이루어짐
- VGGNet은 3x3필터만 사용

&nbsp;

#### ResNet

