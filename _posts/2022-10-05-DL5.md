---
layout: single
title:  "DL-5 CNN example"
categories : DL
tag : CNN
toc : true
use_math : true
---


네트워크는 깊게
parameter는 작게
성능은 높게
하는 과정들을 보여주마

## AlexNet

![alexnet](/images/2022-10-05-DL5/alexnet.png)

성공 이유
- ReLU 사용
linear model 들의 성질 보존
vanishing gradient 문제 극복
gradient descent에 적용 쉽
generalization에 좋음

- GPU 사용 (2개)
- LRN, Overlapping pooling (요즘 안씀)
- Data augmentation
- Dropout


## VGGNet

![image-20221005112021186](/images/2022-10-05-DL5/image-20221005112021186.png)

### Why 3x3 convolution?

3x3 두개를 사용했을 때 가
5x5 한개 사용했을 때보다 
parameter수가 적다

## GoogLeNet
![image-20221005112538756](/images/2022-10-05-DL5/image-20221005112538756.png)

- NiN 구조(Network in Network)
- Inception Block
![image-20221005113040043](/images/2022-10-05-DL5/image-20221005113040043.png)
1x1 으로 parameter수가 1/3로 줄음
![image-20221005113458445](/images/2022-10-05-DL5/image-20221005113458445.png)

## ResNet
이전의 네트워크가 커짐에 따라 학습이 제대로 이뤄지지 않는 문제를 해결하기 위한 방법 제시

![image-20221005113833952](/images/2022-10-05-DL5/image-20221005113833952.png)

차원을 같게 하기 위해 1x1 convolution 방법도 추가(근데 많이 안씀)

![image-20221005114008036](/images/2022-10-05-DL5/image-20221005114008036.png)

## DenseNet
resnet에서 더하면 값이 섞이니까 concat하면 어떨까

![image-20221005114308978](/images/2022-10-05-DL5/image-20221005114308978.png)
concat은 채널이 점점 커지는 단점이 있다.

중간에 채널을 한번씩 줄이는 과정 -> 1x1 conv 사용
