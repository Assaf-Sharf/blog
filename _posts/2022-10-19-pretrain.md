---
title: '[DL/CV] 사전 훈련 모델 : 소규모 데이터 다루기 🏋️'
layout : single
---

## 3. 사전 훈련된 모델 활용하기
### 3.0 들어가며
**사전 훈련된 모델**은 대량의 데이터셋에서 미리 훈련된 모델이다. 사용하는 원본 데이터가 충분히 크고 일반적이라면 사전 훈련된 모델에 의해 학습된 특성은 실제 세상에 대한 일반적인 모델로 효율적인 역할을 할 수 있다. 모델이 다룰 새로운 문제가 사전 훈련된 모델이 다루던 클래스와 완전히 다르더라도 이런 특성은 많은 컴퓨터 비전에 유용하다. 예를 들어, 동물이나 생활 용품으로 이루어진 ImageNet 데이터셋에서 사전 훈련된 모델을 가구를 식별하는 모델에 사용할 수 있다. 이전 글에서도 언급했듯이 이러한 유연성이 딥러닝의 가장 큰 장점이다. 이런 장점을 이용해서 사전 훈련된 모델을 작은 이미지 데이터셋에 적용할 수 있다.

그림 0

이번에는 (1,400만개의 레이블된 이미지와 1,000개의 클래스로 이루어진) ImageNet 데이터셋에서 훈련된 대규모 컨브넷을 사전 훈련된 모델로 사용한다. 사전 훈련된 모델을 사용하는 방법은 두가지가 있다. **특성추출 (feature extraction)**과 **미세조정 (fine tuning)**이다.

### 3.1 사전 훈련된 모델을 사용한 특성 추출
특성 추출은 사전 훈련된 모델의 표현을 사용해서 새로운 샘플에서 흥미로운 특성을 뽑아내는 것이다. 이젠 글에서 보았듯이 컨브넷은 이미지 분류를 위해 연속된 합성곱과 풀링층으로 시작해서 밀집 연결 분류기로 끝난다. 컨브넷에서 합성곱과 풀링층으로 구성된 부분을 **합성곱 기반층**이라고 하는데, 컨브넷의 경우 특성 추출은 사전에 훈련된 합성곱 기반층을 선택하여 새로운 데이터를 통과 시키고, 그 출력으로 새로운 분류기를 훈련한다.

그림1

그럼 왜 합성곱 층만 재사용할까? 합성곱 층에 의해 학습된 표현이 더 일반적이기 때문에 재사용에 용이하기 때문이다. 하지만 (밀집 연결) 분류기에서 학습환 표현은 모델이 훈련된 클래스 집합에 특화되어 있기 때문에 일반적으로 분류기의 재사용은 권장되지 않는다. 

특정 합성곱 층에서 추출한 표현의 일반성(재사용성)의 수준은 모델에 있는 층의 깊이에 따라 다르다. 모델의 하위층은 에지,색,질감 등 매우 일반적인 특성 맵을 추출하고, 상위 층은 '강아지 눈'처럼 좀 더 추상적인 개념을 추출한다. 따라서 모델이 다룰 새로운 데이터셋이 사전훈련된 모델이 훈련한 데이터셋과 많이 다르다면 전체 합성곱 기반층을 사용하기 보단 모델의 하위 층 몇개만 특성 추출에 사용하는 것이 좋다.

우리가 사용할 ImageNet의 클래스 집합에는 많은 강아지와 고양이가 포함되어 있기 때문에 원본 모델의 완전 연결 층에 있는 정보를 재사용하는 것도 좋은 방법이 될것 같지만, 일반적인 문제 상황들을 다루기 위해 이번 글에서는 완전 연결 층을 사용하지는 않겠다. ImageNet 데이터셋에 훈련된 **VGG16** 네트워크의 합성곱 기반층을 사용해서 특성 추출을 사용하겠다. 

VGG16 모델은 케라스에 패키지로 포함되어 있다. kears.applications 모듈에서 임포트할 수 있는데, keras.applications 모듈에서 사용가능한 ImageNet 데이터셋으로 사전 훈련된 이미지 분류 모델은 다음과 같다.


*   Xception
*   ResNet
* MobileNet
* EfficientNet
* DenseNEt
* etc..


먼저 VGG16 모델을 만들기 전에 [**이전 글**](https://hamin-chang.github.io/small/)처럼 dog-vs-cats 데이터를 내려받고 전처리를 진행한다.


```python
import gdown # 데이터 내려받기
gdown.download(id='18uC7WTuEXKJDDxbj-Jq6EjzpFrgE7IAd', output='dogs-vs-cats.zip') 
!unzip -qq dogs-vs-cats.zip
!unzip -qq train.zip
```

    Downloading...
    From: https://drive.google.com/uc?id=18uC7WTuEXKJDDxbj-Jq6EjzpFrgE7IAd
    To: /content/dogs-vs-cats.zip
    100%|██████████| 852M/852M [00:09<00:00, 86.2MB/s]



```python
import os, shutil, pathlib   # 내려 받은 데이터 준비

original_dir = pathlib.Path("train")
new_base_dir = pathlib.Path("cats_vs_dogs_small")

def make_subset(subset_name, start_index, end_index):
    for category in ("cat", "dog"):
        dir = new_base_dir / subset_name / category
        os.makedirs(dir)
        fnames = [f"{category}.{i}.jpg" for i in range(start_index, end_index)]
        for fname in fnames:
            shutil.copyfile(src=original_dir / fname,
                            dst=dir / fname)

make_subset("train", start_index=0, end_index=1000)
make_subset("validation", start_index=1000, end_index=1500)
make_subset("test", start_index=1500, end_index=2500)
```


```python
from tensorflow.keras.utils import image_dataset_from_directory

train_dataset = image_dataset_from_directory(
    new_base_dir / "train",
    image_size=(180, 180),
    batch_size=32)
validation_dataset = image_dataset_from_directory(
    new_base_dir / "validation",
    image_size=(180, 180),
    batch_size=32)
test_dataset = image_dataset_from_directory(
    new_base_dir / "test",
    image_size=(180, 180),
    batch_size=32)
```

    Found 2000 files belonging to 2 classes.
    Found 1000 files belonging to 2 classes.
    Found 2000 files belonging to 2 classes.


이제 VGG16 모델을 구축해보자.


```python
from tensorflow import keras
conv_base = keras.applications.vgg16.VGG16(
    weights = 'imagenet',       # 매개변수1 weights : 모델을 초기화할 가중치 체크포인트를 지정
    include_top = False,        # 매개변수2 include_top : 네트워크 맨위에 놓인 밀집 연결 분류기 포함 여부
    input_shape = (180,180,3))  # 매개변수3 input_shape : 네트워크에 주입할 이미지 텐서 크기 (선택사항)
```

    Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5
    58889256/58889256 [==============================] - 0s 0us/step



```python
conv_base.summary()
```

    Model: "vgg16"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     input_1 (InputLayer)        [(None, 180, 180, 3)]     0         
                                                                     
     block1_conv1 (Conv2D)       (None, 180, 180, 64)      1792      
                                                                     
     block1_conv2 (Conv2D)       (None, 180, 180, 64)      36928     
                                                                     
     block1_pool (MaxPooling2D)  (None, 90, 90, 64)        0         
                                                                     
     block2_conv1 (Conv2D)       (None, 90, 90, 128)       73856     
                                                                     
     block2_conv2 (Conv2D)       (None, 90, 90, 128)       147584    
                                                                     
     block2_pool (MaxPooling2D)  (None, 45, 45, 128)       0         
                                                                     
     block3_conv1 (Conv2D)       (None, 45, 45, 256)       295168    
                                                                     
     block3_conv2 (Conv2D)       (None, 45, 45, 256)       590080    
                                                                     
     block3_conv3 (Conv2D)       (None, 45, 45, 256)       590080    
                                                                     
     block3_pool (MaxPooling2D)  (None, 22, 22, 256)       0         
                                                                     
     block4_conv1 (Conv2D)       (None, 22, 22, 512)       1180160   
                                                                     
     block4_conv2 (Conv2D)       (None, 22, 22, 512)       2359808   
                                                                     
     block4_conv3 (Conv2D)       (None, 22, 22, 512)       2359808   
                                                                     
     block4_pool (MaxPooling2D)  (None, 11, 11, 512)       0         
                                                                     
     block5_conv1 (Conv2D)       (None, 11, 11, 512)       2359808   
                                                                     
     block5_conv2 (Conv2D)       (None, 11, 11, 512)       2359808   
                                                                     
     block5_conv3 (Conv2D)       (None, 11, 11, 512)       2359808   
                                                                     
     block5_pool (MaxPooling2D)  (None, 5, 5, 512)         0         
                                                                     
    =================================================================
    Total params: 14,714,688
    Trainable params: 14,714,688
    Non-trainable params: 0
    _________________________________________________________________


conv_base.summary( )의 결과 코드로 알 수 있듯이, VGG16의 구조는 이전 글의 컨브넷 구조와 비슷하다.최종 특성맵의 크기는 (5,5,512)이다. 이 특성 위에 밀집 연결층을 놓을 것이다. 밀집 연결층을 놓는 방법에는 여러가지가 있지만 이번에 사용할 방법은 느리고 비용이 많이 들지만 훈련하는 동안 데이터 증식 기법을 사용할 수 있는 장점이 있는 방법이다. conv_base와 새로운 밀집 분류기를 연결한 새로운 모델을 만들고 입력 데이터를 사용해서 **엔트-두-엔드(end to end)**로 실행한다.

이 방법을 사용하려면 먼저 **합성곱 기반층을 동결**해야한다. 층을 **동결(freeze)**한다는 것은 훈련하는 동안 동결한 층의 가중치가 업데이트 되지 않도록 막는다는 뜻이다. 층을 동결하지 않으면 사전 학습된 표현이 훈련하는 동안 수정되기 떄문에 동결해주는 것이 중요하다. 특히 Dense 층은 랜덤하게 초기화되었기 때문에 매우 큰 가중치 업데이트 값이 네트워크에 전파될 것이고, 이는 사전에 학습된 표현을 크게 훼손하레 된다.

그림2


```python

conv_base  = keras.applications.vgg16.VGG16(
    weights="imagenet",
    include_top=False)
conv_base.trainable = False  # 층이나 모델을 동결
```

trinable 속성을 False로 지정하면 층이나 모델의 훈련 가능한 가중치 리스트가 빈 상태가 된다.


```python
conv_base.trainable = True
print("합성곱 기반 층을 동결하기 전의 훈련 가능한 가중치 개수:",
      len(conv_base.trainable_weights))
conv_base.trainable = False
print("합성곱 기반 층을 동결한 후의 훈련 가능한 가중치 개수:",
      len(conv_base.trainable_weights))
```

    합성곱 기반 층을 동결하기 전의 훈련 가능한 가중치 개수: 26
    합성곱 기반 층을 동결한 후의 훈련 가능한 가중치 개수: 0


이제 다음을 연결해서 새로운 모델을 만든다.


1.   데이터 증식 층
2.   동결된 합성곱 기반 층
3.   밀집 분류기



```python
from tensorflow.keras import layers
data_augmentation = keras.Sequential(    # 데이터 증식 층 준비
    [
        layers.RandomFlip("horizontal"),
        layers.RandomRotation(0.1),
        layers.RandomZoom(0.2),
    ]
)

inputs = keras.Input(shape=(180, 180, 3))
x = data_augmentation(inputs)                      #1. 데이터 증식 적용
x = keras.applications.vgg16.preprocess_input(x)   #2. 입력 값의 스케일 조정
x = conv_base(x)                                   #3. 사전 훈련 모델의 합성곱 기반층 연결
x = layers.Flatten()(x)                            
x = layers.Dense(256)(x)                           #4. 밀집연결 분류층연결
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1, activation="sigmoid")(x) #4. 밀집연결 분류층연결
model = keras.Model(inputs, outputs)
model.compile(loss="binary_crossentropy",          #5. 모델 컴파일
              optimizer="rmsprop",
              metrics=["accuracy"])
```

이렇게 하면 추가한 2개의 Dense 층의 가중치만 훈련된다. 컴파일을 하고 나서 trainable 속성을 변결하면 반드시 모델을 다시 컴파일해야 변경 사항이 적용된다.

이제 모델을 훈련해보자. (GPU 사용 권장)


```python
callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath="feature_extraction_with_data_augmentation.keras",
        save_best_only=True,
        monitor="val_loss")
]
history = model.fit(
    train_dataset,
    epochs=50,
    validation_data=validation_dataset,
    callbacks=callbacks)
```

    Epoch 1/50
    63/63 [==============================] - 28s 234ms/step - loss: 24.2400 - accuracy: 0.8860 - val_loss: 5.8173 - val_accuracy: 0.9630
    Epoch 2/50
    63/63 [==============================] - 14s 218ms/step - loss: 6.3334 - accuracy: 0.9535 - val_loss: 6.3287 - val_accuracy: 0.9580
    Epoch 3/50
    63/63 [==============================] - 13s 208ms/step - loss: 5.8064 - accuracy: 0.9540 - val_loss: 11.1468 - val_accuracy: 0.9450
    Epoch 4/50
    63/63 [==============================] - 13s 206ms/step - loss: 5.1000 - accuracy: 0.9640 - val_loss: 3.2755 - val_accuracy: 0.9790
    Epoch 5/50
    63/63 [==============================] - 13s 199ms/step - loss: 5.2509 - accuracy: 0.9575 - val_loss: 3.7557 - val_accuracy: 0.9750
    Epoch 6/50
    63/63 [==============================] - 13s 200ms/step - loss: 5.3840 - accuracy: 0.9615 - val_loss: 8.0226 - val_accuracy: 0.9540
    Epoch 7/50
    63/63 [==============================] - 13s 200ms/step - loss: 3.2646 - accuracy: 0.9700 - val_loss: 3.3343 - val_accuracy: 0.9790
    Epoch 8/50
    63/63 [==============================] - 13s 200ms/step - loss: 3.4100 - accuracy: 0.9640 - val_loss: 3.3068 - val_accuracy: 0.9730
    Epoch 9/50
    63/63 [==============================] - 13s 198ms/step - loss: 3.3842 - accuracy: 0.9675 - val_loss: 3.5180 - val_accuracy: 0.9740
    Epoch 10/50
    63/63 [==============================] - 13s 200ms/step - loss: 2.1468 - accuracy: 0.9765 - val_loss: 5.1560 - val_accuracy: 0.9680
    Epoch 11/50
    63/63 [==============================] - 14s 218ms/step - loss: 2.4630 - accuracy: 0.9715 - val_loss: 2.9233 - val_accuracy: 0.9750
    Epoch 12/50
    63/63 [==============================] - 15s 223ms/step - loss: 1.3003 - accuracy: 0.9810 - val_loss: 3.1449 - val_accuracy: 0.9760
    Epoch 13/50
    63/63 [==============================] - 14s 210ms/step - loss: 2.3429 - accuracy: 0.9735 - val_loss: 3.3041 - val_accuracy: 0.9770
    Epoch 14/50
    63/63 [==============================] - 14s 214ms/step - loss: 1.7750 - accuracy: 0.9785 - val_loss: 3.1868 - val_accuracy: 0.9800
    Epoch 15/50
    63/63 [==============================] - 14s 216ms/step - loss: 2.0401 - accuracy: 0.9785 - val_loss: 4.5787 - val_accuracy: 0.9740
    Epoch 16/50
    63/63 [==============================] - 14s 211ms/step - loss: 1.5505 - accuracy: 0.9760 - val_loss: 2.5712 - val_accuracy: 0.9770
    Epoch 17/50
    63/63 [==============================] - 14s 213ms/step - loss: 1.2429 - accuracy: 0.9835 - val_loss: 3.1378 - val_accuracy: 0.9750
    Epoch 18/50
    63/63 [==============================] - 13s 208ms/step - loss: 1.3922 - accuracy: 0.9835 - val_loss: 3.4441 - val_accuracy: 0.9740
    Epoch 19/50
    63/63 [==============================] - 14s 219ms/step - loss: 1.1997 - accuracy: 0.9815 - val_loss: 2.2584 - val_accuracy: 0.9770
    Epoch 20/50
    63/63 [==============================] - 14s 217ms/step - loss: 1.1991 - accuracy: 0.9815 - val_loss: 2.4760 - val_accuracy: 0.9790
    Epoch 21/50
    63/63 [==============================] - 14s 212ms/step - loss: 1.7837 - accuracy: 0.9810 - val_loss: 2.8045 - val_accuracy: 0.9810
    Epoch 22/50
    63/63 [==============================] - 13s 204ms/step - loss: 0.8158 - accuracy: 0.9850 - val_loss: 2.5982 - val_accuracy: 0.9780
    Epoch 23/50
    63/63 [==============================] - 14s 211ms/step - loss: 0.7123 - accuracy: 0.9860 - val_loss: 2.7911 - val_accuracy: 0.9790
    Epoch 24/50
    63/63 [==============================] - 13s 209ms/step - loss: 0.8304 - accuracy: 0.9880 - val_loss: 2.6523 - val_accuracy: 0.9810
    Epoch 25/50
    63/63 [==============================] - 14s 216ms/step - loss: 0.9900 - accuracy: 0.9820 - val_loss: 1.9689 - val_accuracy: 0.9800
    Epoch 26/50
    63/63 [==============================] - 14s 219ms/step - loss: 1.0719 - accuracy: 0.9825 - val_loss: 1.4147 - val_accuracy: 0.9850
    Epoch 27/50
    63/63 [==============================] - 14s 213ms/step - loss: 0.5439 - accuracy: 0.9865 - val_loss: 1.6537 - val_accuracy: 0.9800
    Epoch 28/50
    63/63 [==============================] - 14s 213ms/step - loss: 1.1899 - accuracy: 0.9830 - val_loss: 2.3143 - val_accuracy: 0.9750
    Epoch 29/50
    63/63 [==============================] - 13s 204ms/step - loss: 0.6581 - accuracy: 0.9870 - val_loss: 2.8935 - val_accuracy: 0.9740
    Epoch 30/50
    63/63 [==============================] - 13s 202ms/step - loss: 0.8145 - accuracy: 0.9880 - val_loss: 2.2261 - val_accuracy: 0.9800
    Epoch 31/50
    63/63 [==============================] - 13s 200ms/step - loss: 0.2852 - accuracy: 0.9920 - val_loss: 2.3301 - val_accuracy: 0.9780
    Epoch 32/50
    63/63 [==============================] - 13s 199ms/step - loss: 0.8237 - accuracy: 0.9835 - val_loss: 2.7280 - val_accuracy: 0.9790
    Epoch 33/50
    63/63 [==============================] - 13s 199ms/step - loss: 0.8051 - accuracy: 0.9875 - val_loss: 2.2396 - val_accuracy: 0.9830
    Epoch 34/50
    63/63 [==============================] - 14s 225ms/step - loss: 0.7072 - accuracy: 0.9870 - val_loss: 2.5835 - val_accuracy: 0.9760
    Epoch 35/50
    63/63 [==============================] - 14s 207ms/step - loss: 0.4059 - accuracy: 0.9885 - val_loss: 3.0067 - val_accuracy: 0.9770
    Epoch 36/50
    63/63 [==============================] - 14s 213ms/step - loss: 0.6995 - accuracy: 0.9865 - val_loss: 3.0686 - val_accuracy: 0.9740
    Epoch 37/50
    63/63 [==============================] - 14s 215ms/step - loss: 0.5893 - accuracy: 0.9865 - val_loss: 3.0155 - val_accuracy: 0.9770
    Epoch 38/50
    63/63 [==============================] - 14s 212ms/step - loss: 0.7542 - accuracy: 0.9870 - val_loss: 3.0868 - val_accuracy: 0.9750
    Epoch 39/50
    63/63 [==============================] - 14s 217ms/step - loss: 0.6990 - accuracy: 0.9860 - val_loss: 2.4846 - val_accuracy: 0.9810
    Epoch 40/50
    63/63 [==============================] - 15s 231ms/step - loss: 0.2238 - accuracy: 0.9940 - val_loss: 2.2336 - val_accuracy: 0.9800
    Epoch 41/50
    63/63 [==============================] - 15s 236ms/step - loss: 0.5515 - accuracy: 0.9885 - val_loss: 2.1520 - val_accuracy: 0.9800
    Epoch 42/50
    63/63 [==============================] - 13s 210ms/step - loss: 0.5610 - accuracy: 0.9885 - val_loss: 3.1262 - val_accuracy: 0.9730
    Epoch 43/50
    63/63 [==============================] - 14s 211ms/step - loss: 0.4819 - accuracy: 0.9890 - val_loss: 2.1244 - val_accuracy: 0.9790
    Epoch 44/50
    63/63 [==============================] - 14s 214ms/step - loss: 0.7232 - accuracy: 0.9885 - val_loss: 2.3481 - val_accuracy: 0.9800
    Epoch 45/50
    63/63 [==============================] - 14s 216ms/step - loss: 0.5577 - accuracy: 0.9875 - val_loss: 2.4692 - val_accuracy: 0.9780
    Epoch 46/50
    63/63 [==============================] - 14s 223ms/step - loss: 0.5547 - accuracy: 0.9875 - val_loss: 2.1546 - val_accuracy: 0.9830
    Epoch 47/50
    63/63 [==============================] - 13s 200ms/step - loss: 0.4297 - accuracy: 0.9915 - val_loss: 2.3955 - val_accuracy: 0.9790
    Epoch 48/50
    63/63 [==============================] - 13s 197ms/step - loss: 0.3008 - accuracy: 0.9920 - val_loss: 2.4069 - val_accuracy: 0.9760
    Epoch 49/50
    63/63 [==============================] - 13s 199ms/step - loss: 0.2787 - accuracy: 0.9905 - val_loss: 2.9077 - val_accuracy: 0.9700
    Epoch 50/50
    63/63 [==============================] - 13s 202ms/step - loss: 0.5859 - accuracy: 0.9880 - val_loss: 2.9722 - val_accuracy: 0.9740


결과를 그래프로 나타내보면 , 검증 정확도가 98%에 도달했다. 이전 글에서 다룬 사전 훈련된 모델을 사용하지 않은 컨브넷 모델보다 향상된 성능을 보인다.


```python
import matplotlib.pyplot as plt
acc = history.history["accuracy"]
val_acc = history.history["val_accuracy"]
loss = history.history["loss"]
val_loss = history.history["val_loss"]
epochs = range(1, len(acc) + 1)
plt.plot(epochs, acc, "bo", label="Training accuracy")
plt.plot(epochs, val_acc, "b", label="Validation accuracy")
plt.title("Training and validation accuracy")
plt.legend()
plt.figure()
plt.plot(epochs, loss, "bo", label="Training loss")
plt.plot(epochs, val_loss, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.legend()
plt.show()
```


    
![png](20221019_files/20221019_16_0.png)
    



    
![png](20221019_files/20221019_16_1.png)
    


테스트 정확도를 확인해보면,


```python
test_model = keras.models.load_model(
    "feature_extraction_with_data_augmentation.keras")
test_loss, test_acc = test_model.evaluate(test_dataset)
print(f"테스트 정확도: {test_acc:.3f}")
```

    63/63 [==============================] - 8s 116ms/step - loss: 2.4500 - accuracy: 0.9780
    테스트 정확도: 0.978


테스트 정확도 97.8%를 얻었다. 이전 모델에 비해서 크게 향상되지는 않았다. 모델의 정확도는 항상 평가하려는 샘플 세트에 따라 달라지기 때문에 이 점은 유의해야한다.

### 3.2 사전훈련된 모델 미세조정하기
모델을 재사용하는 또 하나의 방법은 특성 추출을 보완하는 **미세 조정**이다. 아래의 그림중 가운데 그림처럼 미세 조정은 특성 추출에 사용했던 동결 모델의 상위 층 몇개를 동결에서 해제하고 모델에 새로 추가한 층 (여기서는 **밀집 연결 분류기**)과 함께 훈련하는 것이다.

그림 5 

모델을 미세조정하는 단계는 다음과 같다.


1.   사전에 훈련된 기반 네트워크 위에 새로운 네트워크(분류기)를 추가한다.
2.   기반 네트워크를 동결한다.
3.   새로 추가한 네트워크(분류기)를 훈련한다.
4.   기반 네트워크에서 일부 층의 동결을 해제한다. (배치 정규화 층은 동결해제하면 안된다. VGG16에는 이런 층이 없기 때문이다.)
5.   동결을 해제한 층과 새로 추가한 층을 함께 훈련한다.

앞에서 랜덤하게 초기화된 상단 분류기를 훈련하기 위해 VGG16의 합성곱 기반층을 동결하고 상단 분류기를 훈련해야한다는 이유와 같은 이유로, 맨위에 있는 분류기가 훈련된 후 합성곱 기반의 상위층을 미세 조정할 수 있다. 분류기가 미리 훈련되지 않으면 훈련되는 동안 너무 큰 오차 신호가 네트워크에 전파된다.

1,2,3 단계는 특성 추출을 할 때 이미 완료했다. conv_ base의 동결을 해제하고 개별층을 동결하는 네 번째 단계를 진행하자. 다시한번 합성곱 기반 층의 구조를 살펴보자.




```python
conv_base.summary()
```

    Model: "vgg16"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     input_2 (InputLayer)        [(None, None, None, 3)]   0         
                                                                     
     block1_conv1 (Conv2D)       (None, None, None, 64)    1792      
                                                                     
     block1_conv2 (Conv2D)       (None, None, None, 64)    36928     
                                                                     
     block1_pool (MaxPooling2D)  (None, None, None, 64)    0         
                                                                     
     block2_conv1 (Conv2D)       (None, None, None, 128)   73856     
                                                                     
     block2_conv2 (Conv2D)       (None, None, None, 128)   147584    
                                                                     
     block2_pool (MaxPooling2D)  (None, None, None, 128)   0         
                                                                     
     block3_conv1 (Conv2D)       (None, None, None, 256)   295168    
                                                                     
     block3_conv2 (Conv2D)       (None, None, None, 256)   590080    
                                                                     
     block3_conv3 (Conv2D)       (None, None, None, 256)   590080    
                                                                     
     block3_pool (MaxPooling2D)  (None, None, None, 256)   0         
                                                                     
     block4_conv1 (Conv2D)       (None, None, None, 512)   1180160   
                                                                     
     block4_conv2 (Conv2D)       (None, None, None, 512)   2359808   
                                                                     
     block4_conv3 (Conv2D)       (None, None, None, 512)   2359808   
                                                                     
     block4_pool (MaxPooling2D)  (None, None, None, 512)   0         
                                                                     
     block5_conv1 (Conv2D)       (None, None, None, 512)   2359808   
                                                                     
     block5_conv2 (Conv2D)       (None, None, None, 512)   2359808   
                                                                     
     block5_conv3 (Conv2D)       (None, None, None, 512)   2359808   
                                                                     
     block5_pool (MaxPooling2D)  (None, None, None, 512)   0         
                                                                     
    =================================================================
    Total params: 14,714,688
    Trainable params: 0
    Non-trainable params: 14,714,688
    _________________________________________________________________


block4_pool까지의 모든 층은 동결을 유지하고 block_conv1,block5_conv2,block5_conv3의 마지막 3개의 합성곱 층을 미세조정한다. 왜 마지막 3개층만 미세조정을 할까? 다음 사항들을 고려해서 더 많은 층을 미세조정하지 않는다.


*   상위 층이 좀 더 특화된 특성을 인코딩한다. 새로운 문제에 모델을 재사용하도록 수정이 필요한 경우 구체적인 특성들을 미세조정하는 것이 유리하다.
*   훈련해야할 파라미터가 많을수록 과대적합의 위험이 커진다.

위의 특성 추출 코드에 이어서 미세 조정을 진행해보자.


```python
conv_base.trainable = True           # 일단 모든 층 동결 해제
for layer in conv_base.layers[:-4]:  # 마지막에서 네번째 층까지 다시 동결
    layer.trainable = False

model.compile(loss='binary_crossentropy',
              optimizer=keras.optimizers.RMSprop(learning_rate=1e-5),  # 학습률을 낮춘 RMSProp 사용
              metrics = ['accuracy'])
```

위의 코드에서 모델의 미세조정을 할 때 학습률을 낮춘 RMSPRop 옵티마이저를 사용했다. 학습률을 낮추는 이유는 미세 조정하는 3개의 층에서 학습된 표현을 조금씩 수정하기 위해서이다. 변경량이 너무 크면 학습된 표현에 나쁜 영향을 미칠 수 있다.


```python
callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath="fine_tuning.keras",
        save_best_only=True,
        monitor="val_loss")
]
history = model.fit(
    train_dataset,
    epochs=30,
    validation_data=validation_dataset,
    callbacks=callbacks)
```

    Epoch 1/30
    63/63 [==============================] - 18s 229ms/step - loss: 0.4511 - accuracy: 0.9905 - val_loss: 2.0589 - val_accuracy: 0.9790
    Epoch 2/30
    63/63 [==============================] - 15s 231ms/step - loss: 0.2285 - accuracy: 0.9925 - val_loss: 2.0689 - val_accuracy: 0.9780
    Epoch 3/30
    63/63 [==============================] - 15s 235ms/step - loss: 0.4764 - accuracy: 0.9910 - val_loss: 2.0117 - val_accuracy: 0.9800
    Epoch 4/30
    63/63 [==============================] - 14s 219ms/step - loss: 0.3964 - accuracy: 0.9890 - val_loss: 2.0602 - val_accuracy: 0.9800
    Epoch 5/30
    63/63 [==============================] - 15s 234ms/step - loss: 0.2867 - accuracy: 0.9920 - val_loss: 2.0307 - val_accuracy: 0.9780
    Epoch 6/30
    63/63 [==============================] - 14s 223ms/step - loss: 0.3168 - accuracy: 0.9940 - val_loss: 1.9557 - val_accuracy: 0.9800
    Epoch 7/30
    63/63 [==============================] - 14s 225ms/step - loss: 0.4473 - accuracy: 0.9890 - val_loss: 1.7469 - val_accuracy: 0.9810
    Epoch 8/30
    63/63 [==============================] - 14s 226ms/step - loss: 0.2489 - accuracy: 0.9950 - val_loss: 1.5100 - val_accuracy: 0.9830
    Epoch 9/30
    63/63 [==============================] - 14s 220ms/step - loss: 0.1794 - accuracy: 0.9935 - val_loss: 2.1540 - val_accuracy: 0.9810
    Epoch 10/30
    63/63 [==============================] - 14s 220ms/step - loss: 0.1329 - accuracy: 0.9950 - val_loss: 1.5805 - val_accuracy: 0.9820
    Epoch 11/30
    63/63 [==============================] - 14s 220ms/step - loss: 0.2450 - accuracy: 0.9925 - val_loss: 2.0013 - val_accuracy: 0.9800
    Epoch 12/30
    63/63 [==============================] - 14s 218ms/step - loss: 0.1363 - accuracy: 0.9950 - val_loss: 1.9100 - val_accuracy: 0.9800
    Epoch 13/30
    63/63 [==============================] - 15s 227ms/step - loss: 0.2063 - accuracy: 0.9920 - val_loss: 2.2855 - val_accuracy: 0.9800
    Epoch 14/30
    63/63 [==============================] - 14s 220ms/step - loss: 0.2365 - accuracy: 0.9945 - val_loss: 1.8352 - val_accuracy: 0.9800
    Epoch 15/30
    63/63 [==============================] - 14s 222ms/step - loss: 0.1775 - accuracy: 0.9960 - val_loss: 2.0298 - val_accuracy: 0.9810
    Epoch 16/30
    63/63 [==============================] - 14s 220ms/step - loss: 0.0786 - accuracy: 0.9965 - val_loss: 2.0367 - val_accuracy: 0.9790
    Epoch 17/30
    63/63 [==============================] - 14s 219ms/step - loss: 0.1522 - accuracy: 0.9950 - val_loss: 1.7857 - val_accuracy: 0.9820
    Epoch 18/30
    63/63 [==============================] - 14s 221ms/step - loss: 0.1063 - accuracy: 0.9950 - val_loss: 2.0976 - val_accuracy: 0.9780
    Epoch 19/30
    63/63 [==============================] - 14s 222ms/step - loss: 0.0697 - accuracy: 0.9955 - val_loss: 2.7184 - val_accuracy: 0.9790
    Epoch 20/30
    63/63 [==============================] - 14s 219ms/step - loss: 0.3324 - accuracy: 0.9925 - val_loss: 1.9485 - val_accuracy: 0.9800
    Epoch 21/30
    63/63 [==============================] - 14s 222ms/step - loss: 0.1314 - accuracy: 0.9945 - val_loss: 2.0972 - val_accuracy: 0.9780
    Epoch 22/30
    63/63 [==============================] - 14s 220ms/step - loss: 0.1675 - accuracy: 0.9930 - val_loss: 2.1365 - val_accuracy: 0.9770
    Epoch 23/30
    63/63 [==============================] - 14s 221ms/step - loss: 0.0600 - accuracy: 0.9960 - val_loss: 2.0396 - val_accuracy: 0.9790
    Epoch 24/30
    63/63 [==============================] - 14s 221ms/step - loss: 0.1790 - accuracy: 0.9950 - val_loss: 1.8170 - val_accuracy: 0.9860
    Epoch 25/30
    63/63 [==============================] - 14s 218ms/step - loss: 0.1000 - accuracy: 0.9955 - val_loss: 2.5406 - val_accuracy: 0.9780
    Epoch 26/30
    63/63 [==============================] - 14s 227ms/step - loss: 0.0833 - accuracy: 0.9960 - val_loss: 2.1311 - val_accuracy: 0.9840
    Epoch 27/30
    63/63 [==============================] - 14s 221ms/step - loss: 0.0735 - accuracy: 0.9965 - val_loss: 2.1745 - val_accuracy: 0.9790
    Epoch 28/30
    63/63 [==============================] - 14s 220ms/step - loss: 0.0303 - accuracy: 0.9970 - val_loss: 2.0334 - val_accuracy: 0.9830
    Epoch 29/30
    63/63 [==============================] - 14s 220ms/step - loss: 0.1258 - accuracy: 0.9965 - val_loss: 1.9502 - val_accuracy: 0.9820
    Epoch 30/30
    63/63 [==============================] - 14s 223ms/step - loss: 0.0885 - accuracy: 0.9970 - val_loss: 1.9255 - val_accuracy: 0.9800


마지막으로 테스트 데이터에서 모델을 평가해보자.


```python
model = keras.models.load_model("fine_tuning.keras")
test_loss, test_acc = model.evaluate(test_dataset)
print(f"테스트 정확도: {test_acc:.3f}")

```

    63/63 [==============================] - 8s 115ms/step - loss: 1.9344 - accuracy: 0.9770
    테스트 정확도: 0.977


정확도가 97.7%가 나왔다. 미세조정을 하지 않았을 때보다 오히려 낮은 성능을 보여주지만 워낙 높은 정확도라서 성능을 더욱 끌어올리기 어려웠던 것으로 보인다. 하지만 긍정적으로 생각해보면 원본데이터의 10% 가량만 사용하고도 캐글 경연의 최상위 수준의 성능을 뽑아낸 것이기 때문에 좋은 성능의 모델을 만들었다고 말할 수 있다. 

저번 글과 이번글로 이제 이미지 분류문제 중 작은 데이터셋으로 모델을 훈련시키는 일련의 도구들을 익혔다.
