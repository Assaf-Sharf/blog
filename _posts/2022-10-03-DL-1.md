---
layout: single
title:  "DL-1 Historical Review"
categories : DL
tag : 
toc : true
---

# Intro

AI> ML > DL

## Key components of DL
+ the __DATA__ that the model can learn from
+ the __MODEL__ how to transform the data
+ the __LOSS__ function that quantifies the badness of the model
+ the __ALGORITHM__ to adjust the parameters to minimize the loss

## Data
풀고자 하는 문제의 타입에 따라 달라짐
ex) classification, semantic segmentation, detection, pose estimation, visual qna

## Model

## Loss 
우리가 이루고자 하는 목표의 근사값으로 데려가 줄 수 있다.

## Optimization Algorithm

더 나아가 regulation을 넣어 학습이 더디게 일어나게 하는 방식도 생각해본다
Dropout, ealry stopping, weight decay 등등

# Historical Review
출처 : denny britz 의 historical review

## 2012 AlexNet
이미지 대회 수상


## 2013 DQN
강화학습
오늘날의 DEEP MIND가 구글으로 되게한 논문

## 2014 Encoder/Decoder
문장 분석 seq2seq

## 2014 Adam optimzer
현재 가장 많이 쓰이고 있다. 잘 되니까


## 2015 GAN
생성 모델
discriminator와 generator로 나눠서 진행

##  2015 ResNet
이 연구 덕분에 딥러닝 이라는게 가능해졌다
네트워크를 깊게 쌓을 수 있게 도움을 준 논문


## 2017 Transformer
세상을 뒤집어 놓으셨다

## 2018 Bert
큰 말 뭉치로 pretraining 한 뒤에 풀고자 하는 문제에
fine-tuning 하여 적용하는 방식


## 2019 Big Language models(GPT-x)
open ai 에서 제작한 언어 모델
parameter가 굉장히 많다. 

- ## 2020 Self- Supervised Learning
ex) SimCLR
좋은 representation을 학습데이터 외의 데이터로 추가로 나타나겠다

visual representations: 비주얼 적인 것들을 벡터로 잘 바꾸는지

데이터를 오히려 만들어 내는 방향도 있다
