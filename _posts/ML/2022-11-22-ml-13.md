---
title:  "[ML] 경사하강법(Gradient Descent)"
layout: single

categories: "ML"
tags: ["경사하강법"]

toc: true
toc_sticky: true
toc_label : "목차"
toc_icon: "bars"

published: false
---

<small>경사하강법</small>

***

> Gradient 를 이용해서 함숫값을 줄인다고 해서 **Gradient Descent** 라는 표현을 부르게 된다. 경사하강법은 1차 미분계수를 이용해 함수의 최소값을 찾아가는 iterative한 방법이다.

# <span class="half_HL">✔️ 경사하강법</span>
**경사하강법(Gradient Descent)** 은 기본적인 함수 최적화(optimization) 방법 중 하나이다. Steepest Descent 방법이라고도 불린다. 여기서 최적화란 함수의 최댓값 또는 최솟값을 차즌 것을 말한다.

<span class="y_half_HL">머신러닝에서는 Loss를 최소화해 줘야지 가장 좋은 퍼포먼스를 보여줄 수 있기 때문에 Loss를 minimize 하는 parameter를 찾는 것이 핵심이다.</span>

<div style="text-align : center;">
<img src="https://ifh.cc/g/6746FS.png" width="400">
</div>

전역적인 최적해(Global Optimum)
구한다는것은 개런티를 못한다. 아쉽게도... ㅇ지역적인 최솟값으 구한다는 것은 보장되어있지만 ㅠㅠ..
지역적인 최소해만 잘 구하더라도 꽤 성능 좋은 ㅁ노델을 일관되게 구할 수 있다는 것들이 경험적으로 또는 이론적으로 규명되어가고있음 (활발하게 연구되고 있는 분야)

3차원 그래프는 안보이는 부분이 많이 생김 -> 3차원 정보를 2차원에 표현하기위해 등고선(Contour)으로 표현함 

각각의 영역마다 로스가 정의될 거고 가장 작은 loss를 찾는 것이 머신러닝 모형에서 optimization을 잘 진행하는 과정이라고 생각하면 됨
https://ifh.cc/g/6746FS.png

https://ifh.cc/g/BrQ9dm.png

현재가 있고, 가장 좋은 W로 나가나는 과정

https://ifh.cc/g/HZ7khO.png

함수가 복잡하기 때문에 

Gradient를 구하고 gradient 방향으로 해를 계속 업데이트해 나간다.



## 심화
Tc는 하이퍼파라미터로 사람이 결정해 주는 것이다. (ex 0.1, 0.01, ...)


Learning lrate 너무 작으면 줄함숫값이 줄어들긴 줄어드는데 속도가 너무 작다. 한달 두달 까지 학습해야하는 모델들고 있ㄴ는데... Hyper Transfomrmer같은... 이런 모델에는 Learning rate를 조금만 잘못 설정하면 한달 두달 만에 끝나야 하는데 여섯달 이렇게 걸릴 수 있음 

가장 좋은 러닝래이트 구하는 방법은 수학저긍로 정해진 것은 없고 경험적으로 loss 함수의 변화도를 추적하면서 loss함수가 잘 감소안하면 너무 느리게 감소하는데 이런 경우에는 스톱시키고 래이트를 좀 키운 다음에 학습을 시킨다던지 모니터링ㅇ 작업이 계속 필요함

그때그때 상황에 맞게끔 내가 알고
있는 어떤 Optimizer들 그리고 내가
현재 관찰하고 있는 Learning
rate과 어떤 Loss 간의 변화, 이런
걸 잘 분석하면서 적절하게 잘
조절하는 것이 좋겠습니다.
그래서 Learning rate가 너무
작아도 문제인데 너무 커도
문제예요. 모델 complexity도 너무 작으면 안
되고 너무 커도 안 되죠.


step size가 너무 크면 발산해버림 (함숫값이 커지는 영역에 다다르게 돼서 이런식으로 계속 진동하면서 발산해버리는 문제가 생기기도 함)

로스 모니터링을 하면서 발산하고 있는ㄱ ㅓㄴ 아닌ㄴ지 너무 느리게 너무작겟 루혐하는건 아닌ㄴ지 확인해야함

# Stochastic Gradient Descent
모든 데이터를 대상으로 Gradient 를 구할 수 없음 데이터를 잘라서 하나를 batch라고 표현하는데 그 batch마다 gradient descent 를 적용하는 거임.

업데이트 하고 ~~ 5개로 나눴다면 총 5번 하는 효과가 있다.
여러 스텝으로 나아갈 수 있어 효율적으로 함수 최적화해줄수 있음

Full Gradient,
Stochastic Gradient.
Full Gradient가 가능한, 그런
빠르게 계산되는 상황이라면 Full
Gradient를 이용하는 게
당연히 좋겠죠.
하지만 그게 어렵고 비효율적인
상황이라면 Stochastic Gradient
Descent를 쓴다.
특히 딥 러닝에서는 SGD를 이용해서
업데이트를 많이 하더라.

# <span class="half_HL">✔️ Reference</span>
- 


<br>

👩🏻‍💻개인 공부 기록용 블로그입니다
<br>오류나 틀린 부분이 있을 경우 댓글 혹은 메일로 따끔하게 지적해주시면 감사하겠습니다.
{: .notice}