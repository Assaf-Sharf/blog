---
title: "Do Machine Learning Models Learn Statistical Rules Inferred from Data?"
layout: single
excerpt: "Understanding and improving model predictions using rules learned from data."
header:
  overlay_color: "#000"
  overlay_filter: "0.5"
  overlay_image: assets/images/sqrl/workflow.png
  teaser: assets/images/sqrl/workflow.png
  actions:
    - label: "Paper"
      url: https://arxiv.org/abs/2303.01433
    - label: "Code"
      url: https://github.com/DebugML/sqrl
authors: 
  - Aaditya Naik
  - Yinjun Wu
  - Mayur Naik
  - Eric Wong
  

gallery_motivation_error:
  - url: /assets/images/sqrl/mot_fig_2.png
    image_path: /assets/images/sqrl/mot_fig_2.png
    title: A prediction of a car by the EfficientPS model.

gallery_workflow:
  - url: /assets/images/sqrl/workflow.png
    image_path: /assets/images/sqrl/workflow.png
    title: The Workflow of the SQRL Framework.

gallery_od_before:
  - url: /assets/images/sqrl/od-before.png
    image_path: /assets/images/sqrl/od-before.png
    title: A prediction of a car by the EfficientPS model.

gallery_od_after:
  - url: /assets/images/sqrl/od-after.png
    image_path: /assets/images/sqrl/od-after.png
    title: The same prediction after test-time adaptation.

gallery_impute:
  - url: /assets/images/sqrl/impute-before.png
    image_path: /assets/images/sqrl/impute-before.png
    title: The imputed datapoint before test-time adaptation.
  - url: /assets/images/sqrl/impute-after.png
    image_path: /assets/images/sqrl/impute-after.png
    title: The imputed datapoint after test-time adaptation.

gallery_experiments:
  - url: /assets/images/multiplicative_smoothing/blog_plots.png
    image_path: /assets/images/multiplicative_smoothing/blog_plots.png
    title: Experiments

gallery_experiment_inc_stable:
  - url: /assets/images/multiplicative_smoothing/blog_cons_inc_stable.png
    image_path: /assets/images/multiplicative_smoothing/blog_cons_inc_stable.png
    title: Consistent and Incrementally Stable
  - url: /assets/images/multiplicative_smoothing/blog_inc_stable.png
    image_path: /assets/images/multiplicative_smoothing/blog_inc_stable.png
    title: Only Incrementally Stable

gallery_experiment_cert_acc:
  - url: /assets/images/multiplicative_smoothing/blog_cert_acc.png
    image_path: /assets/images/multiplicative_smoothing/blog_cert_acc.png
    title: Certified Accuracy

---

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



> Machine learning models can make critical errors that are easily hidden within vast amounts of data. Such errors often run counter to rules based on human intuition. However, rules based on human knowledge are challenging to scale or to even formalize. We thereby seek to infer statistical rules from the data and quantify the extent to which a model has learned them. We propose a framework SQRL that integrates logic-based methods with statistical inference to derive these rules from a model’s training data without supervision. We further show how to adapt models at test time to reduce rule violations and produce more coherent predictions. SQRL generates up to 300K rules over datasets from vision, tabular, and language settings. We uncover up to 158K violations of those rules by state-of-the-art models for classification, object detection, and data imputation. Test-time adaptation reduces these violations by up to 68.7% with relative performance improvement up to 32%. SQRL is available at https://github.com/DebugML/sqrl.

Machine learning models can make several mistakes that stem from noisy data, poor model generalizability, and domain shift, among others.
Typical metrics aggregate these mistakes into a single quantity that helps assess a model's performance.
However, not all mistakes are created equal, and such metrics cannot distinguish fundamental errors from relatively mundane ones.
Scaling up datasets and models in an attempt to improve model performance further exacerbate this issue.

For instance, here is a figure that shows such a fundamental error.
{% include gallery id="gallery_motivation_error" layout="" caption="A prediction of a car by the EfficientPS Model." %}
The green bounding box is a prediction made by the EfficientPS model, a top performing panoptic segmentation model on the CityScapes dataset.
The bounding box is predicted to be a car, despite defying reasonable notions about the dimensions of cars (this bounding box is clearly too wide for its height to describe an average car).
Such errors, made by models in deployment, can even cause the vehicle's controller to abruptly halt to avoid the perceived danger, potentially causing harmful consequences.

Such errors run counter to rules based on human intuition, such as the intuition about the width of a car with respect to its height.
In this work, we cast the problem of estimating such errors by finding violations of such rules, as well as improving model predictions with respect to these rules.



## Statistical Quantile Rules (SQRs)

One way to find such rules is to manually craft them.
However, doing so is challenging and costly, especially at scale.
In order to estimate such errors at scale without manually crafting such rules, one must therefore generate these rules.
First, however, we identify the desiderata for such rules before we can generate them:
1. **Validity**: They must hold true over most of the data they are generated over while allowing for some exceptions due to potentially noisy data,
2. **Expressivity**: They must be expressive enough to capture complex phenomena and relationships over the data, and
3. **Scalability**: They must be generatable at scale without the requirement of substantial human supervision.

Given these three requirements, we propose to specify such rules as *Statistical Quantile Rules (SQRs)*.
So what is a Statistical Quantile Rule?
Concretely, let $X$ be some random variable for a datapoint and $\phi(X)$ be some statistic of $X$.
Then, a $1 - \delta$ quantile rule over $X$ and $\phi$ is given by $a \leq \phi(X)$ for some constant $a$ and threshold $\delta$ if $P(a \leq \phi(X) \leq b) = 1 - \delta$.
Essentially, this states that the rule $a \leq \phi(X) \leq b$ is *valid* for $1 - \delta$ of all the data, and thus satisfies the validity requirement for a sufficiently small value of $\delta$.
As such, the values of $a$ and $b$ are generated from the data and depend on that of $\delta$ which can be manually specified.

Coming back to our example, if we consider $\phi$ to be the aspect ratio of cars, and $\delta$ to be 0.02, we can learn the relevant SQR by inferring the 98-percentile bounds for the aspect ratios of all the cars in the dataset.
This results in the rule

$$
0.07 \leq ratio(X) \leq 2.77
$$

While this rule is satisfied by 98% of all the ground truth bounding boxes of cars, it is not satisfied by the prediction from our example.
We can therefore use such rules to identify predictions that violate them.

Such rules can also be made more expressive by using more complex statistics, or by even combining statistics.
For instance, one can combine statistics like the widths and aspect ratios of the bounding boxes of cars to produce rules such as the follows:

$$
(ratio(X) < 0.81 \land 20.22 \leq width(X) < 1655.17) \land
(0.81 \leq ratio(X) < 1.16 \land 14.4 \leq width(X) < 614.24) \land \ldots
$$

This, along with the fact that all the values are generated given a dataset and a value of $\delta$, also allows for generating several such rules at scale from a small set of statistics.

## Generating SQRs

Now that we have defined SQRs, we can now try to generate them at scale.
To do so, we propose the Statistical Quantile Rule Learning (SQRL) framework.
SQRL is a framework that integrates logic-based methods with statistical inference to derive these rules from a model’s training data without supervision.
It also allows adapting models to these rules at test time to reduce rule violations and produce more coherent predictions.

{% include gallery id="gallery_workflow" layout="" caption="The Workflow of the SQRL Framework." %}

Let us discuss the workflow of SQRL step-by-step.
We assume that we have a dataset over which a model is trained.
As an example, let us consider the EfficientPS model trained over the KITTI dataset.
Even before we can generate SQRs, we need to first identify the relevant statistics to generate them over.

We specify the characteristics of the rules to be generated using a *rule schema*, as shown in the workflow diagram.
It specifies the form of the rule to generate, the statistics to generate it over, and the labels to consider while generating them, along with other hyperparameters.
For instance, we can specify a rule schema to generate SQRs over the aspect ratios of cars.

Once the rule schema is specified, we can generate the SQRs.
This is a two-step process.
First, we generate *abstract rules* that structurally conform to the rule schema, but do not contain any of the statistically inferred bounds associated with the rules.
Next, we statistically generate the bounds for these abstract rules to produce a suite of *concrete rules* that make up our final set of SQRs.
One can also optionally validate these rules using a held out validation set to consider only the most valid rules.

## Using SQRs

While there are several potential applications for SQRs, we focus on two of them in this work.
We first evaluate models use the suite of generated SQRs, and then improve them by adapting them to the SQRs at test time.
We study this over five applications and domains in our paper, but we will focus on three of them here.

### Object Detection

We first consider the Object Detection task used earlier in this post.
Here, we study the EfficientPS model trained over the KITTI dataset.
We use SQRL to generate 252 SQRs using 6 statistics to evaluate this model over several datasets and test its ability to adapt to different distributions.

An example of a generated SQR is as follows:

$$
(ratio(X) < 0.81 \land 20.22 \leq width(X) < 1655.17) \land
(0.81 \leq ratio(X) < 1.16 \land 14.4 \leq width(X) < 614.24) \land \ldots
$$

The following prediction violates this rule:

{% include gallery id="gallery_od_before" layout="" caption="A prediction of a car by the EfficientPS Model." %}

However, adapting the model at test time to this rule results in the following prediction of a better quality:

{include gallery id="gallery_od_after" layout="" caption="The same prediction after test-time adaptation." %}

Over all, we find around 8000 total violations of the 252 generated SQRs in a rainy version of the CityScapes dataset, which can reduce by around 30% after adapting the model at test time to these SQRs.

### Time Series Data Imputation

In this task, we train the SAITS model over the PhysioNet Challenge 2012 dataset to impute missing values within time-series data.
Here, we choose to generate only 35 SQRs, one for each of the 35 features in the dataset that need to be imputed.
One such SQR generated over the Glasgow Coma Score is as follows:

$$
(3 \leq GCS(X) < 15) \land
$$

Adapting the model at test time to this rule results in the imputed value being closer to the ground truth value, as shown in the figure below.

{include gallery id="gallery_impute" layout="half" caption="A datapoint imputed by the SAITS model. (left) originally and (right) after adapting it to the rule at test-time" %}

Over all, we find around 197 violations per sample (or more than 150K total violations), which are reduced by around 68% after adapting the model at test time to these SQRs. Moreover, the performance of the model improves by around 32% after adapting it to these SQRs.

### Sentiment Analysis

In this task, we evaluate the FinBERT model trained over the Financial PhraseBank dataset for sentiment analysis.
In order to generate SQRs for this task, we first extract 26 features using emotion classification and topic classification models.
This allows us to generate a total of 7878 SQRs over the dataset, of which we use 158 by validating it with a held out validation set.
One such SQR is as follows:

$$
\text{neutral}(x) \leftarrow 0.0204 \leq \text{fitness}(x) < 0.0234 \land 0.0387 \leq \text{news}(x) < 0.1408
$$

as in, if the likelihood that the sentence is about fitness is between 0.0204 and 0.0234, and that it is about news is between 0.0387 and 0.1408, then the sentence is neutral.

This rule is violated when FinBert predicts the sentiment for the following sentence to be positive:
> Anttila's online department store - NetAnttila - has an established position as the best-known, most visited, and most shopped online store in Finland.

The rule is violated because according to the feature extraction models, the likelihood that the sentence is about fitness is 0.021, and that it is about news is 0.141, which is outside the bounds specified by the rule.
However, adapting FinBert to this rule results in the prediction being changed to neutral, which is the correct sentiment for this sentence.

## Conclusion

In conclusion, we formalized statistical quantile rules as a means of characterizing basic errors inconsistent with training data and defined the problem of extracting such rules at scale.
We also proposed the SQRL framework to generate such rules and showed how to use them to evaluate and improve models.
For more details, please refer to our paper [here](https://arxiv.org/abs/2303.01433).


### Citation

> @article{naik2023machine,
  title={Do Machine Learning Models Learn Statistical Rules Inferred from Data?},
  author={Naik, Aaditya and Wu, Yinjun and Naik, Mayur and Wong, Eric},
  year={2023}
}



