---
layout: single
title: "[cs231n]3ê°• Loss Functions and Optimization"
categories: cs231n
tag: python
toc: true
toc_sticky: true
author_profile: false
---



# Loss Function

![cs231n_2017_lecture3_10](../../images/2022-01-14-lecture3/cs231n_2017_lecture3_10.png)

The function that takes in a W, looks at the scores and tells how bad quantitatively is that W.

- *f(x,W) = Wx*

- A dataset of examples

  *x_i* : image

  *y_i* : (integer) label

- *L_i* : Loss function that take in the predicted scores coming out of the function f (*f(x_i,W)*)  together with the true label Y (*y_i*). It give some quantative value for how bad those predictions are, for that training example.

- *L* : Final loss. The average of *L_i* over the entire data set(each of the N examples in data set). 

We'll eventually search over the space of W to find the W that minimizes the loss on training data.



---



# Multi-class SVM loss

![cs231n_2017_lecture3_11](../../images/2022-01-14-lecture3/cs231n_2017_lecture3_11.png)

- *L_i* : Compare the score of the correct category and the score of the incorrect category. If the score for the correct category is greater than the score of the incorrect category by **safety margin**(we set to 1), that means the score for the true category is much larger than any of the false categories, then we'll get a loss of zero. And we'll sum up over all of the incorrect categories for our image, and this will give us final loss for one example in the data set. 
- *L* : Take the average of this loss over the whole training data set.

![cs231n_2017_lecture3_12](../../images/2022-01-14-lecture3/cs231n_2017_lecture3_12.png)

This loss function is often referred to as **a hinge loss**.

- x axis : *S_yi* (the score of the true class for some training example)

  y axis : Loss

![cs231n_2017_lecture3_17](../../images/2022-01-14-lecture3/cs231n_2017_lecture3_17.png)

This is quantitative measure that our classifier is 5.3 bad on this data set.

> Q1: What happens to loss if car scores change a bit?  
> 	The loss will not change.  
> Q2: What is the min/max possible loss?  
> 	zero and infinity.  
> Q3: At initialization W is small so all s := 0. What is the loss?  
> 	Number of the classes -1.  
> 	This is useful thing to be checking in practice.  
> Q4: What if the sum was over all classes? (including j = y_i)  
> 	The loss increases by 1.  
> Q5: What if we used mean instead of sum?  
> 	It doesn't really matter.  
> Q6: What if we used a squared hinge loss?  
> 	This would end up computing a different loss function. This idea of a squared hinge loss used sometimes in practice.

- Example code

``` python
def L_i_vectorized(x, y, W):
    scores = W.dot(x)
    margins = np.maximum(0, scores-scores[y] + 1)
    margins[y] = 0
    loss_i = np.sum(margins)
    return loss_i
```

![cs231n_2017_lecture3_26](../../images/2022-01-14-lecture3/cs231n_2017_lecture3_26.png)

![cs231n_2017_lecture3_27](../../images/2022-01-14-lecture3/cs231n_2017_lecture3_27.png)

There are definitely other Ws. 2W also achieve zero loss.

How is the classifier to choose between these different versions of W that all achieve zero loss?



---



# Regularization

The whole idea of regularization is any thing that you do to your model, that penalizes the complexity of the model, rather than explicitly trying to fit the training data.

![cs231n_2017_lecture3_32](../../images/2022-01-14-lecture3/cs231n_2017_lecture3_32.png)



## Why we need regularization?

We don't really care about the training data performance, we really care about the performance of this classifier on test data.

So what we probably would have preferred the classifier to do is predict this straight green line rather than this very complex wiggly line to perfectly fit all the training data. 

**Regularization term**

We'll add another term to the loss function called a regularization term, which encourages the model to pick a simpler W.

**Occam's Razor**

If you have many different competing hypotheses, you should generally prefer the simpler one, because that is more likely to generalize to new observations in the future.

**Regularization hyper-parameter lambda**

Standard loss function have two terms, data loss and regularization loss. Hyper-parameter lambda, that trades off between the two. This is one of the important ones that we need to tune when training models in practice.



## Types of regularization

- (most common one) **L2 regularization** or **weight decay**

  L2 regularization is the euclidean norm of weigh vector W.

  It sometimes the squared norm or sometimes half the squared norm because it makes your derivatives work out a little bit nicer.

  Penalizing the euclidean norm of the weight vector.

- **L1 regularization**

  Penalizing the L1 norm of the weight vector.

  L1 regularization has some nice properties like encouraging sparsity in this matrix W.

- **Elastic net regularization**

  Some combination of L1 and L2.

- **Max norm regularization**

  Penalizing the max norm rather than the L1 or L2 norm.

- Some types of regularization that are more specific to deep learning : **dropout**, **batch normalization**, **stochastic depth**



## L2 Regularization (Weight Decay)

![cs231n_2017_lecture3_36](../../images/2022-01-14-lecture3/cs231n_2017_lecture3_36.png)

>Q: Which one would L2 regression prefer?  
>L2 regression would prefer W2, because it has a smaller norm.

L2 regression measures complexity of the classifier. L2 regularization prefers to spread that influence across all the different values in x. This might be more robust. In case we come up with xs that vary, then our decisions are spread out and depend on the entire x vector, rather than depending only on certain elements of the x vector.



## L1 regularization

>Q: Which one would L1 regression prefer?  
>L1 regression would prefer W1.

L1 regularization has different notion of complexity. 

The way of measuring complexity for L1 is the number of non-zero entries, and L2 thinks that spread the W across all the values are less complex.

It depends on our data, depends on our problem.

If you're a Bayesian, then using L2 regularization has nice interpretation of MAP inference under a Gaussian prior on the parameter vector.



---



# Softmax Classifier (Multinomial logistic regression)

Multinomial logistic regression loss function, endow scores with some additional meaning.  (For the multi-class SVM, we don't really say what those scores mean.) We're going to use those scores to compute a **probability distribution** over our classes.

![cs231n_2017_lecture3_40](../../images/2022-01-14-lecture3/cs231n_2017_lecture3_40.png)

**Softmax function** 

Take all of our scores, exponentiate them(become positive), then re-normalize them by the sum of those exponents.

So after we send our scores thorugh this softmax function, we end up with probability distribution.  Each probability is between zero and one, and the sum of probabilities across all classes sum to 1.

![cs231n_2017_lecture3_42](../../images/2022-01-14-lecture3/cs231n_2017_lecture3_42-16421392785711.png)

- Computed probability distribution : Coming out of softmax function
- Target probability distribution : If we know that the thing is a cat, put all of the probability mass on cat, so we would have probability of cat equals one, and zero probability for all the other classes.
- Loss function : Negative log of the probability of the true class.

What we want to do is encourage our computed probability distribution to match target probability distribution.

![cs231n_2017_lecture3_46](../../images/2022-01-14-lecture3/cs231n_2017_lecture3_46.png)

>Q1: What is the min/max possible loss L_i?  
>Zero and infinity. We'll never get to these minimum, maximum values with finite precision.  
>Q2: Usually at initialization W is small so all s := 0. What is the loss?  
>Log C. If it's not log C, then something's gone wrong.



---



# Softmax vs. SVM

![cs231n_2017_lecture3_49](../../images/2022-01-14-lecture3/cs231n_2017_lecture3_49.png)

The difference between the two loss functions is how we choose to interpret those scores to quantitatively measure the badness afterwards.

![cs231n_2017_lecture3_51](../../images/2022-01-14-lecture3/cs231n_2017_lecture3_51.png)

- SVM : Jiggling the scores didn't change the multi-class SVM loss at all.
- Softmax : softmax loss is quite different in this respect.
- Interesting difference between these two funtions : SVM, it'll get data point over the bar to be correctly classified and then just give up, it doesn't care about that data point any more. Whereas softmax will always try to continually improve every single data point to get better and better.



---



# Overview of Supervised Learning

![cs231n_2017_lecture3_53](../../images/2022-01-14-lecture3/cs231n_2017_lecture3_53.png)



---



# Optimization

How do we actually find W that minimizes the loss?



## Bad solution : Random search

![cs231n_2017_lecture3_57](../../images/2022-01-14-lecture3/cs231n_2017_lecture3_57.png)

![cs231n_2017_lecture3_58](../../images/2022-01-14-lecture3/cs231n_2017_lecture3_58.png)

Take a bunch of Ws, sampled randomly, and throw them into our loss function.

There's a gap between 15% accuracy and 95% accuracy.



## Follow the slope

![cs231n_2017_lecture3_60](../../images/2022-01-14-lecture3/cs231n_2017_lecture3_60.png)



### Gradient

- A vector of partial derivatives
- Have same shape as x
- Each element of the gradient tell us what is the slope of the function f, if we move in that coordinate direction.
- It points in the direction of greatest increase of the function and correspondingly, if you look at the negative gradient direction, that gives you the direction of greatest decrease of the function.
- It is super important because it gives linear, first-order approximation to your function at your current point.

If you want to know what is the slope of my landscape in any direction, then that's equal to the dot product of the gradient with the unit vector describing that direction.  

In practive, a lot of deep learning is about computing gradients of your functions and then using those gradients to iteratively update your parameter vector. 



### Evaluating gradient : using the method of finite differences

Our goal is to compute the gradient, dW, which will be a vector of the same shape as W, and each slot in that gradient will tell us how much will the loss change if we move a tiny infinitesimal amount in that coordinate direction.

![cs231n_2017_lecture3_63](../../images/2022-01-14-lecture3/cs231n_2017_lecture3_63.png)

If we move a little bit in the first dimension, than loss will decrease from 1.25347 to 1.25322. And then use this limit definition to come up with this finite differences approximation to the gradient in the first dimension.

Repeat this procedure in the second dimension, third dimension, and so on.

`It's terrible idea - super slow.`



### Evaluating gradient : use calculus to compute an analytic gradient

![cs231n_2017_lecture3_71](../../images/2022-01-14-lecture3/cs231n_2017_lecture3_71.png)

Rather than iterating over all the dimensions of W, we'll figure out what is the analytic expression for the gradient, and then just write it down and go directly from the W and compute the dW or the gradient in one step.

`exact, fast.`

![cs231n_2017_lecture3_72](../../images/2022-01-14-lecture3/cs231n_2017_lecture3_72.png)

Analytic gradient

- In practice, always use analytic gradient.

Numerical gradient

- A common debugging strategy : gradient check
- as a unit test to make sure that your analytic gradient was correct.
- Because it is super slow and inexact, you'll tend to scale down the parameter so that it runs in a reasonable amount of time.



---



# Gradient Descent

Gradient descent

1. Initialize our W as some random thing
2. Then while true, compute our loss and our gradient and then update our weights in the opposite of the gradient direction. 
   - We'll take a small step in the direction of minus gradient, and just repeat this forever and eventually your nework will converge.



## Step_size (Learning rate)

- A hyper-parameter
- Tells us that every time we compute the gradient, how far do we step in that direction.
- One of the single most important hyper-parameters that you need to set when training these things in practice.

``` python
# Vanilla Gradient Descent

while True:
    weights_grad = evaluate_gradient(loss_fun, data, weights)
    weights += step_size * weights_grad # perform parameter update
```

**A simple example in two dimensions**

![cs231n_2017_lecture3_74](../../images/2022-01-14-lecture3/cs231n_2017_lecture3_74.png)

- Red region :  the region of low loss we want to get to.
- Blue/green region : higher loss that we want to avoid.

There exist different **update rules** which tell us how exactly do we use that gradient information. But it's all the same basic algorithm of trying to go downhill at every time step.



---



# Stochastic Gradient Descent (SGD)



## Why do we use SGD?

![cs231n_2017_lecture3_76](../../images/2022-01-14-lecture3/cs231n_2017_lecture3_76.png)

N(the number of entire training set) could be very very large. So computing this loss could be very expensive and require computing perhaps millions of evaluations of this function. So that could be really slow.

Because the gradient is a linear operator, the gradient of our loss is the sum of the gradient of the losses for each of the individual terms. So if we want to compute the gradient, it requires us to iterate over the entire training data set(all N of these examples).

`Wait a very very long time before any individual update to W. `

So in practice, we tend to use **stochastic gradient descent**



## Stochastic Gradient Descent (SGD)

Rather than computing the loss and gradient over the entire training set, instead at every iteration, we sample some small set of training examples, called a **minibatch**.

- Minibatch : A power of two by convention(ex. 32, 64, 128)

Use this small minibatch to compute an estimate of the true gradient of the full sum.

This is **stochastic** because you can view this as a Monto Carlo estimate of some expectation of the true value.

``` python
# Vanilla Minibatch Gradient Descent

while True:
    data_batch = sample_training_data(data, 256) # sample 256 examples
    weights_grad = evaluate_gradient(loss_fun, data_batch, weights)
    weights += - step_size * weights_grad # perform parameter update
```

There is slightly fancier **update rules** of how to integrate multiple gradients over time, but this is the basic training algorithm that we use for all deep neural networks in practice.

![cs231n_2017_lecture3_77](../../images/2022-01-14-lecture3/cs231n_2017_lecture3_77.png)

There directions give you the direction of increase for the class scores for each of three classes.

- Changing Ws : cause decision boundaries to rotate.
- Chainging biases : cause decision boundaries not rotate, but move side to side or up and down.
- Changing step size : decision boundaries are flipping around  and trying to fit the data.



---



# Image Features

In practice, feeding raw pixel values into linear classifiers tends to not work so well.

![cs231n_2017_lecture3_79](../../images/2022-01-14-lecture3/cs231n_2017_lecture3_79.png)

So common before the dominance of deep neural networks, was instead to have this two-stage approach.

1. Take your image and compute various feature representations of that image that are maybe computing different kinds of  quantities relating to the appearance of the image.
2. Concatenate these different feature vectors to give you some feature representation of the image, and this feature representation of the image would be fed into a linear classifier.

**Motivation**

![cs231n_2017_lecture3_80](../../images/2022-01-14-lecture3/cs231n_2017_lecture3_80.png)

If we have training data set on the left of red and blue points, there's no way we can draw a linear decision boundary to separate the red points from the blue points.

If we use a clever feature transform, in this case transforming to polar coordinates, then after feature transform, this complex data set might become linearly separable and could be classified correctly by a linear classifier.



## Example of feature representation



### Color histogram

![cs231n_2017_lecture3_81](../../images/2022-01-14-lecture3/cs231n_2017_lecture3_81.png)

You'll take this hue color specturm, and divide it into buckets and then for every pixel, you'll map it into one of those color buckets and then count up how many pixels fall into each of these different buckets. This tells you globally what colors are in the image.

This is a simple feature vector that you might see in practice.



### Histogram of oriented gradients (HoG)

![cs231n_2017_lecture3_82](../../images/2022-01-14-lecture3/cs231n_2017_lecture3_82.png)

Another common feature vector that we saw before the dominance of neural networks.

- Hubel and Wiesel : found these oriented edges are really important in the human visual system.

This histogram of oriented gradients feature representation tries to capture the same intuition and measure the local orientation of edges on the image.

1. Take our image and divide it into little 8*8 pixel regions.
2. Within each of those  8*8 pixel regions, we'll compute what is the dominant edge direction of each pixel.
3. Quantize those edge directions into several buckets and then within each of those regions
4. Compute a histogram over these different edge orientations.
5. Full-feature vector will be these different bucketed histograms of edge orientations across all the different 8*8 regions in the image.

This is saying what types of edge information exist in the image and even localized to different parts of the image, what types of edges exist in different regions.

This was a super common feature representation and was used a lot for object recognition. 



### Bag of Words

![cs231n_2017_lecture3_83](../../images/2022-01-14-lecture3/cs231n_2017_lecture3_83.png)

This is taking inspiration from natural language processing. A way that you represent a paragraph by a feature vector is counting up the occurrences of different words in that paragraph. We take that intuition and apply it to images.

**Two-stage approach**

1. Build codebook

   - Get a bunch of images, sample a whole bunch of tiny random crops from those images and then cluster them(using like K means) 
   - It come up with these different cluster centers that are representing different types of visual words in the images.

   You can see that after clustering step, our visual words capture these different colors as well as different types of oriented edges in different directions.

2. Encode images

   - Encode our image by trying to say, how much does this visual word occur in the image? 

   This give us some slightly different information about what is the visual appearance of this image.



## Image features vs ConvNets

![cs231n_2017_lecture3_84](../../images/2022-01-14-lecture3/cs231n_2017_lecture3_84.png)

**ConvNets**

The only difference is that rather than writing down the features ahead of time, we're going to learn the features directly from the data.

1. Take raw pixels and feed them into convolutional network, which will computing through many different layers some type of feature representation driven by the data.
2. Then we'll train this entire weights for this entire network, rather than just the weights of linear classifier on top.