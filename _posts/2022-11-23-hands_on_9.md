---
layout: single
title:  "핸즈온 머신러닝 - 9"
categories : scikit-learn
tag : [scikit-learn, machine-learning, python]
toc: true
toc_sticky: true
---

![header](https://capsule-render.vercel.app/api?type=waving&color=a2dcec&height=300&section=header&text=핸즈온 머신러닝 - 9&fontSize=40&animation=fadeIn&fontAlignY=38&fontColor=FFFFFF)

- 참고 : [핸즈온 머신러닝 2판](http://www.kyobobook.co.kr/product/detailViewKor.laf?mallGb=KOR&ejkGb=KOR&barcode=9791162242964)

------------------------------------------------------

&nbsp;

## 비지도 학습

- 지도학습에 많은 발전이 이루어졌지만 가장 잠재력이 큰 분야
- **군집**
  - 비슷한 샘플을 클러스터로 모음, 군집은 데이터 분석, 고객 분류, 추천 시스템, 검색 엔진, 이미지 분할, 준지도 학습, 차원 축소 등에 사용할 수 있는 도구

- **이상치 탐지**
  - 정상 데이터가 어떻게 보이는지를 학습, 그다음 비정상 샘플을 감지하는 데 사용
  - 제조 라인에서 결함 제품을 감지하거나 시계열 데이터에서 새로운 트렌드를 찾음

- **밀도 추정**
  - 데이터셋 생성 확률 과정의 **확률 밀도 함수**(PDF)를 추정
  - 밀도 추정은 이상치 탐지에 널리 사용
  - 밀도가 매우 낮은 영역에 놓인 샘플이 이상치일 가능성이 높음, 데이터 분석과 시각화에도 유용


&nbsp;

### 군집

- 비슷한 샘플을 구별해 하나의 **클러스터** 또는 비슷한 샘플의 그룹으로 할당하는 작업
- 분류와 같이 각 샘플은 하나의 그룹에 할당됨, 분류와 다르게 군집은 비지도 학습방법
- ![image-20221123151952040](/images/2022-11-23-hands_on_9/image-20221123151952040.png)
  - 왼쪽은 iris 데이터셋, 클래스 구분이 잘 되어있음
  - 오른쪽은 같은 데이터 셋이지만 구분이 없는 상태


- 군집이 사용되는 예시

  - **고객 분류**
    - 고객을 구매 이력이나 웹사이트 내 행동 기반으로 클러스터로 모을 수 있음
    - 이는 고객이 누구인지, 고객이 무엇을 원하는지 이해하는 데 도움이 됨
    - 고객 그룹 각각에 제품 추천, 마케팅 전략을 다르게 적용할 수 있음

  - **데이터 분석**
    - 새로운 데이터셋을 분석할 때 군집 알고리즘을 실행하고 각 클러스터를 따로 분석하면 도움이 됨
  - **차원 축소 기법**
    - 한 데이터셋에 군집 알고리즘을 적용하면 각 클러스터에 대한 샘플의 **친화성**을 측정할 수 있음
    - 각 샘플의 특성 벡터 x는 클러스터 친화성의 벡터로 바꿀 수 있음
    - k개의 클러스터가 있다면 이 벡터는 k차원이 됨
    - 일반적으로 벡터는 원본 특성 벡터보다 훨씬 저차원이지만 이후의 분석을 위한 충분한 정보를 가질 수 있음
  - **이상치 탐지**
    - 모든 클러스터에 친화성이 낮음 샘플은 이상치일 가능성이 큼
    - 제조 분야의 결함을 감지, 부정 거래 감지 등에 활용 됨
  - **준지도 학습**
    - 레이블된 샘플이 적다면 군집을 수행하고 동일한 클러스터에 있는 모든 샘플에 레이블을 생성할수 있음
    - 이 방법을 통해 지도 학습 알고리즘에 필요한 레이블을 생성하여 성능 향상 가능
  - **검색 엔진**
    - 제시된 이미지와 비슷한 이미지를 찾아줌, 이런 시스템을 구축하려면 먼저 데이터 베이스에 있는 모든 이미지에 군집 알고리즘을 적용
    - 사용자가 찾으려는 이미지를 제공하면 훈련된 군집 모델을 사용해 이미지의 클러스터를 찾음, 이후에 이 클러스터의 모든 이미지를 반환

  - **이미지 분할**
    - 색을 기반으로 픽셀을 클러스터, 그다음 각 픽셀의 색을 해당 클러스터의 평균 색으로 변경
    - 이미지에 있는 색상의 종류를 크게 줄이고 물체의 윤곽을 감지하기 쉬워져 물체 탐지 및 추적 시스템에서 이미지 분할을 많이 활용

&nbsp;

### k-mean

- 각 클러스터의 중심을 찾고 가장 가까운 클러스터에 샘플을 할당

- 찾을 클러스터 개수 k를 지정해야 함

- 예제

  ```python
  from sklearn.cluster import KMeans
  
  k = 5
  kmeans = KMeans(n_clusters=k, random_state=42)
  y_pred = kmeans.fit_predict(X)
  
  ## label 확인
  kmeans.labels_
  >> array([4, 0, 1, ..., 2, 1, 0], dtype=int32)
  
  ## k=5 의 중심좌표
  kmeans.cluster_centers_
  >> array([[-2.80389616,  1.80117999],
             [ 0.20876306,  2.25551336],
             [-2.79290307,  2.79641063],
             [-1.46679593,  2.28585348],
             [-2.80037642,  1.30082566]])
  ```

  - 위 center을 기반으로 그린 보로노이 다이어그램

    ![image-20221123154132415](/images/2022-11-23-hands_on_9/image-20221123154132415.png)

- 샘플은 대부분 적절한 클러스터에 잘 할당됨, 하지만 몇몇 샘플은 레이블이 잘못 부여

- 실제 k-평균 알고리즘은 클러스터의 크기가 많이 다르면 잘 작동하지 않음

- 샘플을 클러스터에 할당할때 센트로이드까지 거리를 고려하는 것이 전부이기 때문

- **하드군집** 샘플을 클러스터에 할당하는 것보다 클러스터마다 샘플에 점수를 부여하는 것이 유용할 수 있음 이를 **소프트 군집** 이라고 함
  - **하드 군집**은 각 샘플에 대해 가장 가까운 것을 선택

```python
kmeans.transform(X_new)
array([[2.81093633, 0.32995317, 2.9042344 , 1.49439034, 2.88633901],
       [5.80730058, 2.80290755, 5.84739223, 4.4759332 , 5.84236351],
       [1.21475352, 3.29399768, 0.29040966, 1.69136631, 1.71086031],
       [0.72581411, 3.21806371, 0.36159148, 1.54808703, 1.21567622]])
```

- kmeans는 transform 매서드 샘플과 각 센트로이드 사이의 거리를 반환
- 이런 방식으로 고차원 데이터셋을 변환하면 k-차원 데이터셋 생성, 효율적인 비선형 차원 축소 방법이 될수 있음

&nbsp;

### k-mean 알고리즘

- `KMeans` 클래스는 기본적으로 최적화된 알고리즘을 적용, 아래에서 설명하는 내용은 init='random', n_init=1, algorithm='full'로 선택한 경우 원래의 k-means 설명

- 처음에는 센트로이드를 랜덤하게 선정, 그다음 샘플에 레이블을 할당하고 센트로이드를 업데이트
- 샘플에 레이블을 할당하고 센트로이드를 업데이트하는 식으로 센트로이드에 변화가 없을 때까지 계속 수행
- 제한된 횟수 안에 수렴하는 것으 보장, 무한 반복하지 않음

> 해당 알고리즘 계산 복잡도 : 샘플 개수(m), 클러스터 개수(k), 차원 개수(n)에 선형적이지만 군집할 수 있는 구조를 가질 경우에 해당
>
> 그렇지 않는 최악의 경우 계산 복잡도는 샘플 개수가 지수적으로 급증할수 있음, 일반적으로 k-means는 빠른 군집 알고리즘

&nbsp;

### 센트로이드 초기화 방법

- 센트로이드 위치를 근사하게 알 수 있다면 (다른 군집 알고리즘을 먼저 실행하여 어느정도 센트로이드를 알고 있을 경우) init 매개변수에 센트로이드 리스트를 담은 넘파이 배열을 지정하고 n_init을 1로 설정 할 수 있음

  - 예시

    ```python
    good_init = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]])
    kmeans = KMeans(n_clusters=5, init=good_init, n_init=1, random_state=42)
    kmeans.fit(X)
    kmeans.inertia_
    >> 211.598537258168
    ```

- 랜덤 초기화를 다르게 하여 여러 번 알고리즘을 실행하고 가장 좋은 솔루션을 선택하는 것
- 랜덤 초기화 횟수는 n_init 매개변수로 조절 가능, 기본값은 10
- 이너셔를 통해 가장 최적의 솔루션을 찾는 지표로 활용
- k-means는 현재는 k-means++ 초기화 방법을 기본으로 사용

&nbsp;

### k-means 속도 개선과 미니배치 k-means

- 불필요한 거리 계산을 많이 피함으로서 알고리즘의 속도를 상당히 높일 수 있음

- 현재는 k-means에서 기본적으로 사용됨

- 전체 데이터셋을 사용해 반복하지 않고 각 반복마다 미니배치를 사용해 센트로이드를 조금씩 이동 시키는 방식이 제안됨

  - 일반적으로 알고리즘 속도를 3배에서 4배 정도 높임

  - 메모리에 들어가지 않는 대량의 데이터셋에 군집 알고리즘을 적용

  - sklearn에서 minibatchkmeans 클래스에 구현함

  - 예제

    ```python
    from sklearn.cluster import MiniBatchKMeans
    
    minibatch_kmeans = MiniBatchKMeans(n_clusters=5, random_state=42)
    minibatch_kmeans.fit(X)
    
    minibatch_kmeans.inertia_
    >> 211.652398504332
    ```

- 미니배치 k-means가 일반 k-means 보다 훨씬 빠름 하지만 이너셔는 일반적으로 안좋음

- 특히 클러스터의 개수가 증가할 때 더욱 안좋아지는 경향을 보임

- 데이터 셋이 크다면 일반적인 k-means보다는 minibatchkmeans가 더 좋을수 있음

  ![image-20221123162905215](/images/2022-11-23-hands_on_9/image-20221123162905215.png)

  

