---
layout: single        # 문서 형식
title: Fully Connected Layer         # 제목
categories: Deep Learning    # 카테고리
toc: true             # 글 목차
author_profiel: false # 홈페이지 프로필이 다른 페이지에도 뜨는지 여부
sidebar:              # 페이지 왼쪽에 카테고리 지정
    nav: "docs"       # sidebar의 주소 지정
#search: false # 블로그 내 검색 비활성화
---

# 1. 
Definition
직역하면 "완전 연결 계층". 본 단어의 의미는 한 층(layer)의 모든 뉴런이 그 다음 층의 모든 뉴런과 연결된 상태를 의미.
1차원 배열의 형태로 평탄화된 행렬을 통해 이미지를 분류하는데 사용하는 계층


# 1. Intorduction


# 2. Background


# 3. Model Architecture
## 3.1. Encoder & Decoder Stacks
### Encoder
### Decoder

## 3.2. Attention
### Scaled Dot-Product Attention
### Multi-Head Attention
### Applications of Attention in our Model

## 3.3. Position-wise Feed-Forward Networks
## 3.4. Embeddings and Softmax
## 3.5. Positional Encoding

# 4. Why Self-Attention

# 참고
https://velog.io/@grovy52/Fully-Connected-Layer-FCL-%EC%99%84%EC%A0%84-%EC%97%B0%EA%B2%B0-%EA%B3%84%EC%B8%B5
