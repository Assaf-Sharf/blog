{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just Seeing GW150914 in LIGO data\n",
    "\n",
    "\n",
    "## prerequisites\n",
    "\n",
    " * This notebook has been tested in `Python2.7` and `Python3.5`.\n",
    " * To run this notebook you will need `numpy`, `scipy`, `matplotlib`, and `h5py`.\n",
    " * To automatically download the data, this notebook uses `wget`, you can download it manually if you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read in the LOSC data\n",
    "We will read in LIGO data in `hdf5` format available from [LOSC](https://www.losc.ligo.org/data/).\n",
    "This format contains metadata that we will read.\n",
    "The data read in here is 32 sec of data around GW150914 (in each of the two LIGO detectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# download data\n",
    "!wget https://www.losc.ligo.org/s/events/GW150914/H-H1_LOSC_4_V2-1126259446-32.hdf5\n",
    "!wget https://www.losc.ligo.org/s/events/GW150914/L-L1_LOSC_4_V2-1126259446-32.hdf5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also possible to use the ASCII data files (`.txt`) provided by LOSC, reading them in with `np.loadtxt()`. In that case you would have to modify this notebook and manually enter some metadata (sample rate, duration).\n",
    "\n",
    "This data reading code block is based on the LOSC provided [readligo.py](https://www.losc.ligo.org/s/sample_code/readligo.py) and [hdf5fits.py](https://www.losc.ligo.org/s/sample_code/hdf5fits.py) scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with h5py.File('H-H1_LOSC_4_V2-1126259446-32.hdf5', 'r') as dataFile:\n",
    "    # read H1 strain data\n",
    "    h_data = dataFile['strain']['Strain'][...]\n",
    "\n",
    "    # get metadata\n",
    "    dT = dataFile['strain']['Strain'].attrs['Xspacing']\n",
    "    Tobs = dataFile['meta']['Duration'].value\n",
    "    GPSstart = dataFile['meta']['GPSstart'].value\n",
    "\n",
    "    # compute other useful metadata\n",
    "    samp_rate = 1/dT\n",
    "    N = len(h_data)\n",
    "\n",
    "with h5py.File('L-L1_LOSC_4_V2-1126259446-32.hdf5', 'r') as dataFile:\n",
    "    # read L1 strain data; same length, start time, etc as H1\n",
    "    l_data = dataFile['strain']['Strain'][...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make lists of sample times and positive Fourier freqs\n",
    "time = np.arange(N)*dT\n",
    "freq = np.fft.rfftfreq(N, d=dT)\n",
    "\n",
    "dF = freq[1]-freq[0] # frequency spacing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## raw data\n",
    "\n",
    "We begin by plotting the raw data to see if we can spot GW150914.\n",
    "\n",
    "We know that GW150914 occured at about 16.4 sec in this segment.\n",
    "Do to the location and orientation of the two detectors, there was a time, phase, and amplitude offset $(\\Delta t, \\Delta\\phi, \\alpha)$ between the signals observed in the two detectors.\n",
    "\n",
    "We can overlay the two data streams to see the signal in both, if we know these offsets *a priori*:\n",
    "\n",
    "$$\\Delta t \\sim 0.0070 \\,\\mathrm{sec}$$\n",
    "$$\\Delta\\phi \\sim 180^\\circ$$\n",
    "$$\\alpha \\sim 1.3$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,4))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot(time, h_data, label='H data raw', color='C0')\n",
    "ax1.plot(time+0.007, -1.3*l_data, label='L data raw, shifted/scaled', color='C1')\n",
    "ax1.set_xlim([16.30, 16.45])\n",
    "ax1.set_ylim([-0.6e-18, 0.4e-18])\n",
    "ax1.set_xlabel('time (sec)')\n",
    "ax1.legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## whiten the data\n",
    "\n",
    "The LIGO noise spectrum is **colored**, meaning there is a different noise level at each frequency.\n",
    "This is in contrast to **white** noise, which has the same level for all frequencies.\n",
    "In our analysis we want to down-weight frequencies with lots of noise.\n",
    "We accomplish this by dividing the data by the noise spectrum.\n",
    "This is called data **whitening**, which puts each measured data sample onto a scale that is directly related to the signal-to-noise ratio or SNR of that sample.\n",
    "Integrating the whitened power gives the SNR-squared.\n",
    "\n",
    "We will write a few functions to do this.\n",
    "First we must compute the noise power spectral density (PSD) from the data using a running median of the Fourier transform of the data in the `get_PSD()` function.\n",
    "This function also  determines \"noise spikes\", narrow regions of the spectrum where the noise is much greater than the median.\n",
    "Next the `whiten_data()` function divides the data by the PSD.\n",
    "\n",
    "Before computing the FFT of the data, we will taper the time domain data and set it to zero mean, which helps with numerical stability of the FFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_taper(N, taplen):\n",
    "    \"\"\"\n",
    "    calculate a taper function array of length N with the first and\n",
    "    last taplen samples windowed\n",
    "    :param N: Number of samples in total array\n",
    "    :param taplen: number of samples to be tapered\n",
    "    :return ar: a shape (N,) array containing the taper function\n",
    "    \"\"\"\n",
    "    ar = np.ones(N, dtype=np.float64)  # populate with 1\n",
    "    if taplen > 0:\n",
    "        up_arg = np.arange(taplen)/taplen - 1.\n",
    "        down_arg = np.arange(-(taplen-1), 1) / taplen + 1.\n",
    "        ar[:taplen] = 0.5*(1. + np.cos(np.pi * up_arg))  # up taper\n",
    "        ar[-taplen:] = 0.5*(1. + np.cos(np.pi * down_arg))  # down taper\n",
    "    return ar\n",
    "\n",
    "\n",
    "def get_PSD(fdat, df, smooth_win=16, line_model=True):\n",
    "    \"\"\"empirically determine noise PSD from data with median smoothing\n",
    "    :param fdat: array of Fourier domain strain data\n",
    "    :param df: freq spacing (Hz) of input data\n",
    "    :param smooth_win: width of median smoothing window in Hz\n",
    "    :param line_model: return model for \"lines\"\n",
    "    :return PSD: noise model\n",
    "    \"\"\"\n",
    "    MED_2_MEAN 0.69314718055994529  # convert median to mean, chi_sq 2 deg o' free\n",
    "\n",
    "    Nfft = len(fdat)\n",
    "\n",
    "    # raw PSD of data\n",
    "    Snf = 2.*np.real(fdat*fdat.conj())\n",
    "    Snf[0] = 0\n",
    "\n",
    "    ### compute noise model\n",
    "    # median smoothing\n",
    "    win = int(smooth_win/df)  # size of median window\n",
    "    wins = np.array( [Snf[i:i+win] for i in range(Nfft-win)] )\n",
    "    meds = np.median(wins, axis=1) / MED_2_MEAN\n",
    "\n",
    "    # pad ends to correct length\n",
    "    lpad = win//2  # place first median in middle of first window\n",
    "    if (Nfft-len(meds))/2 == lpad:\n",
    "        rpad = lpad\n",
    "    else:\n",
    "        rpad = lpad+1\n",
    "    PSD = np.pad(meds, (lpad,rpad),\n",
    "                 'constant',\n",
    "                 constant_values=(meds[0], meds[-1]))  # median noise\n",
    "\n",
    "    if line_model:\n",
    "        # find 10 sigma excursions from median\n",
    "        line = Snf / PSD\n",
    "        line[line<10.] = 1.\n",
    "        return PSD, line\n",
    "    else:\n",
    "        return PSD\n",
    "\n",
    "\n",
    "def whiten_data(fdat, df, flow=16., fhigh=1024.,\n",
    "                psd=None, **psd_kwargs):\n",
    "    \"\"\"whitens data with PSD model\n",
    "    :param fdat: array of Fourier domain strain data\n",
    "    :param df: freq spacing (Hz) of input data\n",
    "    :param flow: low freq cutoff\n",
    "    :param fhigh: high freq cutoff\n",
    "    :param psd: noise power spectral density model\n",
    "    :param psd_kwargs: kwargs to use in get_PSD() if psd is None\n",
    "    :return wdat: time domain data whitened with the computed PSD\n",
    "    \"\"\"\n",
    "\n",
    "    N = 2*(len(data)-1)\n",
    "    fs = np.arange(len(data))*df\n",
    "\n",
    "    if psd is None:\n",
    "        psd = get_PSD(fdat, df, psd_kwargs)\n",
    "\n",
    "    # band pass and whiten\n",
    "    scale = 2. / np.sqrt(psd)\n",
    "    scale[fs<flow] = 0.\n",
    "    scale[fs>fhigh] = 0.\n",
    "\n",
    "    return fdat * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "taper_frac = 0.05  # taper 5% of data (at start and end)\n",
    "taplen = int(taper_frac * N)\n",
    "taper = get_taper(N, taplen)\n",
    "\n",
    "# set zero mean and taper in place\n",
    "for data in [h_data, l_data]:\n",
    "    mean = data.mean()\n",
    "    data -= mean\n",
    "    data *= taper\n",
    "\n",
    "# FFT data\n",
    "h_four = np.fft.rfft(h_data)\n",
    "l_four = np.fft.rfft(l_data)\n",
    "\n",
    "h_PSD, h_lines = get_PSD(h_four, dF, line_model=True)\n",
    "l_PSD, l_lines = get_PSD(l_four, dF, line_model=True)\n",
    "\n",
    "h_noise = h_PSD*h_lines\n",
    "l_noise = l_PSD*l_lines\n",
    "\n",
    "h_white = whiten_data(h_four, dF, psd=h_noise)\n",
    "l_white = whiten_data(l_four, dF, psd=l_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot our noise model.\n",
    "It's easiest to interpret if we take the square root of the PSD to get an **amplitude spectral density**.\n",
    "We will compare the amplitude at each frequency relative to the maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h_asd = np.sqrt(h_noise)\n",
    "l_asd = np.sqrt(l_noise)\n",
    "a0 = np.max((h_asd, l_asd))\n",
    "\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.loglog(freq, h_asd/a0, label='H ASD', color='C0')\n",
    "ax.loglog(freq, l_asd/a0, label='L ASD', color='C1')\n",
    "ax.set_xlabel('freq (Hz)')\n",
    "ax.set_ylabel(r'relative amplitude specral density')\n",
    "ax.set_xlim([8, 1200])\n",
    "ax.set_ylim([5.0e-6, 1])\n",
    "ax.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the whitened data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,4))\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot(time, h_white, label='H data, whitened', color='C0')\n",
    "ax1.plot(time+0.007, -1.3*l_white, label='L data, whitened, shifted/scaled', color='C1')\n",
    "ax1.set_xlim([16.30, 16.45])\n",
    "ax1.set_xlabel('time (sec)')\n",
    "ax1.legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot panel shows the whitened Hanford data zoomed in on GW150914 with the shifted and scaled whitened Livingston data overlayed.\n",
    "\n",
    "From the ASD plot above we saw that the noise is slightly louder in the Livingston detector, especially at low frequency.\n",
    "This is one reason why the L data has lower whitened amplitude, which is a proxy for the signal-to-noise ratio (SNR)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
